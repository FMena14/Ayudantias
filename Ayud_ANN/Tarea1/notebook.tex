
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{Enunciado}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    INF-395/477 Redes Neuronales Artificiales I-2018

Tarea 0 - Introducción a Redes Neuronales

\textbf{Temas}\\
* NNs por dentro: \emph{back-propagation from scratch}. * Principales
hiperparámetros de \emph{back propagation} * Introducción a keras *
Verificación numérica de las derivadas implementadas.

** Formalidades **\\
* Equipos de trabajo de: 2 personas (\emph{cada uno debe estar en
condiciones de responder preguntas sobre cada punto del trabajo
realizado}) * Se debe preparar un (breve) Jupyter/IPython notebook que
explique la actividad realizada y las conclusiones del trabajo * Fecha
de entrega: 30 de Marzo. * Formato de entrega: envı́o de link Github al
correo electrónico del ayudante
(\emph{\href{mailto:francisco.mena.13@sansano.usm.cl}{\nolinkurl{francisco.mena.13@sansano.usm.cl}}})
, incluyendo al profesor en copia
(\emph{\href{mailto:jnancu@inf.utfsm.cl}{\nolinkurl{jnancu@inf.utfsm.cl}}}).
Por favor especificar el siguiente asunto: {[}Tarea0-INF395-I-2018{]}

\paragraph{Paquetes instalación}\label{paquetes-instalaciuxf3n}

Para poder trabajar en el curso se necesitará instalar librerías para
Python, por lo que se recomienda instalarlas a través de \emph{anaconda}
(para Windows y sistemas Unix) en un entorno virtual, donde podrán
elegir su versión de Python. Se instalarán librerías como
\textbf{\href{http://scikit-learn.org/stable/}{\emph{sklearn}}}, una
librería simple y de facil acceso para \emph{data science},
\textbf{\href{https://keras.io/}{\emph{keras}}} en su versión con GPU
(para cálculo acelerado a través de la tarjeta gráfica), además de que
ésta utiliza como \emph{backend} \emph{TensorFlow} o \emph{Theano}, por
lo que habrá que instalar alguno de éstos, además de las librerías
básicas de \emph{computer science} como \emph{numpy}, \emph{matplotlib},
\emph{pandas}, además de claramente \emph{jupyter}.

\begin{itemize}
\item
  \textbf{\href{https://www.anaconda.com/download/\#linux}{Descargar
  anacona}}
\item
  Luego de instalar Anaconda y tenerla en el \emph{path} de su
  computador crear un entorno virtual:

\begin{verbatim}
conda create -n redesneuronales python=version
\end{verbatim}
\end{itemize}

con \emph{version}, la version de Python que desea utilizar. Si está en
Windows, se recomienda Python 3 debido a dependencias con una de las
librerías a utilziar.

\begin{itemize}
\item
  Acceder al ambiente creado

\begin{verbatim}
source activate redesneuronales
\end{verbatim}
\item
  Instalar los paquetes a utilizar

\begin{verbatim}
conda install jupyter sklearn numpy pandas matplotlib keras-gpu tensorflow-gpu 
\end{verbatim}
\item
  Para salir del entorno

\begin{verbatim}
source deactivate redesneuronales
\end{verbatim}
\end{itemize}

La tarea se divide en cuatro secciones:

Section \ref{primero} Back-propagation (BP) from \emph{Scratch}\\
Section \ref{segundo} Comparar back-propagation (BP) de Keras\\
Section \ref{tercero} Verificación numérica del gradiente para una
componente\\
Section \ref{cuarto} Implementar momentum como variante

     \#\# 1. Back-propagation (BP) from \emph{Scratch}

BP (Back-propagation) es sin duda el paradigma dominante para entrenar
redes neuronales \emph{feed-forward}. En redes grandes, diseñadas para
problemas reales, implementar BP eficientemente puede ser una tarea
delicada que puede ser razonable delegar a una librerı́a especializada.
Sin embargo, construir BP \emph{from scratch} es muy útil con fines
pedagógicos.

\[ w^{(t+1)} \leftarrow w^{(t)} - \eta \nabla_{w^{(t)}} Loss \]

\begin{quote}
\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\tightlist
\item
  Escriba un programa que permita entrenar una red FF con una
  arquitectura fija de 2 capa ocultas (con 32 neuronas en la primera
  capa y 16 en la segunda) y \(K\) neuronas de salida, sin usar
  librerı́as, excepto eventualmente \emph{numpy} para implementar
  operaciones básicas de algebra lineal. Por simplicidad, asuma
  funciones de activacion y error (\emph{loss function}) diferenciables
  o subdiferenciables, además de tener la misma función de activación
  para las 2 capas ocultas. Adapte la arquitectura para un problema de
  clasificación de 3 clases, es decir la función de activación para la
  capa de salida debe ser \textbf{softmax} con número de neuronas
  \(K\)=3. Escriba funciones para:\\
\end{enumerate}

\begin{itemize}
\item
  \begin{enumerate}
  \def\labelenumi{(\roman{enumi})}
  \tightlist
  \item
    implementar el \emph{forward pass}\\
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{(\roman{enumi})}
  \setcounter{enumi}{1}
  \tightlist
  \item
    implementar el \emph{backward pass}\\
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{(\roman{enumi})}
  \setcounter{enumi}{2}
  \tightlist
  \item
    implementar la rutina principal de entrenamiento, adoptando, por
    simplicidad, la variante cíclica aleatorizada de SGD (un ejemplo a
    la vez, pero iterando cíclicamente sobre una configuración aleatoria
    del conjunto de entrenamiento) con una tasa de aprendizaje fija de
    0.1 y número de ciclos fijos (\emph{epochs}).
  \end{enumerate}
\end{itemize}
\end{quote}

\begin{quote}
\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  Escriba una función que permita hacer predicciones mediante la red FF
  definida anteriormente, sin usar librerı́as, excepto eventualmente
  \emph{numpy}. Escriba una función vectorizada que implemente el
  forward pass sobre un conjunto de \(n_{t}\) ejemplos, además de
  implementar la función de decisión, que a través de la salida de la
  red prediga el valor categórico de la clase (1, 2 o 3).
\end{enumerate}
\end{quote}

\begin{quote}
\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\setcounter{enumi}{2}
\tightlist
\item
  Demuestre que sus programas funcionan en un problema de clasificación.
  Para esto utilice el dataset \textbf{iris}, disponible a través de la
  librería \textbf{\href{http://scikit-learn.org}{\emph{sklearn}}}, el
  cual corresponde a la clasificación de distintos tipos de plantas de
  iris (3 clases) mediante 4 características reales continuas
  específicas de la planta, deberá entrenar (ajustar) los pesos de la
  red para realizar la tarea encomendada, variando las funciones de
  error (\emph{loss}) entre \emph{categorical cross entropy} y
  \emph{mean squared error}, además de variar las funciones de
  activación para las 2 capas ocultas entre ReLU (Rectifier Linear Unit)
  y la función logística (\emph{sigmoid}). Especifique explícitamente
  las funciones anteriores, así como sus gradientes. Recuerde que debe
  transformar las etiquetas usando \emph{one hot vectors}.

  Es una buena práctica el normalizar los datos antes de trabajar con el
  modelo
\end{enumerate}
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ sklearn.preprocessing }\ImportTok{import}\NormalTok{ StandardScaler}
\ImportTok{from}\NormalTok{ sklearn.datasets }\ImportTok{import}\NormalTok{ load_iris}
\NormalTok{X_train,y_train }\OperatorTok{=}\NormalTok{ load_iris(return_X_y}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{scaler }\OperatorTok{=}\NormalTok{ StandardScaler().fit(X_train)}
\NormalTok{X_train }\OperatorTok{=}\NormalTok{ scaler.transform(X_train)}
\CommentTok{#transform target to one hot vector}
\ImportTok{import}\NormalTok{ keras}
\NormalTok{y_onehot }\OperatorTok{=}\NormalTok{ keras.utils.to_categorical(y_train)}
\end{Highlighting}
\end{Shaded}

Para evaluar los resultados, construya un gráfico correspondiente al
error de clasificación versus número de epochs, utilizando sólo el
conjunto de entrenamiento (el objetivo de esta sección es familiarizarse
con el algoritmo BP, no encontrar la mejor red). Grafique también la
evolución de la función objetivo utilizada para el entrenamiento. Además
de reportar el tiempo de entrenamiento mediante el algoritmo
implementado.\\
Por último, para alguna configuración elegida, reporte la matriz de
confusión mediante el uso de librerías como \emph{sklearn} o
\emph{keras}.

     \#\#\# 2. Comparar back-propagation (BP) de Keras

Keras es una de las librerı́as más populares para desarrollar nuevos
modelos de redes neuronales o implementar eficientemente modelos
conocidos con fines prácticos, puesto que ofrece una interfaz para poder
trabajar de una manera mucho mas simple además de permitir también el
manejo de configuraciones mas específicas.\\
Como actividad pedagógica ahora se les pide comparar el algoritmo
implementado por ustedes con el de alto nivel de la librería
\textbf{\href{https://keras.io/}{keras}} . Se les pedirá comparar sobre
el mismo dataset con la misma arquitectura utilizada anteriormente, es
decir, dos capas ocultas (con 32 y 16 neuronas respectivamente), 3
neuronas en la capa de salida con función de activación softmax,
optimizador Gradiente Descentente (GD) con tasa de aprendizaje fija.

\begin{quote}
\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\tightlist
\item
  Defina, a través de la interfaz de keras, la arquitectura de la red,
  con las funciones de activación para comparar con la sección anterior.
\end{enumerate}
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ keras.models }\ImportTok{import}\NormalTok{ Sequential}
\ImportTok{from}\NormalTok{ keras.layers.core }\ImportTok{import}\NormalTok{ Dense}
\NormalTok{model }\OperatorTok{=}\NormalTok{ Sequential()}
\NormalTok{model.add(Dense(}\DecValTok{32}\NormalTok{, input_dim}\OperatorTok{=}\NormalTok{X_train.shape[}\DecValTok{1}\NormalTok{], activation}\OperatorTok{=}\StringTok{"sigmoid or relu"}\NormalTok{))}
\NormalTok{model.add(Dense(}\DecValTok{16}\NormalTok{, activation}\OperatorTok{=}\StringTok{"sigmoid or relu"}\NormalTok{))}
\NormalTok{model.add(Dense(}\DecValTok{3}\NormalTok{, activation}\OperatorTok{=}\StringTok{"softmax"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{quote}
\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  Defina, a través de la interfaz de keras, el optimizador de la red, en
  conjunto con la función de error, para poder comparar con la sección
  anterior.
\end{enumerate}
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ keras.optimizers }\ImportTok{import}\NormalTok{ SGD}
\NormalTok{model.}\BuiltInTok{compile}\NormalTok{(optimizer}\OperatorTok{=}\NormalTok{SGD(lr}\OperatorTok{=}\FloatTok{0.1}\NormalTok{),loss}\OperatorTok{=}\StringTok{"categorical_crossentropy or mse"}\NormalTok{, metrics}\OperatorTok{=}\NormalTok{[}\StringTok{"accuracy"}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{quote}
\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\setcounter{enumi}{2}
\tightlist
\item
  Entrene (ajuste) los pesos de la red definida mediante keras,
  reportando los mismos gráficos de la sección anterior para poder
  comparar. Si hay diferencias en la convergencia del algoritmo ¿A qué
  podría deverse? si hay una gran diferencia en los tiempos de
  entrenamiento ¿A qué podría deverse?
\end{enumerate}
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model.fit(X_train, y_onehot, epochs}\OperatorTok{=}\DecValTok{100}\NormalTok{, batch_size}\OperatorTok{=}\DecValTok{1}\NormalTok{, verbose}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

     \#\#\# 3. Verificación numérica del gradiente para una componente

En esta sección deberá verificar numéricamente el gradiente para los
parámetros del modelo (que en este caso son los pesos de la red), que
hasta ahora a definido de manera analítica en su programa, por ejemplo
la derivada de \(x^2\) es \(2x\). Ahora deberá verificar estos cálculos
usando la definición de gradiente.

\[ \nabla_{w} Loss = \lim_{\epsilon \rightarrow 0} \frac{Loss(w+ \epsilon)-Loss(w)}{\epsilon} \]

Debido a que el \emph{forward propagation} es relativamente fácil de
implementar, se puede confiar en que se realizó de manera correcta, por
lo que el cómputo del error (\emph{loss}) debería ser correcto. Esto
significa que podemos verificar el gradiente o la derivada analítica del
error \(\frac{\partial Loss}{\partial w}\) comprobando que el resultado
obtenido es similar (dentro de una tolerancia numérica, por ejemplo
\(10^6\)) al valor que obtenemos aplicando la fórmula anterior.
Naturalmente interpretaremos \(\lim_{\epsilon \rightarrow 0}\) como un
valor "\emph{suficientemente pequeño}" de \(\epsilon\).

\begin{quote}
\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\tightlist
\item
  Para un peso escogido aleatoriamente entre la primera capa de la red
  (\emph{input}) y la primera capa oculta, calcule el valor del
  gradiente de la función de error para ambas funciones utilizadas
  (ayúdese mediante las funciones de \emph{backward pass} implementadas
  anteriormente), luego compare y verifique con el valor numérico del
  gradiente mediante el procedimiento explicado anteriormente.
\end{enumerate}
\end{quote}

\begin{quote}
\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  Vuelva a verificar el valor del gradiente para otros dos pesos
  escodigos aleatoriamente en la primera operación de la red. Compare y
  concluya.
\end{enumerate}
\end{quote}

     \#\#\# 4. Implementar \emph{momentum} como variante

En esta sección deberá construir, sin usar librerı́as, excepto
eventualmente \emph{numpy} para implementar operaciones básicas de
algebra lineal, una variante del programa definido anteriormente
(Section \ref{primero}) que entrene la red utilizando \emph{momentum}
clásico.

\[ v^{(t+1)} \leftarrow \mu v^{(t)} - \eta \nabla_{w^{(t)}} Loss \\
w^{(t+1)} \leftarrow w^{(t)} + v^{(t+1)}
\]

\begin{quote}
\emph{Sutskever, I., Martens, J., Dahl, G., \& Hinton, G. (2013,
February). On the importance of initialization and momentum in deep
learning. In International conference on machine learning (pp.
1139-1147).}
\end{quote}

Demuestre que su programa funciona en el mismo problema de clasificación
presentado anteriormente, para esto, además deberá construir un gráfico
de la función de error o pérdida (\emph{loss}) \emph{vs} el número de
\emph{epochs} y comentar/analizar la convergencia. ¿Es una mejora
significativa?


    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
