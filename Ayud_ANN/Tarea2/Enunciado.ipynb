{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Y1tHj3DNqhX"
   },
   "source": [
    "<img src=\"http://www.exalumnos.usm.cl/wp-content/uploads/2015/06/Isotipo-Negro.gif\" title=\"Title text\" width=\"20%\" height=\"20%\" />\n",
    "\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "<h1 align='center'> INF-395/477 Redes Neuronales Artificiales I-2018 </h1>\n",
    "\n",
    "<H3 align='center'> Tarea 2 - Aplicaciones Recientes de Redes Neuronales </H3>\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "\n",
    "**Temas**  \n",
    "\n",
    "* Diseño e implementación detallado de Redes Recurrentes (RNN) y sus derivados.\n",
    "* Compuertas LSTM y GRU. \n",
    "* Arquitectura Encoder-Decoder y mecanismo de atención.\n",
    "* Autoencoders para traducción\n",
    "* Modelos generativos profundos: VAEs (*Variational Autoencoder*)\n",
    "\n",
    "** Formalidades **  \n",
    "* Equipos de trabajo de: 2-3 personas (*cada uno debe estar en condiciones de realizar una presentación y discutir sobre cada punto del trabajo realizado*)\n",
    "* Se debe preparar una presentación de 20 minutos. Presentador será elegido aleatoriamente.\n",
    "* Se debe preparar un (breve) Jupyter/IPython notebook que explique la actividad realizada y las conclusiones del trabajo\n",
    "* Fecha de entrega y discusión: 31 de Mayo.\n",
    "* Formato de entrega: envı́o de link Github al correo electrónico del ayudante (*<francisco.mena.13@sansano.usm.cl>*) , incluyendo al profesor en copia (*<jnancu@inf.utfsm.cl>*). Por favor especificar el siguiente asunto: [Tarea2-INF395-I-2019]\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "La tarea se divide en secciones:\n",
    "\n",
    "[1.](#primero) RNN sobre texto  \n",
    "[2.](#segundo) *Encoder-Decoder* sobre imágenes    \n",
    "[3.](#tercero)    \n",
    "[4.](#cuarto) Distintos tipos de autoencoders (AEs) en MNIST    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "84-hf1CuNqhZ"
   },
   "source": [
    "<a id=\"primero\"></a>\n",
    "## 1. RNN sobre texto\n",
    "---\n",
    "\n",
    "Hoy en dı́a, una aplicación relevante de las redes neuronales recurrentes es el modelamiento de texto y lenguaje natural. En esta sección abordaremos el problema de procesar sentencias de texto, proporcionadas por GMB (*Groningen Meaning Bank*), para reconocimiento de entidades y *tagger*. En específico, trabajaremos con el dataset proprocionado a través de __[Kaggle](https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus)__, que está compuesto por más de un millón de palabras, a fin de realizar predicciones sobre distintas tareas del tipo *many to many* y *many to one*.\n",
    "\n",
    "<img src=\"https://i.stack.imgur.com/b4sus.jpg\" width=\"70%\" />\n",
    "\n",
    "\n",
    "Descargue los datos de la página de __[Kaggle](https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus)__ y cárguelos mediante *pandas*.\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "df_ner = pd.read_csv(\"./entity-annotated-corpus/ner.csv\", encoding =\"cp1252\", error_bad_lines=False)\n",
    "df_ner.dropna(inplace=True)\n",
    "```\n",
    "\n",
    "\n",
    "> a) En esta primera instancia trabajaremos con la tarea de realizar un NER *tag* (**Named Entity Recognition**) sobre cada una de las palabras en las sentencias que se nos presenta en los datos. Esta tarea es del tipo *many to many*, es decir, la entrada es una secuencia y la salida es una secuencia, sin *shift*, por lo que necesitaremos una estructura de red adecuada a ésto. En primer lugar extraiga las columnas que utilizaremos del dataset **¿Por qué es conveniente utilizar *lemma* en vez de la misma palabra?**\n",
    "```python\n",
    "dataset = df_ner.loc[:,[\"lemma\",\"word\",\"pos\",\"tag\",\"prev-iob\"]]\n",
    "```\n",
    "Luego de esto cree una estructura que contendrá todas las sentencias u oraciones y otra estructura que contendrá los las etiquetas (*tags*), esto es un arreglo de arreglos de *lemmas* y un arreglo de arreglos de *tags* respectivamente. **¿Cuales son las dimensiones de ambas estructuras? ¿Cada dato de ejemplo tiene las mismas dimensiones que el resto?**\n",
    "```python\n",
    "n_used = 500000 #data to use-- your choice\n",
    "dataX_raw,dataY_raw = [],[]\n",
    "lemmas,labels = set(), set()  #uniques\n",
    "for fila in dataset.values[:n_used]:\n",
    "    if fila[-1]==\"__START1__\": \n",
    "        dataX_raw.append(sentence)\n",
    "        dataY_raw.append(labels_sentence)\n",
    "        sentence= []\n",
    "        labels_sentence = []\n",
    "    lemmas.add(fila[0])\n",
    "    labels.add(fila[3])\n",
    "    sentence.append(fila[0]) #add lemma\n",
    "    labels_sentence.append(fila[3]) #TAG\n",
    "dataX_raw = dataX_raw[1:]\n",
    "dataY_raw = dataY_raw[1:]\n",
    "```    \n",
    "\n",
    "> b) Estudie la distribución del largo de los textos a procesar. Estudie también la frecuencia con la que aparecen las palabras en todo el dataset. **¿Se observa una ley Zipf?**[[1]](#refs) Realice un gráfico de la cantidad de datos por clase. Comente.\n",
    "\n",
    "\n",
    "> c) Para representar los textos, cada posible *tag* y *lemma*, será necesario codificarlos para que la red pueda manejar ésto, a través de un número único (*indice*) ¿Cuántos *tags* y *lemmas* distintos existen?  Comente sobre el significado del *tag* para cada *lemma*. **Finalmente mida cual es el largo máximo de entre todas las sentencias**.\n",
    "```python\n",
    "n_labels = len(labels)\n",
    "lab2idx = {t: i for i, t in enumerate(labels)}\n",
    "dataY = [[lab2idx[ner] for ner in ner_tags ] for ner_tags in dataY_raw] #Converting tags to indexs\n",
    "n_lemmas = len(lemmas)\n",
    "lemma2idx = {w: i for i, w in enumerate(lemmas)} \n",
    "dataX = [[lemma2idx[lemma] for lemma in sentence ] for sentence in dataX_raw] #Converting text to indexs\n",
    "```\n",
    "\n",
    "> d) Debido a la distinta extensión de textos se deberá **realizar *padding* para estandarizar el largo**,\n",
    "considere algun carácter especial **no presente en el vocabulario** para codificar el espacio en blanco en ambos (entrada y salida), por ejemplo si el largo máximo es de 4 y se tiene la sentencia \"the rocket\" codificada como [32,4] será necesario agregar un *lemma* que codificado significará el fin de la sentencia \"the rocket *ENDPAD ENDPAD*\" que codificado quedará como [32,4,*N, N*]. Decida, respecto al cómo funciona una red recurrente y su *memoria*, sobre qué le parece más conveniente al momento de rellenar con un valor especial ¿Al principio o al final de la sentencia? Comente\n",
    "```python\n",
    "...#add fullfill lemma and tag to the dictionary\n",
    "lemma2idx[\"END\"] = n_lemmas\n",
    "lab2idx[\"END\"] = n_labels\n",
    "n_labels +=1\n",
    "n_lemmas +=1\n",
    "from keras.preprocessing import sequence\n",
    "X = sequence.pad_sequences(dataX, maxlen=max_input_lenght,padding='pre' or 'post',value=lemma2idx[\"yourspecialcharacter\"])\n",
    "y = sequence.pad_sequences(dataY, maxlen=max_input_lenght,padding='pre' or 'post',value=lab2idx[\"endtagger\"])\n",
    "del dataY[:],dataX[:]\n",
    "```\n",
    "\n",
    "\n",
    "> e) Para poder generar una representación adecuada sobre los datos de entrada que permita realizar operaciones lineales, deberá generar una representación a un vector denso. Para ésto se utilizará la arquitectura de autoencoder **Word2Vec** [[2]](#refs) sobre textos *raws*, en donde el *encoder* codifica una palabra categórica (*target*) a un vector denso de dimensionalidad $d$ mientras que el *decoder* genera palabras en el contexto (*context*) de la palabra *target* (en una vecindad alrededor). La idea detrás es que palabras similares sean proyectadas a una región cercana en el espacio de *embedding*.\n",
    "```python\n",
    "from gensim.models import Word2Vec\n",
    "EMBEDDING_DIM = 32\n",
    "window_size = 5\n",
    "nb_epoch = 5\n",
    "batch_size = 6000\n",
    "model = Word2Vec(dataX_raw,size=EMBEDDING_DIM,window=window_size,batch_words=batch_size,iter=nb_epoch,\n",
    "                 min_count=3, negative=5,sg=1) #sg=1 mean skip-gram\n",
    "embeddings_index = {vocab_word: model.wv[vocab_word] for vocab_word in model.wv.vocab}\n",
    "```\n",
    "Genere una matriz de *embeddings* que se utilizarán como capa neuronal.\n",
    "```python\n",
    "embedding_matrix = np.zeros((n_lemmas, EMBEDDING_DIM))\n",
    "for word, i in lemma2idx.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: #if word does not has embedding\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "```\n",
    "Luego, para poder realizar una clasificación sobre los datos en la salida será necesario representarlos *one hot vectors*, esto resultará en un arreglo tridimensional.\n",
    "```python\n",
    "from keras.utils import to_categorical\n",
    "y = np.asarray([to_categorical(i, num_classes=n_labels) for i in y])\n",
    "```\n",
    "\n",
    "> o utilizar word2vec pre-entrenados en otro lado? https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join('glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Se encontraron %s terminos con sus vectores de embedding.' % len(embeddings_index))\n",
    "\n",
    "\n",
    "> f) Luego de esto cree los conjuntos de entrenamiento y de prueba con el código a continuación **¿Cuáles son las dimensiones de entrada y salida de cada conjunto?** Comente\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=22)\n",
    "y_train.shape\n",
    "```\n",
    "\n",
    "> g) Defina una red neuronal recurrente *many to many* con compuertas LSTM para aprender a *tagear* la entidad en el texto. Esta red debe procesar la secuencia de *lemmas* rellenados (o sin rellenar) y entregar el *tag* a cada uno de los *lemmas*, por lo que la salida de la red es una por cada instante de tiempo que se necesita entregar un *output*. La primera capa de la red a construir debe tener los vectores de *embedding* encontrados por **Word2Vec**. **Comente sobre los cambios que sufre un dato al ingresar a la red y la cantidad de parámetros de la red**. Entrene y luego evalúe su desempeño sobre ambos conjuntos. \n",
    "```python\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Embedding, Dense, Dropout\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=n_lemmas, output_dim=EMBEDDING_DIM, input_length=max_input_lenght,\n",
    "                    trainable=False, weights = [embedding_matrix]))\n",
    "model.add(LSTM(units=100,return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(n_labels, activation='softmax'))\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=15, batch_size=128)\n",
    "```\n",
    "Para evaluar su modelo utilice una métrica adecauda para el desbalance presente entre las clases como identificó en el punto b).\n",
    "```python\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"F1 score on test: \", f1_score(y_test, model.predict(X_test,verbose=0),average='macro')\n",
    "```\n",
    "\n",
    "> h) Varíe con seguir entrenando la capa de embedding seteada al definir la arquitectura, ésto es cambiar a *trainable=True*, compare el desempeño y el número de parámetros (entrenables) con lo anterior.\n",
    "\n",
    "\n",
    "> ?? Algunos autores señalan la importante dependencia que existe en texto, no solo con las palabras anteriores, sino que con las que siguen.**Mejore la red definida en f) utilizando una red neuronal recurrente Bidireccional**, es decir, con recurrencia en ambas direcciones sobre la secuencia de *lemmas* de entrada. Comente **cuál debiera ser la forma correcta de usar el parámetro *merge_mode*** (concatenar, multiplicar, sumar o promediar) para este caso. Además comente las transformaciones que sufre el patrón de entrada al pasar por las capas. **¿Mejora o empeora el desempeño?** Analice.\n",
    "```python\n",
    "from keras.layers import Bidirectional\n",
    "model = Sequential()\n",
    "model.add(your choice of embedding)\n",
    "layer_lstm = LSTM(units=100,return_sequences=True)\n",
    "model.add(Bidirectional(layer_lstm,merge_mode=choose))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(n_labels, activation='softmax'))\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=15, batch_size=128)\n",
    "```\n",
    "\n",
    "> i) Experimente con cambiar la *gate* de recurrencia a una con menos parámetros pero que mantiene la escencia de la LSTM, ésta es la compuerta GRU. Comente sobre los resultados esperados y observados.\n",
    "```python\n",
    "from keras.layers import GRU\n",
    "...\n",
    "model.add(GRU(units=100,return_sequences=True))\n",
    "...\n",
    "```\n",
    "\n",
    "> j) En base a lo experimentado, **intente mejorar el desempeño de las redes encontradas**, ya sea utilizando y/o combinando las distintas variaciones que se hicieron en los distintos ítemes, como bien alguna mejora en el pre-proceso de los datos (largo de secuencia, el tipo de *padding* o alguna otra), agregar mayor profundidad, variar el número de unidades/neuronas, utilizando otra *gate* de recurrencia (en https://keras.io/layers/recurrent/),  entre otras.\n",
    "\n",
    "\n",
    "> k) Utilice la red entranda anteriormente, **se espera que sea la mejor de esta sección**, y **muestre las predicciones**, el *NER tager*, sobre algún ejemplo de pruebas, comente.  \n",
    "```python\n",
    "p = model.predict(np.array([X_test[i]]))\n",
    "p = np.argmax(p, axis=-1)\n",
    "print(\"{:15}: {}\".format(\"Lemma\", \"Pred\"))\n",
    "for w,pred in zip(X_test[i],p[0]):\n",
    "    print(\"{:15}: {}\".format(lemmas[w],labels[pred]))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iFtCUBp9Nqhb"
   },
   "source": [
    "<a id=\"segundo\"></a>\n",
    "## 2. Encoder-Decoder sobre imágenes\n",
    "---\n",
    "En la presente seccion se experimentará con arquitecturas del tipo *encoder-decoder* [[3]](#refs) aplicadas sobre imágenes, tales como *image translation*, *object location*, *image segmentation*, entre otros. La idea es aplicar una red convolucional en ambas partes del modelo (encoder y decoder), así utilizamos un modelo que se adapte a estos casos.\n",
    "\n",
    "La tarea consistirá en realizar **Image Segmentation** [[4]](#refs) para identificar ciertos segmentos o regiones de interés en una imagen a través de procesar de manera semántica (en la codificación) si cada pixel corresponde a un segmento a destacar. Esta tarea puede ser aplicada tanto para identificar un segmento como para identificar múltiples segmentos a través de colocar varios canales/filtros de salida en el *decoder*. Para ésto trabajaremos con un dataset creado (*A BENCHMARK FOR SEMANTIC IMAGE SEGMENTATION*) en el área. El dataset resulta bastante pequeño en cantidad de datos, por lo que deberá pensar en formas de conllevar ésto.\n",
    "\n",
    "<img src=\"https://qph.fs.quoracdn.net/main-qimg-78a617ec1de942814c3d23dab7de0b24\" width=\"70%\" />\n",
    "\n",
    "Descargue los datos a través del siguiente __[link](http://www.ntu.edu.sg/home/ASJFCai/Benchmark_Website/benchmark_index.html)__. Luego cargue las pocas imágenes a trabajar. Debido a la dimensionalidad variable de los datos de entrada deberá redimensionar a un valor que considere prudente, *se aconseja menos de 250*.\n",
    "```python\n",
    "import numpy as np\n",
    "import os\n",
    "img_size = choose\n",
    "folder = \"imagefolder..\"\n",
    "data = [archivo.split(\".\")[0] for archivo in os.listdir(folder+\"/image\")]\n",
    "from PIL import Image\n",
    "X_image = []\n",
    "for archivo in data:\n",
    "    I = Image.open(folder+\"/image/\"+archivo+\".jpg\")\n",
    "    I = np.asarray(I.resize( (img_size,img_size),Image.ANTIALIAS ))\n",
    "    X_image.append(I)\n",
    "X_image = np.asarray(X_image)\n",
    "Y_image = []\n",
    "for archivo in data:\n",
    "    I = Image.open(folder+\"/ground-truth/\"+archivo+\".png\")\n",
    "    I = np.asarray(I.resize( (img_size,img_size),Image.ANTIALIAS ))\n",
    "    Y_image.append(I)\n",
    "Y_image = np.asarray(Y_image)\n",
    "```\n",
    "\n",
    "> a) Explore los datos a trabajar, visualice la entrada y salida del modelo, además de las dimensionalidades de entrada ¿Es un problema las dimensiones de los datos? Normalicelos datos como se acostumbra en imágenes y genere una dimensión extra a la salida.\n",
    "```python\n",
    "...#visualize and do nice plots!\n",
    "X_image = X_image/255.\n",
    "Y_image = Y_image/255.\n",
    "Y_image = Y_image[:,:,:,None]\n",
    "```\n",
    "\n",
    "> b) extra?\n",
    "\n",
    "> c) Debido a la poca cantidad de datos presentes defina la arquitectura a utilizar utilizando únicamente convolucionales (*fully convolutional*) [[5]](#refs), como la presente en el código. Comente sobre los cambios en la dimensionalidad a través del *forward pass*. Decida el tamaño del *batch* en base a la cantidad de datos que se presenta para entrenar.\n",
    "```python\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D,MaxPool2D, Conv2DTranspose, UpSampling2D, BatchNormalization\n",
    "model = Sequential()\n",
    "...#ENCODER PART\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', padding='same',input_shape=X_image.shape[1:]))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPool2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPool2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "...#DECODER PART\n",
    "model.add(UpSampling2D((2, 2)))\n",
    "model.add(Conv2DTranspose(32, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2DTranspose(32, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2DTranspose(1, (3, 3), strides=(2,2), activation='sigmoid', padding='same')) #pixel-wise classification\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',optimizer='rmsprop')\n",
    "model.fit(X_image,Y_image,epochs=100,batch_size=...)\n",
    "```\n",
    "\n",
    "> d) Para medir el desempeño del modelo realice un análisis cualitativo en base a visualizar la segmentación que realiza *versus* la segmentación real, además de verificar el *precision* y *recall* asumiendo valores binarios de pixel ¿Qué valor debería ser más importante los ceros o 1? Comente.\n",
    "```python\n",
    "Y_hat_image = np.squeeze( model.predict(X_image) )\n",
    "...#visualice Y_hat and Y_image\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "Y_label = Y_hat.flatten() >0.5\n",
    "Y_hat_label = Y_image.flatten() >0.5\n",
    "print(precision_score(Y_label, Y_hat_label, average=None, labels=[0,1] ))\n",
    "print(recall_score(Y_label, Y_hat_label, average=None , labels=[0,1]))\n",
    "```\n",
    "\n",
    "> e) Experimente con realizar *data augmentation* sobre el problema. Debido a que las operaciones clásicas de *augmentation* como rotar, invertir, girar, cambiarian la etiqueta de segmentación, genere una estratégia que mantenga esta etiqueta $Y$. Se presenta un código de ejemplo, *Denoising*, de aplicar una máscara binaria aleatoria sobre la imagen de entrada $X$, **de todas formas se espera que proponga alguna distinta**. Compare el desempeño con la forma de evaluar realizada en (d).\n",
    "```python\n",
    "from numpy.random import binomial #DENOISING IDEA\n",
    "T = 100\n",
    "for _ in range(T):\n",
    "    noise_level = np.random.randint(4,10)/10.\n",
    "    noise_mask = binomial(n=1,p=noise_level,size=X_image.shape)\n",
    "    X_augmented = X_image*noise_mask\n",
    "    model.fit(X_augmented,Y_image,epochs=1,batch_size=32,validation_data=(X_image,Y_image))\n",
    "```\n",
    "\n",
    "> f) Intente variar la arquitectura presentada en pos de obtener un mejor modelo, basado en la evaluación realizada en (d). Recuerde tomar en cuenta la poca cantidad de datos que se tiene.\n",
    "\n",
    "\n",
    "Otro dataset con 350 imagenes, vs las 100: https://github.com/preddy5/segnet/tree/master/CamVid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"tercero\"></a>\n",
    "## 3. Problema 3\n",
    "\n",
    "Opcion 1: (texto)\n",
    "\n",
    "Experimentar con encoder-decoder y atención..\n",
    "\n",
    "translation de audio a texto?  \n",
    "link: https://www.kaggle.com/mozillaorg/common-voice/home\n",
    "\n",
    "Lectura dataset\n",
    "codigo\n",
    "\n",
    "> a) analisis previos\n",
    "\n",
    "> b) generar representacion para texto\n",
    "\n",
    "> c) generar representacion para audio\n",
    "\n",
    "> d) Definar arquitectura\n",
    "\n",
    "> e) entrenar \n",
    "\n",
    "> f) Visualizar\n",
    "\n",
    "> se podria mejorar agregando informacion extra? -- mejor no. solo audio.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"cuarto\"></a>\n",
    "## 4. Distintos tipos de autoencoders (AEs) en MNIST\n",
    "---\n",
    "\n",
    "Como se ha discutido en clases, las RBM’s y posteriormente los AE's (redes no supervisadas) fueron un componente crucial en el desarrollo de los modelos que entre 2006 y 2010 vigorizaron el área de las redes neuronales artificiales con logros notables de desempeño en diferentes tareas de aprendizaje automático. Recientemente se ha propuesto AE's con distribuciones de probabilidades en su codificación, VAE. Los VAE son una variación bayesiana que aprende los parámetros de alguna distribución de variables latentes de los datos, a través de una muestra sobre ésta variable latente, el decodificar generar/reconstruye nuevos datos $\\hat{x}$. Dicho de otra palabras es un autoencoder que aprende el modelo de las variables latentes de los datos.\n",
    "\n",
    "<img src=\"https://www.researchgate.net/profile/Steven_Young11/publication/306056875/figure/fig1/AS:393921575309346@1470929630835/Example-images-from-the-MNIST-dataset.png\" title=\"mnist\" width=\"25%\" style=\"float: right;\" />\n",
    "\n",
    "Con este objetivo en mente, utilizaremos un dataset simple denominado **MNIST**. Se trata de una colección de 70000 imágenes de 28 $\\times$ 28 pixeles correspondientes a dígitos manuscritos (números entre 0 y 9). En su versión tradicional, la colección se encuentra separada en dos subconjuntos: uno de entrenamiento de 60000 imágenes y otro de test de 10000 imágenes\n",
    "\n",
    "\n",
    "\n",
    "Cargue los datos desde el repositorio de Keras.\n",
    "```python\n",
    "from keras.datasets import mnist\n",
    "import keras.backend as K\n",
    "import keras\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train[:,:,:,None] #add channels\n",
    "x_test = x_test[:,:,:,None]\n",
    "img_rows, img_cols,channel = x_train.shape[1:]\n",
    "original_img_size = (img_rows, img_cols,channel) # input image dimensions\n",
    "```\n",
    "\n",
    "> a) Normalice las imágenes de modo que los pixeles queden en el rango [0, 1] como se acostumbra. Comente sobre los datos a trabajar.\n",
    "```python\n",
    "import numpy as np\n",
    "x_train = x_train.astype('float32') / 255. #and x_test\n",
    "...#Define here your validation set\n",
    "```\n",
    "\n",
    "### 4.1 Autoencoder clásico\n",
    "Una de las aplicaciones tı́picas de un AE es **reducción de dimensionalidad**, es decir, implementar una transformación $\\phi:{\\rm I\\!R}^d \\rightarrow {\\rm I\\!R}^{d'}$ de objetos representados originalmente por $d$ atributos en una nueva representación de $d'$ atributos, de modo tal que se preserve lo mejor posible la “información” original. Obtener tal representación es útil desde un punto de vista computacional (compresión) y estadı́stico (permite construir modelos con un menor número de parámetros libres). Un AE es una técnica de reducción de dimensionalidad no supervisada porque no hace uso de información acerca de las clases a las que pertenecen los datos de entrenamiento.\n",
    "\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/0*yGqTBMopqHbR0fcF.\" title=\"AE\" width=\"50%\" />\n",
    "\n",
    "\n",
    "> a) Entrene un AE básico, 1 capa escondida *feed forward*, para generar una representación de MNIST en $d'= 2, 8, 16, 32$ dimensiones. Justifique la elección de la función de pérdida a utilizar y del criterio de entrenamiento en general. Determine el porcentaje de compresión obtenido y el error de reconstrucción en cada caso. ¿Mejora el resultado si elegimos una función de activación **ReLU** para el Encoder? ¿Podrı́a utilizarse esta activación en el Decoder?\n",
    "```python\n",
    "from keras.layers import Input, Dense, Flatten,Reshape\n",
    "from keras.models import Model\n",
    "latent_dim = 2\n",
    "input_img = Input(shape=original_img_size)\n",
    "input_fl = Flatten()(input_img) #to get a vector representation\n",
    "encoded = Dense(latent_dim, activation='sigmoid')(input_fl)\n",
    "decoded = Dense(np.prod(original_img_size), activation='sigmoid')(encoded)\n",
    "decoded = Reshape(original_img_size)(decoded)\n",
    "autoencoder = Model(inputs=input_img, outputs=decoded)\n",
    "encoder = Model(inputs=input_img, outputs=encoded)\n",
    "autoencoder.compile(optimizer='rmsprop', loss='binary_crossentropy')\n",
    "autoencoder.fit(x_train,x_train,epochs=40,batch_size=32,validation_data=(x_test,x_test))\n",
    "autoencoder.save('basic_autoencoder.h5')\n",
    "```\n",
    "\n",
    "> b) Compare visualmente la reconstrucción que logra hacer el autoencoder desde la representación en ${\\rm I\\!R}^{d'}$ para algunas imágenes del conjunto de pruebas. Determine si la percepción visual se corresponde con el error de reconstrucción observada. Comente.\n",
    "```python\n",
    "from keras.models import load_model\n",
    "autoencoder = load_model('basic_autoencoder.h5')\n",
    "decoded_test = autoencoder.predict(x_test)\n",
    "encoded_test = encoder.predict(x_test)\n",
    "import matplotlib.pyplot as plt\n",
    "n = 10\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28),cmap='gray')\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_test[i].reshape(28, 28),cmap='gray')\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "> c) Para verificar la calidad de la representación obtenida, implemente el clasificador denominado $kNN$ (k-nearest neighbor): dada una imagen $x$, el clasificador busca las k = 10 imágenes de entrenamiento más similares (de acuerdo a una distancia, e.g. euclidiana) y predice como clase, la etiqueta más popular entre las imágenes cercanas. Mida el error de pruebas obtenido construyendo este clasificador sobre la data reducida a través del autocnder comparando con la representación reducida obtenida vía PCA (una técnica clásica de reducción de dimensionalidad) utilizando el mismo número de dimensiones $d'$= 2, 8, 16, 32. Considere tanto el error de reconstrucción como el desempeño en clasificación , además de comparar los tiempos medios de predicción en ambos escenarios.\n",
    "```python\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "pca = PCA(n_components=d)\n",
    "pca.fit(x_train)\n",
    "pca_train = pca.transform(x_train)\n",
    "pca_test = pca.transform(x_test)\n",
    "encoded_train = encoder.predict(x_train)\n",
    "encoded_test = encoder.predict(x_test)\n",
    "clf = KNeighborsClassifier(10) #CLASIFICATION\n",
    "clf.fit(pca_train, y_train)\n",
    "print('Classification Accuracy PCA %.2f' % clf.score(pca_test,y_test))\n",
    "clf = KNeighborsClassifier(10) #CLASIFICATION\n",
    "clf.fit(encoded_train, y_train)\n",
    "print('Classification Accuracy %.2f' % clf.score(encoded_test,y_test))\n",
    "```\n",
    "\n",
    "\n",
    "> d) Modifique el autoencoder básico construido en (a) para implementar un *deep autoencoder* (más de dos capas) haciendo uso de las capas convolucionales para trabajar sobre matrices. Comente cómo sufre las transformaciones el patrón de entrada. Demuestre experimentalmente que este autoencoder puede mejorar la compresión obtenida por PCA y por el obtenido en (a) utilizando el mismo número de dimensiones $d'$ . Experimente con $d' =2, 8, 16, 32$. Considere en esta comparación tanto el error de reconstrucción como el desempeño en clasificación (vı́a kNN) de cada representación. Comente.\n",
    "```python\n",
    "from keras.layers import Input,Conv2D,Flatten,Dense,MaxPooling2D, UpSampling2D\n",
    "latent_dim = 2\n",
    "input_img = Input(shape=original_img_size)\n",
    "x = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "before_F_shape =  (x.shape[1].value, x.shape[2].value, x.shape[3].value)\n",
    "x = Flatten()(x)\n",
    "encoded = Dense(latent_dim, activation='sigmoid')(x)\n",
    "x = Dense(np.prod(before_F_shape),activation='relu')(encoded)\n",
    "x = Reshape(before_F_shape)(x)\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(16, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "autoencoder = Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='rmsprop', loss='binary_crossentropy')\n",
    "autoencoder.summary()\n",
    "autoencoder.fit(x_train,x_train,epochs=40,batch_size=32,validation_data=(x_test,x_test))\n",
    "autoencoder.save('deep_autoencoder.h5')\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "pca = PCA(n_components=target_dim)\n",
    "pca.fit(x_train)\n",
    "```\n",
    "\n",
    "\n",
    "> e) Elija algunas de las representaciones aprendidas anteriormente y visualı́celas usando la herramienta *TSNE* disponible en la librerı́a *sklearn*. Compare cualitativamente el resultado con aquel obtenido usando PCA con el mismo número de componentes\n",
    "```python\n",
    "nplot=5000 #warning: mind your memory!\n",
    "encoded_train = encoder.predict(x_train[:nplot])\n",
    "from sklearn.manifold import TSNE\n",
    "model = TSNE(n_components=2, random_state=0)\n",
    "encoded_train = model.fit_transform(encoded_train)\n",
    "plt.figure(figsize=(10, 10))\n",
    "colors={0:'b',1:'g',2:'r',3:'c',4:'m',5:'y',6:'k',7:'orange',8:'darkgreen',9:'maroon'}\n",
    "markers={0:'o',1:'+',2: 'v',3:'<',4:'>',5:'^',6:'s',7:'p',8:'*',9:'x'}\n",
    "for idx in xrange(0,nplot):\n",
    "    label = y_train[idx]\n",
    "    line = plt.plot(encoded_train[idx][0], encoded_train[idx][1],\n",
    "        color=colors[label], marker=markers[label], markersize=6)\n",
    "pca_train = pca.transform(x_train)\n",
    "encoded_train = pca_train[:nplot]\n",
    "... #plot PCA\n",
    "```\n",
    "\n",
    "### 4.2 Variational Autoencoder tradicional\n",
    "El enfoque optimizador de los VAE sobre los parámetros modelados $\\theta$ (decoder) y $\\phi$ (encoder) es que minimiza la reconstrucción de los datos (al igual que un autoencoder tradicional), en base a alguna medicicón de error (*mse* por ejemplo) agregando una regularización que se impone para que la distribución aprendida de las variables latentes sea similar alguna distribución deseada *a priori*.  \n",
    "\n",
    "$$ Min \\ \\mathcal{L}(p_{\\theta}(x\\mid z), \\ x)\\ +\\ KL( q_{\\phi}(z\\mid x) \\mid \\mid p_{\\theta}(z))$$\n",
    "\n",
    "Con $\\mathcal{L}$ la función de pérdida de reconstrucción, $KL$ la *KL Divergence* [[5]](#refs), $p_{\\theta}(x\\mid z)$ la recontrucción de los datos a través de las variables aleatoria latentes $z$ y  $p_{\\theta}(z)$ una distribución *a priori* asignada. \n",
    "\n",
    "<img src=\"https://i.imgur.com/ZN6MyTx.png\" title=\"VAE\" width=\"60%\" />\n",
    "\n",
    "> a) Defina la sección del *encoder* del VAE como el que se muestra en el código, de 3 tandas convolucionales y una *fully conected*, con una distribución Normal de 2 componentes para las variables latentes, $z \\sim \\mathcal{N} (\\mu, \\sigma^2 )$. Describa la arquitectura utilizada.\n",
    "```python\n",
    "from keras.layers import Input,Conv2D,Flatten,Dense,MaxPool2D\n",
    "from keras.models import Model\n",
    "filters = 32 # number of convolutional filters to use\n",
    "num_conv = 3 # convolution kernel size\n",
    "intermediate_dim = 128\n",
    "latent_dim = 2\n",
    "x = Input(shape=original_img_size)\n",
    "conv_1 = Conv2D(filters,kernel_size=num_conv,padding='same', activation='relu')(x)\n",
    "conv_2 = Conv2D(filters,kernel_size=num_conv,padding='same', activation='relu')(conv_1)\n",
    "conv_3 = Conv2D(filters*2, kernel_size=num_conv, padding='same', activation='relu', strides=2)(conv_2)\n",
    "flat = Flatten()(conv_3)\n",
    "hidden = Dense(intermediate_dim, activation='relu')(flat)\n",
    "z_mean = Dense(latent_dim,activation='linear')(hidden)\n",
    "z_log_var = Dense(latent_dim,activation='linear')(hidden)\n",
    "encoder = Model(x, z_mean) # build a model to project inputs on the latent space\n",
    "```\n",
    "\n",
    "> b) Defina la sección del *decoder* del VAE como el que se muestra en el código, una tanda *fully conected* y 3 tandas de la operación inversa a una convolución (**Convolución transpuesta** [[2]](#refs)), comente cómo ésta trabaja y cómo funcionan los parámetros de *stride*.\n",
    "```python\n",
    "from keras.layers import Reshape,Conv2DTranspose,Activation\n",
    "shape_before_flattening = K.int_shape(conv_3)[1:] # we instantiate these layers separately to reuse them later\n",
    "decoder_hid = Dense(intermediate_dim, activation='relu')\n",
    "decoder_upsample = Dense(np.prod(shape_before_flattening), activation='relu')\n",
    "decoder_reshape = Reshape(shape_before_flattening)\n",
    "decoder_deconv_1 = Conv2DTranspose(filters,kernel_size=num_conv, padding='same',strides=2,activation='relu')\n",
    "decoder_deconv_2 = Conv2DTranspose(filters,kernel_size=num_conv,padding='same', activation='relu')\n",
    "decoder_mean_squash = Conv2DTranspose(channel, kernel_size=num_conv,padding='same', activation='sigmoid')\n",
    "```\n",
    "\n",
    "> c) Defina la sección que conecta a estas dos partes a través de un muestreo explícito de la distribución Normal (con $\\epsilon \\sim \\mathcal{N}(0,1)$ se tiene $g = \\mu + \\sigma \\cdot \\epsilon$), ésto es lo que lo hace que sea un enfoque probabilístico/bayesiano. Describa el modelo completo.\n",
    "```python\n",
    "def sampling(args):\n",
    "    epsilon_std = 1.0\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim),mean=0., stddev=epsilon_std)\n",
    "    return z_mean + K.exp(z_log_var) * epsilon\n",
    "from keras.layers import Lambda\n",
    "z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "hid_decoded = decoder_hid(z)\n",
    "up_decoded = decoder_upsample(hid_decoded)\n",
    "reshape_decoded =  decoder_reshape(up_decoded)\n",
    "deconv_1_decoded = decoder_deconv_1(reshape_decoded)\n",
    "x_decoded_relu = decoder_deconv_2(deconv_1_decoded)\n",
    "x_decoded_mean_squash = decoder_mean_squash(x_decoded_relu)\n",
    "vae_norm = Model(x, x_decoded_mean_squash) # instantiate VAE model\n",
    "vae_norm.summary()\n",
    "```\n",
    "\n",
    "> d) Como la función objetivo es *customizada* deberemos definirla y poner una distribución a *priori* sobre las variables latentes, en este caso se tendrá como media un vector de ceros y la matriz de covarianza la matriz identidad $p_{\\theta}(z) \\sim N (\\vec{0},I)$. Elija la función de pérdida para la reconstrucción. Comente porqué la *KL Divergence* podría funcionar como regularizador del criterio de entrenamiento obtenido.\n",
    "```python\n",
    "from keras import backend as K # Compute VAE loss\n",
    "choised_loss =  keras.metrics.binary_crossentropy(K.flatten(x),K.flatten(x_decoded_mean_squash))\n",
    "choised_loss =  keras.metrics.mean_squared_error(K.flatten(x),K.flatten(x_decoded_mean_squash))\n",
    "reconstruction_loss = img_rows * img_cols * channel* choised_loss\n",
    "kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1) #closed form\n",
    "vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "vae_norm.add_loss(vae_loss)\n",
    "vae_norm.summary()\n",
    "```\n",
    "\n",
    "> e) Entrene el modelo definido con los datos de MNIST entre 20 a 30 *epochs* con el optimizador de *RMSprop* y tamaño de batch el que estime conveniente.\n",
    "```python\n",
    "batch_size = ...\n",
    "epochs =  [20,30]\n",
    "vae_norm.compile(optimizer='rmsprop')\n",
    "vae_norm.fit(x_train,epochs=epochs, batch_size=batch_size,validation_data=(x_test, None))\n",
    "```\n",
    "\n",
    "> f) Visualice la representación codificada $z$ (variables latentes) de los datos en base a su media $\\mu_i$, compare cualitativamente con la representación *TSNE* del AE tradicional. Además genere un histograma de la media y la varianza $\\sigma_i^2$ de las dos componentes. Comente.\n",
    "```python\n",
    "x_test_encoded = encoder.predict(x_test, batch_size=batch_size)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(x_test_encoded[:, 0], x_test_encoded[:, 1], c=y_test)\n",
    "plt.colorbar()\n",
    "plt.show() # display a 2D plot of the digit classes in the latent space\n",
    "encoder_log_var = Model(x,z_log_var)\n",
    "...#histogram\n",
    "```\n",
    "\n",
    "> g) Genere nuevos datos artificialmente a través del espacio de las variables latentes. Para esto deberá generar puntos linealmente separados por debajo de la distribución Normal. Comente qué significada cada eje en la imagen ¿qué sucede más allá en el espacio del 90% confianza de las variables latentes? ¿Qué objetos se generan?\n",
    "```python\n",
    "decoder_input = Input(shape=(latent_dim,))\n",
    "_hid_decoded = decoder_hid(decoder_input)\n",
    "_up_decoded = decoder_upsample(_hid_decoded)\n",
    "_reshape_decoded = decoder_reshape(_up_decoded)\n",
    "_deconv_1_decoded = decoder_deconv_1(_reshape_decoded)\n",
    "_x_decoded_relu = decoder_deconv_2(_deconv_1_decoded)\n",
    "_x_decoded_mean_squash = decoder_mean_squash(_x_decoded_relu)\n",
    "generator = Model(decoder_input, _x_decoded_mean_squash) \n",
    "n = 30  # figure with 15x15 images \n",
    "image_size = img_cols\n",
    "figure = np.zeros((image_size * n, image_size * n))\n",
    "from scipy.stats import norm\n",
    "grid_x = norm.ppf(np.linspace(0.05, 0.95, n)) #metodo de la transformada inversa\n",
    "grid_y = norm.ppf(np.linspace(0.05, 0.95, n))\n",
    "for i, yi in enumerate(grid_x):\n",
    "    for j, xi in enumerate(grid_y):\n",
    "        z_sample = np.array([[xi, yi]])            \n",
    "        x_decoded = generator.predict(z_sample,batch_size=batch_size)\n",
    "        figure[i * image_size: (i + 1) * image_size,\n",
    "               j * image_size: (j + 1) * image_size] = x_decoded[0][:,:,0]\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(figure,cmap='gnuplot2')\n",
    "pos = np.arange(image_size/2,image_size*n,image_size)\n",
    "plt.yticks(pos,np.round(grid_y,1))\n",
    "plt.xticks(pos,np.round(grid_x,1))\n",
    "plt.show()\n",
    "grid = norm.ppf(np.linspace(0.000005, 0.999995, n)) #en los extremos del intervalo de confianza\n",
    "```\n",
    "\n",
    "> h) Experimente y comente si mejora o empeora el desempeño de clasificación de la representación encontrada al aumentar la dimensionalidad de las variables latentes $z$, contrarrestándolo con el AE tradicional ($d' = 2,8,16,32 $). Explique.\n",
    "\n",
    "\n",
    "### 4.3 Variational Autoencoder categórico\n",
    "\n",
    "En esta última sección se explorará el caso en que se cambia el modelamiento sobre la variable latente a una distribuida Multinomial para representar una variable **categórica** que podría entregarnos cierta intuición de capturar las clases del problema de manera no supervisada. Para éste objetivo definiremos el número de variables latentes iguales a la cantidad de clases que sospechamos (en este caso son conocidas y corresponden a 10 dígitos).\n",
    "\n",
    "\n",
    "> a) En primer lugar deberá definir la arquitectura realizando unos cambios leves a la presentada anteriormente. Comente las diferencias sobre los parámetros obtenidos.\n",
    "\n",
    "> El primer cambio es en la distribución obtenida en el encoder.\n",
    "```python\n",
    "... #traditional VAE code here\n",
    "latent_dim = 10\n",
    "logits_z = Dense(latent_dim,activation='linear')(hidden) #log(p(z))\n",
    "encoder = Model(x, logits_z) # build a model to project inputs on the latent space\n",
    "```\n",
    "\n",
    "> Luego, con el decoder creado (igual al caso anterior) es necesario cambiar la forma en que se conectan, ya que el muestreo ahora será a través de un truco diferente para variables categoricas (Gumbel-Softmax (REEEF)).\n",
    "```python\n",
    "def sample_gumbel(shape,eps=K.epsilon()):\n",
    "    \"\"\"Inverse Sample function from Gumbel(0, 1)\"\"\"\n",
    "    U = K.random_uniform(shape, 0, 1)\n",
    "    return - K.log( -K.log(U + eps) + eps)\n",
    "def sampling(logits_z):\n",
    "    \"\"\" Perform a Gumbel-Softmax sampling\"\"\"\n",
    "    tau = K.variable(2/3, name=\"temperature\") \n",
    "    z = logits_z + sample_gumbel(K.shape(logits_z)) # logits + gumbel noise\n",
    "    return keras.activations.softmax( z/tau )    \n",
    "from keras.layers import Lambda\n",
    "z = Lambda(sampling, output_shape=(latent_dim,))(logits_z)\n",
    "... #traditional VAE ode here\n",
    "vae_norm = Model(x, x_decoded_mean_squash) # instantiate VAE model\n",
    "vae_norm.summary()\n",
    "```\n",
    "\n",
    "> Finalmente la función de pérdida KL cambia ya que se asume un *prior* Multinomial con probabilidad uniforme. ¿Qué interpretación se le da a este regularizador?\n",
    "```python\n",
    "... #traditional VAE ode here\n",
    "dist =  keras.activations.softmax(logits_z) # =p(z)\n",
    "dist_neg_entropy = K.sum(dist * K.log(dist + K.epsilon()), axis=1)\n",
    "kl_disc_loss =  np.log(latent_dim) + dist_neg_entropy #discrete KL-loss\n",
    "vae_loss = K.mean(reconstruction_loss + kl_disc_loss)\n",
    "vae_norm.add_loss(vae_loss)\n",
    "... #traditional VAE ode here\n",
    "```\n",
    "\n",
    "> b) Entrene el VAE categórico de la misma manera que realizó con el VAE tradicional en (e) ¿Nota algún cambio en este paso?.\n",
    "\n",
    "> c) Para ver la efectividad del encoder en lograr capturar las clases es necesario medir una métrica de desempeño, sin embargo, las métricas clásicas como *accuracy* o *f1 score* no corresponderían a este caso debido a que las categoría capturada por el encoder no debería estar en el mismo orden de la clase real, ya que fue un entrenamiento no supervisado. Con esto en mente mida alguna métrica de __[*clustering*](https://scikit-learn.org/stable/modules/clustering.html)__ sobre las categorías inferidas por el VAE. Comente\n",
    "```python\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - x.max(axis=-1,keepdims=True) )\n",
    "    return e_x / e_x.sum(axis=-1, keepdims=True)\n",
    "p_z_train = softmax(encoder.predict(x_train))\n",
    "p_z_test = softmax(encoder.predict(x_test))\n",
    "y_train_pred = p_z_train.argmax(axis=-1)\n",
    "y_test_pred = p_z_test.argmax(axis=-1)\n",
    "...#Example\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "print(normalized_mutual_info_score(y_train, y_train_pred))\n",
    "print(normalized_mutual_info_score(y_test, y_test_pred))\n",
    "```\n",
    "\n",
    "> d) Para entender mejor las categorías inferidas por el VAE genere datos \"activando\" una categoría y luego realizando un *forward pass* sobre el decoder/generador. Comente cualitativamente.\n",
    "```python\n",
    "decoder_input = Input(shape=(latent_dim,))\n",
    "_hid_decoded = decoder_hid(decoder_input)\n",
    "_up_decoded = decoder_upsample(_hid_decoded)\n",
    "_reshape_decoded = decoder_reshape(_up_decoded)\n",
    "_deconv_1_decoded = decoder_deconv_1(_reshape_decoded)\n",
    "_x_decoded_relu = decoder_deconv_2(_deconv_1_decoded)\n",
    "_x_decoded_mean_squash = decoder_mean_squash(_x_decoded_relu)\n",
    "generator = Model(decoder_input, _x_decoded_mean_squash) \n",
    "predictions =np.zeros((img_cols * 1 ,img_cols* latent_dim))\n",
    "for i in range(latent_dim):\n",
    "    activate_aux = np.zeros((1,10))\n",
    "    activate_aux[:,i] = 1 #activate a class\n",
    "    predictions[:, i * img_cols: (i + 1) * img_cols] = np.squeeze(generator.predict(activate_aux))\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(predictions, cmap='gnuplot2')\n",
    "pos = np.arange(img_cols/2, img_cols*latent_dim, img_cols)\n",
    "plt.xticks(pos,range(latent_dim))\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "> e) Genere algunas imágenes aleatorias, comente cualitativamente con lo obtenido con el VAE tradicional ¿Cuál parciera ser mejor para ésto? ¿Por qué?\n",
    "\n",
    "\n",
    "\n",
    "### [Opcional] Extras\n",
    "> Vea qué sucede al cambiar algún aspectro estructural de la red (en su arquitectura) en ambos modelos, AE y VAE. Recuerde que no es necesario que la estructura del *decoder* sea análoga a la del *encoder.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YAYlvfT-Nqhw"
   },
   "source": [
    "<a id=\"refs\"></a>\n",
    "## Referencias\n",
    "[1] zIP  \n",
    "[2] Word2vec\n",
    "[3] Encoder-decoder  \n",
    "[4] Image segmentation  \n",
    "[5] https://arxiv.org/pdf/1511.00561v2.pdf  "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Enunciado.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
