{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 281837: expected 25 fields, saw 34\\n'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemma</th>\n",
       "      <th>word</th>\n",
       "      <th>pos</th>\n",
       "      <th>tag</th>\n",
       "      <th>prev-iob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>thousand</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "      <td>__START1__</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>demonstr</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>have</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>march</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      lemma           word  pos tag    prev-iob\n",
       "0  thousand      Thousands  NNS   O  __START1__\n",
       "1        of             of   IN   O           O\n",
       "2  demonstr  demonstrators  NNS   O           O\n",
       "3      have           have  VBP   O           O\n",
       "4     march        marched  VBN   O           O"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "df_ner = pd.read_csv(\"./ner.csv\", encoding =\"cp1252\", error_bad_lines=False)\n",
    "df_ner.dropna(inplace=True)\n",
    "dataset = df_ner.loc[:,[\"lemma\",\"word\",\"pos\",\"tag\",\"prev-iob\"]]\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_used = 100000 #data to use\n",
    "dataX,dataY = [],[]\n",
    "sentence, labels_sentence = [], []\n",
    "lemmas,labels = set(), set()  #uniques\n",
    "for fila in dataset.values[:n_used]:\n",
    "    if fila[-1]==\"__START1__\": \n",
    "        dataX.append(sentence)\n",
    "        dataY.append(labels_sentence)\n",
    "        sentence= []\n",
    "        labels_sentence = []\n",
    "    lemmas.add(fila[0])\n",
    "    labels.add(fila[3])\n",
    "    sentence.append(fila[0]) #add lemma\n",
    "    labels_sentence.append(fila[3]) #TAG\n",
    "dataX = dataX[1:]\n",
    "dataY = dataY[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_labels = len(labels)\n",
    "lab2idx = {t: i for i, t in enumerate(labels)}\n",
    "dataY = [[lab2idx[ner] for ner in ner_tags ] for ner_tags in dataY]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x7fea08e1cba8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "EMBEDDING_DIM = 32\n",
    "window_size = 5\n",
    "nb_epoch = 5\n",
    "batch_size = 6000\n",
    "model = Word2Vec(dataX, size=EMBEDDING_DIM, window=window_size,batch_words=batch_size,iter=nb_epoch,\n",
    "                 min_count=3, negative=5,sg=1) #sg=1 mean skip-gram\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamano del vocabulario:  3144\n"
     ]
    }
   ],
   "source": [
    "vocab_words = list(model.wv.vocab)\n",
    "print(\"Tamano del vocabulario: \",len(vocab_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embeddings_index = {vocab_word: model.wv[vocab_word] for vocab_word in model.wv.vocab}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma2idx = {w: i for i, w in enumerate(lemmas)} #Converting text to numbers\n",
    "n_lemmas = len(lemmas)\n",
    "\n",
    "dataX = [[lemma2idx[lemma] for lemma in sentence ] for sentence in dataX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       ..., \n",
       "       [ 0.05347466,  0.0790691 , -0.12822857, ..., -0.26420334,\n",
       "        -0.17218229,  0.02646903],\n",
       "       [ 0.10988804,  0.10135647, -0.17103973, ..., -0.37504655,\n",
       "        -0.27834073,  0.06954294],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((n_lemmas+1, EMBEDDING_DIM))\n",
    "for word, i in lemma2idx.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: #if word does not has embedding\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4543, 34)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7513, 32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3180, 34)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7513, 32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_lemmas+=1\n",
    "n_labels+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 34, 32)            240416    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 34, 100)           53200     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 34, 100)           0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 34, 18)            1818      \n",
      "=================================================================\n",
      "Total params: 295,434\n",
      "Trainable params: 55,018\n",
      "Non-trainable params: 240,416\n",
      "_________________________________________________________________\n",
      "Train on 3180 samples, validate on 1363 samples\n",
      "Epoch 1/10\n",
      "3180/3180 [==============================] - 4s 1ms/step - loss: 2.0487 - val_loss: 1.6018\n",
      "Epoch 2/10\n",
      "3180/3180 [==============================] - 2s 547us/step - loss: 1.3990 - val_loss: 1.1465\n",
      "Epoch 3/10\n",
      "3180/3180 [==============================] - 2s 542us/step - loss: 0.9803 - val_loss: 0.8374\n",
      "Epoch 4/10\n",
      "3180/3180 [==============================] - 2s 544us/step - loss: 0.7902 - val_loss: 0.7269\n",
      "Epoch 5/10\n",
      "3180/3180 [==============================] - 2s 551us/step - loss: 0.7102 - val_loss: 0.6694\n",
      "Epoch 6/10\n",
      "3180/3180 [==============================] - 2s 575us/step - loss: 0.6625 - val_loss: 0.6331\n",
      "Epoch 7/10\n",
      "3180/3180 [==============================] - 2s 552us/step - loss: 0.6312 - val_loss: 0.6035\n",
      "Epoch 8/10\n",
      "3180/3180 [==============================] - 2s 554us/step - loss: 0.6030 - val_loss: 0.5791\n",
      "Epoch 9/10\n",
      "3180/3180 [==============================] - 2s 551us/step - loss: 0.5812 - val_loss: 0.5590\n",
      "Epoch 10/10\n",
      "3180/3180 [==============================] - 2s 553us/step - loss: 0.5619 - val_loss: 0.5409\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe9d9f69be0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Embedding, Dense, Dropout\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=n_lemmas, output_dim=EMBEDDING_DIM, input_length=34,trainable=False,\n",
    "                   weights = [embedding_matrix]))\n",
    "model.add(LSTM(units=100,return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(n_labels, activation='softmax'))\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train[:,:,:,None] #add channels\n",
    "x_test = x_test[:,:,:,None]\n",
    "img_rows, img_cols,channel = x_train.shape[1:]\n",
    "original_img_size = (img_rows, img_cols,channel) # input image dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32') / 255. #and x_test\n",
    "x_test = x_test.astype('float32') / 255. #and x_test\n",
    "\n",
    "#...#Define here your validation set\n",
    "import keras\n",
    "Y_train = keras.utils.to_categorical(y_train, 10)\n",
    "Y_test = keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      " 5216/60000 [=>............................] - ETA: 8s - loss: 0.2853"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-f4cbe56a4dbc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rmsprop'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'basic_autoencoder.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py3/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1040\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda2/envs/py3/lib/python3.5/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py3/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2659\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2661\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2662\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2663\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py3/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2630\u001b[0m                                 session)\n\u001b[0;32m-> 2631\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2632\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[0;32m-> 1451\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Flatten,Reshape\n",
    "from keras.models import Model\n",
    "input_img = Input(shape=original_img_size)\n",
    "input_fl = Flatten()(input_img) #to get a vector representation\n",
    "encoded = Dense(32, activation='relu')(input_fl)\n",
    "decoded = Dense(np.prod(original_img_size), activation='sigmoid')(encoded)\n",
    "decoded = Reshape(original_img_size)(decoded)\n",
    "autoencoder = Model(inputs=input_img, outputs=decoded)\n",
    "encoder = Model(inputs=input_img, outputs=encoded)\n",
    "autoencoder.compile(optimizer='rmsprop', loss='binary_crossentropy')\n",
    "autoencoder.fit(x_train,x_train,epochs=30,batch_size=32,validation_data=(x_test,x_test))\n",
    "autoencoder.save('basic_autoencoder.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_26 (InputLayer)        (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 28, 28, 16)        160       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling (None, 14, 14, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_30 (Conv2D)           (None, 14, 14, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling (None, 7, 7, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_20 (Flatten)         (None, 1568)              0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 32)                50208     \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 1568)              51744     \n",
      "_________________________________________________________________\n",
      "reshape_18 (Reshape)         (None, 7, 7, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_31 (Conv2D)           (None, 7, 7, 32)          9248      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_5 (UpSampling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_32 (Conv2D)           (None, 14, 14, 16)        4624      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_6 (UpSampling2 (None, 28, 28, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_33 (Conv2D)           (None, 28, 28, 1)         145       \n",
      "=================================================================\n",
      "Total params: 120,769\n",
      "Trainable params: 120,769\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      " 4928/60000 [=>............................] - ETA: 1:17 - loss: 0.2702"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-05eb793e6b54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rmsprop'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda2/envs/py3/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1040\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda2/envs/py3/lib/python3.5/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py3/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2659\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2661\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2662\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2663\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py3/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2630\u001b[0m                                 session)\n\u001b[0;32m-> 2631\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2632\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[0;32m-> 1451\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.layers import Input,Conv2D,Flatten,Dense,MaxPooling2D, UpSampling2D\n",
    "input_img = Input(shape=original_img_size)\n",
    "x = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "before_F_shape =  (x.shape[1].value, x.shape[2].value, x.shape[3].value)\n",
    "x = Flatten()(x)\n",
    "encoded = Dense(32, activation='relu')(x)\n",
    "x = Dense(np.prod(before_F_shape),activation='relu')(encoded)\n",
    "x = Reshape(before_F_shape)(x)\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(16, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "autoencoder = Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='rmsprop', loss='binary_crossentropy')\n",
    "autoencoder.summary()\n",
    "autoencoder.fit(x_train,x_train,epochs=30,batch_size=32,validation_data=(x_test,x_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 7, 32)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 784)               25872     \n",
      "_________________________________________________________________\n",
      "reshape_6 (Reshape)          (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 50,992\n",
      "Trainable params: 50,992\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa03ea0b7f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#encoded_test = encoder.predict(x_test)\n",
    "#decoded_test = decoder.predict(encoded_test)\n",
    "decoded_test = autoencoder.predict(x_test)\n",
    "import matplotlib.pyplot as plt\n",
    "n = 10\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28),cmap='gray')\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_test[i].reshape(28, 28),cmap='gray')\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input,Conv2D,Flatten,Dense,MaxPool2D\n",
    "from keras.models import Model\n",
    "filters = 32 # number of convolutional filters to use\n",
    "num_conv = 3 # convolution kernel size\n",
    "intermediate_dim = 128\n",
    "\n",
    "#cambia\n",
    "latent_dim = 10\n",
    "\n",
    "x = Input(shape=original_img_size)\n",
    "conv_1 = Conv2D(filters,kernel_size=num_conv,padding='same', activation='relu')(x)\n",
    "conv_2 = Conv2D(filters,kernel_size=num_conv,padding='same', activation='relu')(conv_1)\n",
    "conv_3 = Conv2D(filters*2, kernel_size=num_conv, padding='same', activation='relu', strides=2)(conv_2)\n",
    "flat = Flatten()(conv_3)\n",
    "hidden = Dense(intermediate_dim, activation='relu')(flat)\n",
    "\n",
    "#cambio..\n",
    "logits_z = Dense(latent_dim,activation='linear')(hidden) #log(p(z))\n",
    "encoder = Model(x, logits_z) # build a model to project inputs on the latent space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from keras.layers import Reshape,Conv2DTranspose,Activation\n",
    "shape_before_flattening = K.int_shape(conv_3)[1:] # we instantiate these layers separately to reuse them later\n",
    "decoder_hid = Dense(intermediate_dim, activation='relu')\n",
    "decoder_upsample = Dense(np.prod(shape_before_flattening), activation='relu')\n",
    "decoder_reshape = Reshape(shape_before_flattening)\n",
    "decoder_deconv_1 = Conv2DTranspose(filters,kernel_size=num_conv, padding='same',strides=2,activation='relu')\n",
    "decoder_deconv_2 = Conv2DTranspose(filters,kernel_size=num_conv,padding='same', activation='relu')\n",
    "decoder_mean_squash = Conv2DTranspose(channel, kernel_size=num_conv,padding='same', activation='sigmoid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 28, 28, 32)        9248      \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               1605760   \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "lambda_3 (Lambda)            (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 128)               1408      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 12544)             1618176   \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTr (None, 28, 28, 32)        18464     \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_5 (Conv2DTr (None, 28, 28, 32)        9248      \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_6 (Conv2DTr (None, 28, 28, 1)         289       \n",
      "=================================================================\n",
      "Total params: 3,282,699\n",
      "Trainable params: 3,282,699\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def sample_gumbel(shape,eps=K.epsilon()):\n",
    "    \"\"\"Inverse Sample function from Gumbel(0, 1)\"\"\"\n",
    "    U = K.random_uniform(shape, 0, 1)\n",
    "    return - K.log( -K.log(U + eps) + eps)\n",
    "        \n",
    "def sampling(logits_z):\n",
    "    \"\"\" Perform a Gumbel-Softmax sampling\"\"\"\n",
    "    tau = K.variable(2/3, name=\"temperature\") \n",
    "    z = logits_z + sample_gumbel(K.shape(logits_z)) # logits + gumbel noise\n",
    "    return keras.activations.softmax( z/tau )    \n",
    "\n",
    "from keras.layers import Lambda\n",
    "z = Lambda(sampling, output_shape=(latent_dim,))(logits_z)\n",
    "#para arriba cambio...\n",
    "\n",
    "hid_decoded = decoder_hid(z)\n",
    "up_decoded = decoder_upsample(hid_decoded)\n",
    "reshape_decoded =  decoder_reshape(up_decoded)\n",
    "deconv_1_decoded = decoder_deconv_1(reshape_decoded)\n",
    "x_decoded_relu = decoder_deconv_2(deconv_1_decoded)\n",
    "x_decoded_mean_squash = decoder_mean_squash(x_decoded_relu)\n",
    "vae_norm = Model(x, x_decoded_mean_squash) # instantiate VAE model\n",
    "vae_norm.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 28, 28, 32)        9248      \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               1605760   \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "lambda_3 (Lambda)            (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 128)               1408      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 12544)             1618176   \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTr (None, 28, 28, 32)        18464     \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_5 (Conv2DTr (None, 28, 28, 32)        9248      \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_6 (Conv2DTr (None, 28, 28, 1)         289       \n",
      "=================================================================\n",
      "Total params: 3,282,699\n",
      "Trainable params: 3,282,699\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "choised_loss =  keras.metrics.binary_crossentropy(K.flatten(x),K.flatten(x_decoded_mean_squash))\n",
    "reconstruction_loss = img_rows * img_cols * channel* choised_loss\n",
    "\n",
    "#KL cambia:\n",
    "dist =  keras.activations.softmax(logits_z) # =p(z)\n",
    "dist_neg_entropy = K.sum(dist * K.log(dist + K.epsilon()), axis=1)\n",
    "kl_disc_loss =  np.log(latent_dim) + dist_neg_entropy #discrete KL-loss\n",
    "vae_loss = K.mean(reconstruction_loss + kl_disc_loss)\n",
    "\n",
    "vae_norm.add_loss(vae_loss)\n",
    "vae_norm.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/casapanshop/anaconda2/envs/py3/lib/python3.5/site-packages/ipykernel/__main__.py:3: UserWarning: Output \"conv2d_transpose_6\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"conv2d_transpose_6\" during training.\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/25\n",
      "60000/60000 [==============================] - 18s 301us/step - loss: 138.1076 - val_loss: 138.6182\n",
      "Epoch 2/25\n",
      "60000/60000 [==============================] - 17s 280us/step - loss: 137.1350 - val_loss: 139.8848\n",
      "Epoch 3/25\n",
      "60000/60000 [==============================] - 17s 281us/step - loss: 136.3050 - val_loss: 135.7248\n",
      "Epoch 4/25\n",
      "60000/60000 [==============================] - 17s 280us/step - loss: 135.5400 - val_loss: 136.2309\n",
      "Epoch 5/25\n",
      "60000/60000 [==============================] - 17s 279us/step - loss: 134.9025 - val_loss: 134.9311\n",
      "Epoch 6/25\n",
      "60000/60000 [==============================] - 17s 279us/step - loss: 134.4601 - val_loss: 135.2788\n",
      "Epoch 7/25\n",
      "60000/60000 [==============================] - 17s 279us/step - loss: 133.7722 - val_loss: 133.9731\n",
      "Epoch 8/25\n",
      "60000/60000 [==============================] - 17s 282us/step - loss: 133.3055 - val_loss: 133.6667\n",
      "Epoch 9/25\n",
      "60000/60000 [==============================] - 17s 284us/step - loss: 132.7804 - val_loss: 133.1550\n",
      "Epoch 10/25\n",
      "60000/60000 [==============================] - 17s 286us/step - loss: 132.5055 - val_loss: 133.8186\n",
      "Epoch 11/25\n",
      "60000/60000 [==============================] - 17s 284us/step - loss: 132.0732 - val_loss: 132.5945\n",
      "Epoch 12/25\n",
      "60000/60000 [==============================] - 17s 278us/step - loss: 131.5256 - val_loss: 131.2716\n",
      "Epoch 13/25\n",
      "60000/60000 [==============================] - 17s 285us/step - loss: 131.3121 - val_loss: 130.8322\n",
      "Epoch 14/25\n",
      "60000/60000 [==============================] - 17s 279us/step - loss: 130.9246 - val_loss: 130.9984\n",
      "Epoch 15/25\n",
      "60000/60000 [==============================] - 17s 278us/step - loss: 130.4236 - val_loss: 130.8325\n",
      "Epoch 16/25\n",
      "60000/60000 [==============================] - 17s 278us/step - loss: 130.2755 - val_loss: 130.6602\n",
      "Epoch 17/25\n",
      "60000/60000 [==============================] - 16s 274us/step - loss: 129.9069 - val_loss: 131.0847\n",
      "Epoch 18/25\n",
      "60000/60000 [==============================] - 17s 279us/step - loss: 129.7133 - val_loss: 128.8709\n",
      "Epoch 19/25\n",
      "60000/60000 [==============================] - 17s 278us/step - loss: 129.4060 - val_loss: 130.3154\n",
      "Epoch 20/25\n",
      "60000/60000 [==============================] - 17s 276us/step - loss: 129.1874 - val_loss: 130.0773\n",
      "Epoch 21/25\n",
      "60000/60000 [==============================] - 16s 274us/step - loss: 129.0726 - val_loss: 129.4568\n",
      "Epoch 22/25\n",
      "60000/60000 [==============================] - 17s 275us/step - loss: 128.8686 - val_loss: 129.4143\n",
      "Epoch 23/25\n",
      "60000/60000 [==============================] - 17s 277us/step - loss: 128.3805 - val_loss: 128.0150\n",
      "Epoch 24/25\n",
      "60000/60000 [==============================] - 16s 274us/step - loss: 128.3933 - val_loss: 129.0224\n",
      "Epoch 25/25\n",
      "60000/60000 [==============================] - 17s 276us/step - loss: 128.0752 - val_loss: 127.7739\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdf4b1e2128>"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64\n",
    "epochs =  25\n",
    "vae_norm.compile(optimizer='rmsprop')\n",
    "vae_norm.fit(x_train,epochs=epochs, batch_size=batch_size,validation_data=(x_test, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.93214577e-09, 3.42115709e-05, 3.34711804e-05, ...,\n",
       "        9.18326557e-01, 7.88376778e-02, 1.22381802e-08],\n",
       "       [5.01510799e-02, 1.18385833e-04, 1.04509264e-01, ...,\n",
       "        2.30292767e-06, 2.50481698e-05, 2.21619487e-01],\n",
       "       [4.96984764e-09, 2.85342723e-01, 7.14640677e-01, ...,\n",
       "        8.22795391e-06, 9.44979983e-10, 4.77221238e-06],\n",
       "       ...,\n",
       "       [2.92682046e-07, 3.96850646e-01, 3.09091178e-03, ...,\n",
       "        4.07346815e-01, 6.24903696e-05, 1.12699415e-03],\n",
       "       [1.94816457e-05, 1.52378857e-06, 4.45192470e-08, ...,\n",
       "        6.37474470e-03, 8.71460512e-03, 2.05271999e-06],\n",
       "       [3.02969944e-04, 6.97063399e-07, 1.66004733e-03, ...,\n",
       "        5.49713430e-09, 4.10732959e-08, 1.04517244e-06]], dtype=float32)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - x.max(axis=-1,keepdims=True) )\n",
    "    return e_x / e_x.sum(axis=-1, keepdims=True)\n",
    "p_z_train = softmax(encoder.predict(x_train))\n",
    "p_z_test = softmax(encoder.predict(x_test))\n",
    "y_train_pred = p_z_train.argmax(axis=-1)\n",
    "y_test_pred = p_z_test.argmax(axis=-1)\n",
    "p_z_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.31587453023976225\n",
      "0.323335570035452\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "print(normalized_mutual_info_score(y_train, y_train_pred))\n",
    "print(normalized_mutual_info_score(y_test, y_test_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input = Input(shape=(latent_dim,))\n",
    "_hid_decoded = decoder_hid(decoder_input)\n",
    "_up_decoded = decoder_upsample(_hid_decoded)\n",
    "_reshape_decoded = decoder_reshape(_up_decoded)\n",
    "_deconv_1_decoded = decoder_deconv_1(_reshape_decoded)\n",
    "_x_decoded_relu = decoder_deconv_2(_deconv_1_decoded)\n",
    "_x_decoded_mean_squash = decoder_mean_squash(_x_decoded_relu)\n",
    "generator = Model(decoder_input, _x_decoded_mean_squash) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions =np.zeros((img_cols * 1 ,img_cols* latent_dim))\n",
    "for i in range(latent_dim):\n",
    "    activate_aux = np.zeros((1,10))\n",
    "    activate_aux[:,i] = 1 #activate a class\n",
    "    predictions[:, i * img_cols: (i + 1) * img_cols] = np.squeeze(generator.predict(activate_aux))\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(predictions, cmap='gnuplot2')\n",
    "pos = np.arange(img_cols/2, img_cols*latent_dim, img_cols)\n",
    "plt.xticks(pos,range(latent_dim))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://arxiv.org/pdf/1511.00561v2.pdf \n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "#ENCODER PART\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "#BN??\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D((2, 2), strides=(2,2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D((2, 2), strides=(2,2)))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D((2, 2), strides=(2,2)))\n",
    "\n",
    "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D((2, 2), strides=(2,2)))\n",
    "\n",
    "#quizas sacar..\n",
    "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "#model.add(MaxPooling2D((2, 2), strides=(2,2))) -- debe terminar sin pooling\n",
    "\n",
    "#reverse..\n",
    "\n",
    "#DECODER PART\n",
    "#model.add(UpSampling2d((2, 2), strides=(2,2)))\n",
    "model.add(Conv2DTranspose(256, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2DTranspose(256, (3, 3), activation='relu', padding='same'))\n",
    "\n",
    "model.add(UpSampling2d((2, 2), strides=(2,2)))\n",
    "model.add(Conv2DTranspose(256, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2DTranspose(256, (3, 3), activation='relu', padding='same'))\n",
    "\n",
    "model.add(UpSampling2d((2, 2), strides=(2,2)))\n",
    "model.add(Conv2DTranspose(128, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2DTranspose(128, (3, 3), activation='relu', padding='same'))\n",
    "\n",
    "model.add(UpSampling2d((2, 2), strides=(2,2)))\n",
    "model.add(Conv2DTranspose(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2DTranspose(64, (3, 3), activation='relu', padding='same'))\n",
    "\n",
    "model.add(UpSampling2d((2, 2), strides=(2,2)))\n",
    "model.add(Conv2DTranspose(32, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2DTranspose(32, (3, 3), activation='relu', padding='same'))\n",
    "\n",
    "model.add(UpSampling2d((2, 2), strides=(2,2)))\n",
    "#una que lleve a size original??\n",
    "\n",
    "model.add(Activation('softmax')) #debe ir softmax al final: pixel-wise classification\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3]",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
