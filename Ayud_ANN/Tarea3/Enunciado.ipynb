{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interesante**: traduccion http://www.mn.uio.no/ifi/english/research/groups/ltg/news/new-corpus-release%3A-opensubtitles-2016.html\n",
    "\n",
    "* 1 redes recurrentes en series de tiempo  \n",
    "\n",
    "* 2 Redes recurrentes sobre texto  \n",
    "    * quora (2 sentences) https://medium.com/mlreview/implementing-malstm-on-kaggles-quora-question-pairs-competition-8b31b0b16a07  \n",
    "    * clasificacion de texto con CNN - https://www.kaggle.com/eliotbarr/text-mining-with-sklearn-keras-mlp-lstm-cnn  con ese mismo dataset autocompletar (puede generar despues) https://chunml.github.io/ChunML.github.io/project/Creating-Text-Generator-Using-Recurrent-Neural-Network/\n",
    "https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/\n",
    "    * Annotation text https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus/data\n",
    "\n",
    "* 3 Autoencoder  \n",
    "* 4 regulerazacion en CNN -meter batch normalization con transferlearning\n",
    "\n",
    "\n",
    "IDEA:  \n",
    "1) RNN sobre series de tiempo (reutilizar)\n",
    "    * (academico) experimentar con las distintas formas de entrenar una red neuronal recurrente\n",
    "2) clasificaicon sobre texto\n",
    "    * CNN luego RNN (Dropout here)\n",
    "    * mismo dataset para autocompletar por caracter (puede generar despues)\n",
    "3) Autoencoder\n",
    "    * Autoencoder tradicional (reutilizar)\n",
    "    * Autoencoder sobre mismo dataset.. convolucional \n",
    "4) Transfer Learning\n",
    "    * mismo dataset de autoencoder\n",
    "    * utilizar autoencoder como pre entrenamiento\n",
    "    * utilizar una red ya entrenada de keras para algun problema dificil con pocos datos\n",
    "    * ver el efecto de batch normalization como adaptador (drift) sobre la escala de los datos y regularizador\n",
    "    \n",
    "Para cambiar batch size de red: https://machinelearningmastery.com/use-different-batch-sizes-training-predicting-python-keras/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.exalumnos.usm.cl/wp-content/uploads/2015/06/Isotipo-Negro.gif\" title=\"Title text\" width=\"20%\" height=\"20%\" />\n",
    "\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "<h1 align='center'> INF-395/477 Redes Neuronales Artificiales I-2018 </h1>\n",
    "\n",
    "<H3 align='center'> Tarea 2 - Redes Neuronales Convolucionales y Recurrentes </H3>\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "\n",
    "**Temas**  \n",
    "* Regularización en Redes Convolucionales.\n",
    "* Diseño e implementación de redes recurrentes (RNN) usando keras.\n",
    "* Diseño y entrenamiento de autoencoders (AEs)\n",
    "* Transfer Learning, pre-entrenamiento (*fine tunning*).\n",
    " \n",
    "\n",
    "** Formalidades **  \n",
    "* Equipos de trabajo de: 2 personas (*cada uno debe estar en condiciones de realizar una presentación y discutir sobre cada punto del trabajo realizado*)\n",
    "* Se debe preparar una presentación de 20 minutos. Presentador será elegido aleatoriamente.\n",
    "* Se debe preparar un (breve) Jupyter/IPython notebook que explique la actividad realizada y las conclusiones del trabajo\n",
    "* Fecha de entrega y discusión: ---\n",
    "* Formato de entrega: envı́o de link Github al correo electrónico del ayudante (*<francisco.mena.13@sansano.usm.cl>*) , incluyendo al profesor en copia (*<jnancu@inf.utfsm.cl>*). Por favor especificar el siguiente asunto: [Tarea1-INF395-I-2018]\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "\n",
    "La tarea se divide en secciones:\n",
    "\n",
    "[1.](#primero) Entrenamiento de RNNs en una Serie de Tiempo    \n",
    "[2.](#segundo) Redes recurrentes sobre texto  \n",
    "[3.](#tercero) Autoencoders (AEs) en MNIST  \n",
    "[4.](#cuarto) Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"primero\"></a>\n",
    "## 1. Entrenamiento de RNNs en una Serie de Tiempo\n",
    "En esta sección emplearemos redes neuronales recurrentes para modelar series de tiempo, es decir una serie\n",
    "de registros (tı́picamente valores reales) regularmente indexados en el tiempo. Para ello utilizaremos un dataset\n",
    "denominado \"*Minimum Daily Temperatures*\", el cual describe la temperatura mínima diaria en un período de 10 años (1981 a 1990) en la ciudad de Melbourne, Australia. Las unidades de las 3670 observaciones fueron medidas en grados celsius.  \n",
    "A continuación se muestra la secuencia de tiempo:\n",
    "<img src=\"https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2016/11/Minimum-Daily-Temperatures.png\" width=\"60%\" />\n",
    "\n",
    "\n",
    "\n",
    "La tarea consiste en predecir la temperatura mínima diaria de algún día basado en la información de días anteriores.  \n",
    "*La fuente es acreditada a Australian Bureau of Meteorology.*\n",
    "\n",
    "> a) Escriba una función que cargue los datos, los divida en 1500 de entrenamiento y el resto (500) de pruebas. Además de esto escalelos apropiadamente para trabajar con redes recurrentes.  \n",
    "```python\n",
    "name_f = \"time_series_data.csv\"\n",
    "dataframe = pd.read_csv(name_f,sep=',',usecols=[1],engine='python',skipfooter = 3)[:2000]\n",
    "dataframe[:] = dataframe[:].astype('float32')\n",
    "df_train, df_test = dataframe[:1500].values, dataframe[1500:].values\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0,1)).fit(df_train)\n",
    "stream_train_scaled = scaler.transform(df_train)\n",
    "stream_test_scaled = scaler.transform(df_test)\n",
    "```\n",
    "\n",
    "> b) Ahora nos gustarı́a manipular los datos, para que hagamos la predicción de la temperatura mínima para el tiempo siguiente usando la temperatura mínima de los últimos perı́odos de tiempo. El número de perı́odos de tiempos que usaremos se denomina *lag*. Por ejemplo, tendremos un *lag* igual a 3, si para predecir el valor $x_{t+1}$ en el tiempo siguiente usamos la información del tiempo actual $x_t$ y la de los dos perı́odos anteriores $x_{t-1}$ y $x_{t-2}$ como variables de entrada. Realice una función que reciba una secuencia de valores y la transforme en dos arreglos *dataX* (inputs) y *dataY* (targets) donde el número de caracterı́sticas de la la matriz de entrada (columnas) sea el número de tiempos que se considerarán como información (*lag*).\n",
    "```python\n",
    "def create_dataset(dataset,lag=1):\n",
    "    return np.array(dataX),np.array(dataY)\n",
    "```\n",
    "Por ejemplo si en el dataset tenemos el arreglo 20.7,17.9,18.8,14.6,15.8,15.8,10.1.\n",
    "```python\n",
    "create_dataset(dataset,3)\n",
    "```\n",
    "La función debiese generar $(X_1,X_2,X_3)$ e $Y$:\n",
    "\n",
    "\n",
    "|X0|X1|X2|Y|\n",
    "|---|---|---|---|\n",
    "|20.7|17.9|18.8|14.6|\n",
    "|17.9|18.8|14.6|15.8|\n",
    "|18.8|14.6|15.8|15.8|\n",
    "|14.6|15.8|15.8|10.1|\n",
    "\n",
    "\n",
    "> c) Usando la función anterior genere los conjuntos de entrenamiento y test para el problema.\n",
    "```python\n",
    "lag = 3\n",
    "trainX, trainY = create_dataset(stream_train_scaled, lag)\n",
    "testX, testY = create_dataset(stream_test_scaled, lag)\n",
    "```\n",
    "\n",
    "> d) En estos momentos tenemos nuestros datos en la forma [ejemplos, atributos]. Sin embargo, la red LSTM necesita que los datos se encuentren en un arreglo de tres dimensiones [*samples, time steps, features*]. Transforme el  train y test sets a la estructura deseada.\n",
    "```python\n",
    "trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "```\n",
    "\n",
    "> e) Entrene una LSTM usando un lag de 3\n",
    "```python\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "model = Sequential()\n",
    "model.add(LSTM(4, input_dim=lag, activation='tanh', inner_activation='sigmoid'))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(trainX, trainY, nb_epoch=25, batch_size=1, verbose=1)\n",
    "```\n",
    "\n",
    "> f) Realice las predicciones del modelo para los conjuntos de entrenamiento y prueba. Denormalice los datos para que el error pueda ser computado en la escala original.\n",
    "```python\n",
    "trainPredict = model.predict(trainX)\n",
    "trainPredict = scaler.inverse_transform(trainP\n",
    "trainY = scaler.inverse_transform([trainY])\n",
    "```\n",
    "\n",
    "> g) Compute el root mean square error (RMSE) sobre los conjuntos de entrenamiento y test.\n",
    "```python\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# calculate root mean squared error\n",
    "trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\n",
    "print('Train Score: %.2f RMSE' % (trainScore))\n",
    "testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n",
    "print('Test Score: %.2f RMSE' % (testScore))\n",
    "```\n",
    "\n",
    "> h) Grafique las predicciones del train y test set, y contrástelas con la serie de tiempo original.\n",
    "```python\n",
    "# shift train predictions for plotting\n",
    "trainPredictPlot = np.empty_like(dataframe.values)\n",
    "trainPredictPlot[:, :] = np.nan\n",
    "#\n",
    "trainPredictPlot[lag:len(trainPredict)+lag, :] = trainPredict\n",
    "# shift test predictions for plotting\n",
    "testPredictPlot = np.empty_like(dataframe.values)\n",
    "testPredictPlot[:, :] = np.nan\n",
    "#\n",
    "testPredictPlot[(len(trainPredict)+2*lag):, :] = testPredic\n",
    "```\n",
    "\n",
    "> i) En lugar de aumentar el número de dimensiones como el el paso e), entrene la red con un *timestep* de 3 (con dimensión de entrada 1). ¿Se produce una mejora del error? ¿Los tiempos de computación son comparables? Comente brevemente sobre cual es la forma correcta para aprovechar la información a través del tiempo, si con esta forma o la realizada en el paso e).\n",
    "```python\n",
    "trainX = numpy.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\n",
    "model = Sequential()\n",
    "model.add(LSTM(4, input_dim=1, activation='tanh', inner_activation='sigmoid'))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(trainX, trainY, nb_epoch=25, batch_size=1, verbose=1)\n",
    "```\n",
    "\n",
    "> j) Determine el parámetro del número de bloques para la LSTM de la pregunta e) o i), lo que le parezca mas sensato en base a lo analizado en la pregunta anterior. Utilice 5-fold *cross validation*, o bien, en su defecto, los datos restante no utilizados ni como entrenamiento ni como pruebas (los de índice 2000 hacia adelante) como conjunto fijo de validación.\n",
    "```python\n",
    "nb=range(4,13,2)\n",
    "model = Sequential()\n",
    "model.add(LSTM(nb=range(4,13,2), input_dim=lag or 1, activation='tanh', inner_activation='sigmoid'))\n",
    "model.add(Dense(1))\n",
    "```\n",
    "\n",
    "> k) Compare el desempeño de la red LSTM variando el lag de 1 a 4. Comente brevemente.\n",
    "\n",
    "> l) Usando un lag de 3, compare el desempeño de la LSTM con una red recurrente simple y una GRU. Comente sobre la convergencia y el tiempo de ejeución.\n",
    "```python\n",
    "from keras.layers import GRU\n",
    "from keras.layers import SimpleRNN\n",
    "GRU(output_dim, inner_init='orthogonal', activation='tanh')\n",
    "SimpleRNN(output_dim, inner_init='orthogonal',activation='tanh')\n",
    "```\n",
    "\n",
    "> m) Entrene la red LSTM con memoria entre batches.\n",
    "```python\n",
    "batch_size = 1\n",
    "model = Sequential()\n",
    "model.add(LSTM(4, batch_input_shape=(batch_size, lag, 1), stateful=True, return_sequences=True))\n",
    "model.add(LSTM(4, batch_input_shape=(batch_size, lag, 1), stateful=True))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "for i in range(25):\n",
    "    model.fit(trainX, trainY, nb_epoch=1, batch_size=batch_size, verbose=1, shuffle=False)\n",
    "    model.reset_states()\n",
    "```\n",
    "> n) Compare el resultado anterior usando un tamaño de batch de 3.\n",
    "> o) Construya una LSTM apilada, y compárela con la obtenida en m). Comente brevemente lo sucedido.\n",
    "```python\n",
    "model.add(LSTM(4, batch_input_shape=(batch_size, lag, 1), stateful=True, return_sequences=True))\n",
    "model.add(LSTM(4, batch_input_shape=(batch_size, lag, 1), stateful=True))\n",
    "trainPredict = model.predict(trainX, batch_size=batch_size)\n",
    "```\n",
    "> o) Construya una LSTM bidireccional, y compárela con la obtenida en i) y l). Comente brevemente lo sucedido y cual deberia ser la forma correcta de usar el parámetro *merge_mode* (concatenar, multiplicar, sumar o promediar). Además comente las transformaciones que sufre el patrón de entrada al pasar por las capas.\n",
    "```python\n",
    "batch_size = 1\n",
    "lstm_layer = LSTM(4, return_sequences=False)\n",
    "model.add(Bidirectional(lstm_layer, input_shape=(lag, 1), merge_mode='concat'))\n",
    "model.add(Dense(1, activation ='linear'))\n",
    "model.summary()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"segundo\"></a>\n",
    "## 2.  Redes recurrentes sobre texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"tercero\"></a>\n",
    "## 3. Autoencoders (AEs) en MNIST\n",
    "Como se ha discutido en clases, las RBM’s y posteriormente los AE’s (redes no supervisadas) fueron un componente crucial en el desarrollo de los modelos que entre 2006 y 2010 vigorizaron el área de las redes neuronales artificiales con logros notables de desempeño en diferentes tareas de aprendizaje automático. En esta sección aprenderemos a utilizar el más sencillo de estos modelos: un autoencoder o AE. Consideraremos tres aplicaciones clásicas: reducción de dimensionalidad, denoising y pre-entrenamiento. Con este objetivo en mente, utilizaremos un dataset denominado MNIST. Se trata de una colección de 70000 imágenes de 28 $\\times$ 28 pixeles correspondientes a dígitos manuscritos (números entre 0 y 9). En su versión tradicional, la colección se encuentra separada en dos subconjuntos: uno de entrenamiento de 60000 imágenes y otro de test de 10000 imágenes. La tarea consiste en construir un programa para que aprenda a identificar correctamente el dı́gito representado en la imagen\n",
    "\n",
    "\n",
    "> a) Escriba una función que cargue los datos desde el repositorio de keras, normalice las imágenes de modo que los pixeles queden en [0, 1], transforme las imágenes en vectores ($\\in {\\rm I\\!R}^{784}$) y devuelva tres subconjuntos disjuntos: uno de entrenamiento, uno de validación y uno de pruebas. Construya el conjunto de validación utilizando los últimos nval = 5000 casos del conjunto del entrenamiento. El conjunto de entrenamiento consistirá en las primeras 60000 − nval imágenes\n",
    "```python\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.astype('float32') / 255. #and x_test\n",
    "#Define here your validation set\n",
    "Y_train = np_utils.to_categorical(y_train, 10)\n",
    "Y_test = np_utils.to_categorical(y_test, 10)\n",
    "```\n",
    "\n",
    "### 3.1 Reducción de dimensionalidad\n",
    "Para esta primera sección se trabajará con un autoencoder tradicional (*feed forward*) en donde las capas de este son densas. Para esto se re estructuraran los datos de entradas en forma de vector, es decir la matriz de 28 $\\times$ 28 pasa a ser un vector de 784 componentes.\n",
    "\n",
    "```python\n",
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "```\n",
    "\n",
    "Una de las aplicaciones tı́picas de un AE es reducción de dimensionalidad, es decir, implementar una transformación $\\phi:{\\rm I\\!R}^d \\leftarrow {\\rm I\\!R}^{d'}$ de objetos representados originalmente por $d$ atributos en una nueva representación de $d'$ atributos, de modo tal que se preserve lo mejor posible la “información” original. Obtener tal representación es útil desde un punto de vista computacional (compresión) y estadı́stico (permite construir modelos con un menor número de parámetros libres). Un AE es una técnica de reducción de dimensionalidad no supervisada porque no hace uso de información acerca de las clases a las que pertenecen los datos de entrenamiento\n",
    "> a) Entrene un AE básico (1 capa escondida) para generar una representación de MNIST en $d'$= 2, 8, 32, 64 dimensiones. Justifique la elección de la función de pérdida a utilizar y del criterio de entrenamiento en general. Determine el porcentaje de compresión obtenido y el error de reconstrucción en cada caso. ¿Mejora el resultado si elegimos una función de activación **ReLU** para el Encoder? ¿Podrı́a utilizarse esta activación en el Decoder?\n",
    "```python\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "input_img = Input(shape=(784,))\n",
    "encoded = Dense(32, activation='sigmoid')(input_img)\n",
    "decoded = Dense(784, activation='sigmoid')(encoded)\n",
    "autoencoder = Model(input=input_img, output=decoded)\n",
    "encoder = Model(input=input_img, output=encoded)\n",
    "encoded_input = Input(shape=(32,))\n",
    "decoder_layer = autoencoder.layers[-1]\n",
    "decoder = Model(input=encoded_input, output=decoder_layer(encoded_input))\n",
    "##\n",
    "autoencoder.compile(optimizer=SGD(lr=1.0), loss='binary_crossentropy')\n",
    "autoencoder.fit(x_train,x_train,nb_epoch=50,batch_size=32,shuffle=True, validation_data=(x_val, x_val))\n",
    "autoencoder.save('basic_autoencoder_768x32.h5')\n",
    "#save other stuffs\n",
    "```\n",
    "\n",
    "> b) Compare visualmente la reconstrucción que logra hacer el autoencoder desde la representación en ${\\rm I\\!R}^{d'}$ para algunas imágenes del conjunto de pruebas. Determine si la percepción visual se corresponde con el error de reconstrucción observada. Comente.\n",
    "```python\n",
    "from keras.models import load_model\n",
    "autoencoder = load_model('basic_autoencoder_768x32.h5')\n",
    "#load other stuff ...\n",
    "encoded_test = encoder.predict(x_test)\n",
    "decoded_test = decoder.predict(encoded_test)\n",
    "import matplotlib\n",
    "n = 10\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28),cmap='gray')\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_test[i].reshape(28, 28),cmap='gray')\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "> c) Para verificar la calidad de la representación obtenida, implemente el clasificador denominado $kNN$ (k-nearest neighbor): dada una imagen $x$, el clasificador busca las k = 10 imágenes de entrenamiento más similares (de acuerdo a una distancia, e.g. euclidiana) y predice como clase, la etiqueta más popular entre las imágenes cercanas. Mida el error de pruebas obtenido construyendo este clasificador sobre la data reducida a través del autocnder comparando con la representación reducida obtenida vía PCA (una técnica clásica de reducción de dimensionalidad) utilizando el mismo número de dimensiones $d'$= 2, 4, 8, 16, 32. Considere tanto el error de reconstrucción como el desempeño en clasificación , además de comparar los tiempos medios de predicción en ambos escenarios.\n",
    "```python\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "pca = PCA(n_components=d)\n",
    "#PCA\n",
    "pca.fit(x_train)\n",
    "pca_train = pca.transform(x_train)\n",
    "pca_test = pca.transform(x_test)\n",
    "#AUTOENCODER\n",
    "encoded_train = encoder.predict(x_train)\n",
    "encoded_test = encoder.predict(x_test)\n",
    "#CLASIFICATION\n",
    "clf = KNeighborsClassifier(10)\n",
    "clf.fit(pca_train, y_train)\n",
    "print 'Classification Accuracy PCA %.2f' % clf.score(pca_test,y_test)\n",
    "clf = KNeighborsClassifier(10)\n",
    "clf.fit(encoded_train, y_train)\n",
    "print 'Classification Accuracy %.2f' % clf.score(encoded_test,y_test)\n",
    "```\n",
    "\n",
    "> d) Modifique el autoencoder básico construido en (a) para implementar un deep autoencoder (*deep AE*), es decir, un autoencoder con al menos dos capas ocultas. Demuestre experimentalmente que este autoencoder puede mejorar la compresión obtenida por PCA utilizando el mismo número de dimensiones $d'$ . Experimente con $d'$ =2, 4, 8, 16 y distintas profundidades (L = 2, 3, 4). Considere en esta comparación tanto el error de reconstrucción como el desempeño en clasificación (vı́a kNN) de cada representación. Comente.\n",
    "```python\n",
    "target_dim = 2 #try other and do a nice plot\n",
    "input_img = Input(shape=(784,))\n",
    "encoded1 = Dense(1000, activation='relu')(input_img)\n",
    "encoded2 = Dense(500, activation='relu')(encoded1)\n",
    "encoded3 = Dense(250, activation='relu')(encoded2)\n",
    "encoded4 = Dense(target_dim, activation='relu')(encoded3)\n",
    "decoded4 = Dense(250, activation='relu')(encoded4)\n",
    "decoded3 = Dense(500, activation='relu')(encoded3)\n",
    "decoded2 = Dense(1000, activation='relu')(decoded3)\n",
    "decoded1 = Dense(784, activation='sigmoid')(decoded2)\n",
    "autoencoder = Model(input=input_img, output=decoded1)\n",
    "encoder = Model(input=input_img, output=encoded3)\n",
    "autoencoder.compile(optimizer=SGD(lr=1.0), loss='binary_crossentropy')\n",
    "autoencoder.fit(x_train,x_train,nb_epoch=40,batch_size=32,validation_data=(x_val,x_val))\n",
    "autoencoder.save('my_autoencoder_768x1000x500x250x2.h5')\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "pca = PCA(n_components=target_dim)\n",
    "pca.fit(x_train)\n",
    "```\n",
    "\n",
    "> e) Elija algunas de las representaciones aprendidas anteriormente y visualı́celas usando la herramienta *TSNE* disponible en la librerı́a *sklearn*. Compare cualitativamente el resultado con aquel obtenido usando PCA con el mismo número de componentes\n",
    "```python\n",
    "nplot=5000 #warning: mind your memory!\n",
    "encoded_train = encoder.predict(x_train[:nplot])\n",
    "from sklearn.manifold import TSNE\n",
    "model = TSNE(n_components=2, random_state=0)\n",
    "encoded_train = model.fit_transform(encoded_train)\n",
    "plt.figure(figsize=(10, 10))\n",
    "colors={0:'b',1:'g',2:'r',3:'c',4:'m',5:'y',6:'k',7:'orange',8:'darkgreen',9:'maroon'}\n",
    "markers={0:'o',1:'+',2: 'v',3:'<',4:'>',5:'^',6:'s',7:'p',8:'*',9:'x'}\n",
    "for idx in xrange(0,nplot):\n",
    "    label = y_train[idx]\n",
    "    line = plt.plot(encoded_train[idx][0], encoded_train[idx][1],\n",
    "        color=colors[label], marker=markers[label], markersize=6)\n",
    "pca_train = pca.transform(x_train)\n",
    "encoded_train = pca_train[:nplot]\n",
    "... #plot PCA\n",
    "```\n",
    "\n",
    "> f) Modifique el autoencoder construido en (a) para trabajar directamente sobre las imágenes de MNIST, sin tratarlas como vectores de 784 atributos, sino como matrices de tamaño $1\\times28\\times28$. Es posible lograr este objetivo utilizando capas convolucionales para definir el Encoder y el Decoder, comente como sufre las transformaciones el patrón de entrada. Compare la calidad de la representación reducida obtenida por el nuevo autoencoder con aquella obtenida anteriormente utilizando el mismo número de dimensiones. Comente.\n",
    "```python\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = np.reshape(x_train, (len(x_train), 28, 28, 1)) #modify for th dim ordering\n",
    "x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))\n",
    "input_img = Input(shape=(28, 28, 1))\n",
    "x = Conv2D(16, 3, 3, activation='relu', border_mode='same')(input_img)\n",
    "x = MaxPooling2D((2, 2), border_mode='same')(x)\n",
    "x = Conv2D(8, 3, 3, activation='relu', border_mode='same')(x)\n",
    "encoded = MaxPooling2D((2, 2))(x)\n",
    "x = Conv2D(8, 3, 3, activation='relu', border_mode='same')(encoded)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(16, 3, 3, activation='relu', border_mode='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "decoded = Conv2D(1, 3, 3, activation='sigmoid', border_mode='same')(x)\n",
    "autoencoder = Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
    "autoencoder.summary()\n",
    "```\n",
    "\n",
    "### 3.2 Denoising\n",
    "Como se ha discutido en clases, un denoising autoencoder (dAE) es esencialmente un autoencoder entrenado para reconstruir ejemplos parcialmente corruptos. Varios autores han demostrado que mediante esta modificación simple es posible obtener representaciones más robustas y significativas que aquellas obtenidas por un AE básico. En esta sección exploraremos la aplicación más “natural” o “directa” del método.\n",
    "\n",
    "> a) Genere artificialmente una versión corrupta de las imágenes en MNIST utilizando el siguiente modelo de ruido (masking noise): si $x\\in {\\rm I\\!R}^d$ es una de las imágenes originales, la versión ruidosa $\\~{x}$ se obtiene como $\\~{x} = x \\odot \\xi$ donde $\\odot$ denota el producto de Hadamard (componente a componente) y $\\xi \\in {\\rm I\\!R}^d$ es un vector aleatorio binario con componentes *Ber(p)* independientes.\n",
    "```python\n",
    "from numpy.random import binomial\n",
    "noise_level = 0.1\n",
    "noise_mask = binomial(n=1,p=noise_level,size=x_train.shape)\n",
    "noisy_x_train = x_train*noise_mask\n",
    "noise_mask = binomial(n=1,p=noise_level,size=x_val.shape)\n",
    "noisy_x_val = x_val*noise_mask\n",
    "noise_mask = binomial(n=1,p=noise_level,size=x_test.shape)\n",
    "noisy_x_test = x_test*noise_mask\n",
    "```\n",
    "\n",
    "> b) Entrene un autoencoder para reconstruir las imágenes corruptas generadas en el ı́tem anterior. Mida el error de reconstrucción y evalúe cualitativamente (visualización de la imagen corrupta y reconstruida) el resultado para un subconjunto representativo de imágenes. Experimente diferentes valores de *p* en el rango (0, 1).\n",
    "```python\n",
    "# DEFINE YOUR AUTOENCODER AS BEFORE\n",
    "autoencoder.fit(noisy_x_train, x_train, nb_epoch=40, batch_size=32, validation_data=(noisy_x_val, x_val))\n",
    "```\n",
    "\n",
    "> c) Utilice estas imágenes intencionalmente corruptas para entrenar un AE con fines de reducción de dimensionalidad. Durante el entrenamiento, proceda exactamente como en (b), pero su objetivo no será hacer *denoising* sino obtener una representación comprimida de alta calidad de las imágenes originales. Al final del entrenamiento, mida el error de reconstrucción como el desempeño en clasificación (vı́a kNN como en la sección anterior) de la representación obtenida. Comente.\n",
    "\n",
    "### 3.3 Pre entrenamiento\n",
    "En esta sección utilizaremos un AE para pre-entrenar redes profundas. Como hemos discutido en clases, el efecto esperado es regularizar el modelo, posicionando el modelo de partida en una buena zona del espacio de parámetros.\n",
    "\n",
    "> a) Construya y entrene una red FF para clasificar las imágenes de MNIST. Utilice SGD básico con tasa de aprendizaje fija η = 0.01 y no más de 50 “*epochs*”. Para empezar, utilice una arquitectura $768 \\times 1000 \\times 1000 \\times 10$ y **funciones de activación sigmoidales**. Determine error de clasificación alcanzado por el modelo en el conjunto de test.\n",
    "```python\n",
    "rom keras.models import Sequential\n",
    "model = Sequential()\n",
    "model.add(Dense(1000, activation='sigmoid', input_shape=(784,)))\n",
    "model.add(Dense(1000, activation='sigmoid'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.summary()\n",
    "optimizer_ = SGD(lr=0.01)\n",
    "model.compile(optimizer=optimizer_,loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, Y_train,nb_epoch=50, batch_size=128, validation_data=(x_val, Y_val))\n",
    "model.save('ReluNet-768x1000x1000x10-NFT-50epochs.h5')\n",
    "#TRAINING CAN THEN BE RESUMED FROM THIS POINT :-)\n",
    "```\n",
    "\n",
    "> b) Construya y entrene una red neuronal profunda para clasificar las imágenes de MNIST utilizando la arquitectura propuesta en (a) y pre-entrenando los pesos de cada capa mediante un autoencoder básico. Proceda en modo clásico, es decir, entrenando en modo no-supervisado una capa a la vez y tomando como input de cada nivel la representación (entrenada) obtenida en el nivel anterior. Después del entrenamiento efectúe un entrenamiento supervisado convencional (*finetunning*) con 20 *epochs*. Compare los resultados de clasificación sobre el conjunto de pruebas con aquellos obtenidos en (a), sin pre-entrenamiento. Evalúe también los resultados antes del *finetunning*. Comente.\n",
    "*Hint: se recomienda que la tasa de aprendizaje en un *finetunning* sea pequeña.\n",
    "```python\n",
    "## Load and preprocess MNIST as usual\n",
    "###AUTOENCODER 1\n",
    "input_img1 = Input(shape=(784,))\n",
    "encoded1 = Dense(n_hidden_layer1,activation=activation_layer1)(input_img1)\n",
    "decoded1 = Dense(784, activation=decoder_activation_1)(encoded1)\n",
    "autoencoder1 = Model(input=input_img1, output=decoded1)\n",
    "encoder1 = Model(input=input_img1, output=encoded1)\n",
    "autoencoder1.compile(optimizer=optimizer_, loss=loss_)\n",
    "autoencoder1.fit(x_train, x_train, nb_epoch=epochs_, batch_size=batch_size_,\n",
    "    shuffle=True, validation_data=(x_val, x_val))\n",
    "encoded_input1 = Input(shape=(n_hidden_layer1,))\n",
    "autoencoder1.save('autoencoder_layer1.h5')\n",
    "encoder1.save('encoder_layer1.h5')\n",
    "###AUTOENCODER 2\n",
    "x_train_encoded1 = encoder1.predict(x_train) #FORWARD PASS DATA THROUGH FIRST ENCODER\n",
    "x_val_encoded1 = encoder1.predict(x_val)\n",
    "x_test_encoded1 = encoder1.predict(x_test)\n",
    "input_img2 = Input(shape=(n_hidden_layer1,))\n",
    "encoded2 = Dense(n_hidden_layer2, activation=activation_layer2)(input_img2)\n",
    "decoded2 = Dense(n_hidden_layer2, activation=decoder_activation_2)(encoded2)\n",
    "autoencoder2 = Model(input=input_img2, output=decoded2)\n",
    "encoder2 = Model(input=input_img2, output=encoded2)\n",
    "autoencoder2.compile(optimizer=optimizer_, loss=loss_)\n",
    "autoencoder2.fit(x_train_encoded1,x_train_encoded1,nb_epoch=epochs_,batch_size=batch_size_,\n",
    "    shuffle=True, validation_data=(x_val_encoded1, x_val_encoded1))\n",
    "encoded_input2 = Input(shape=(n_hidden_layer2,))\n",
    "autoencoder2.save('autoencoder_layer2.h5')\n",
    "encoder2.save('encoder_layer2.h5')\n",
    "#FINE TUNNING\n",
    "from keras.models import Sequential\n",
    "model = Sequential()\n",
    "model.add(Dense(n_hidden_layer1, activation=activation_layer1, input_shape=(784,)))\n",
    "model.layers[-1].set_weights(autoencoder1.layers[1].get_weights())\n",
    "model.add(Dense(n_hidden_layer2, activation=activation_layer2))\n",
    "model.layers[-1].set_weights(autoencoder2.layers[1].get_weights())\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.summary()\n",
    "model.compile(optimizer=optimizer_,loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, Y_train,nb_epoch=20, batch_size=128, validation_data=(x_val, Y_val))\n",
    "model.save('Net-768x1000x1000x10-finetunned.h5')\n",
    "```\n",
    "\n",
    "> c) Repita (b) usando funciones de activación **Tanh** y **ReLu**. Comente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"cuarto\"></a>\n",
    "## 4. Transfer Learning\n",
    "En esta sección se trabajará con el dataset anteriormente trabajado, CIFAR [3], pero en su versión mas fina en el cual se presentan 100 tipos de categorías a clasificar la imagen, no 10 como se usó en las actividades anteriores. La estructura es la misma, son 60000 imágenes RGB de 32 $\\times$ 32 píxeles separados en 50 mil de entrenamiento y 10 mil de pruebas.  \n",
    "Aquí se experimentará con el concepto de *transfer learning* el cual consta den transferir conocimiento de un dominio fuente (*source domain*) a un dominio objetivo (*target domain*). En redes neuronales existen muchas representaciones de esto, en común consta en pre inicializar los pesos de la red de alguna manera que no sea aletoria con alguna distribución, o en utilizar una representación generada a través de otra red.  \n",
    "\n",
    "*Nota: Para esta actividad si es que no se cuenta con GPU se recomienda utilizar el entorno virtual de __[Colaboratory - Google](https://colab.research.google.com/)__\n",
    "\n",
    "Para cargar los datos utilice el siguiente comando:\n",
    "```python\n",
    "from keras.datasets import cifar100\n",
    "(x_train, y_train), (x_test, y_test) = cifar100.load_data(label_mode='fine')\n",
    "```\n",
    "\n",
    "Normalice y transforme las etiquetas en *one hot* vector.\n",
    "```python\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes=100)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes=100)\n",
    "x_train = x_train/255.0\n",
    "x_test = x_test/255.0\n",
    "```\n",
    "\n",
    "> a) Entrene una red neuronal convolucional como se presenta en el código a continuación durante 15 *epochs*, realizando un gráfico de evolución de la función de pérdida y de la exactitud del algoritmo (*accuracy*) sobre ambos conjuntos, entrenamiento y pruebas. Comente sobre el tiempo de ejecución de este entrenamiento. Reporte el *accuracy* del modelo final sobre el conjunto de pruebas.\n",
    "```python\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D,MaxPooling2D,Dropout\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same',input_shape=x_train.shape[1:],activation='relu'))\n",
    "model.add(Conv2D(32, (3, 3),padding='same',activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(64, (3, 3),padding='same',activation='relu'))\n",
    "model.add(Conv2D(64, (3, 3),padding='same',activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024,activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "...#clasification\n",
    "model.summary()\n",
    "#train it\n",
    "optimizer_ = SGD(lr=0.01,momentum=0.9,decay=1e-6)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer_, metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, batch_size=128, nb_epoch=15, verbose=1, validation_data=(x_test, y_test))\n",
    "```\n",
    "\n",
    "> b) Debido al comportamiento de las curvas de entrenamiento, claramente se ve que se necesita un regularizador. Experimente utilizando Dropout con una tasa de 0.25 en las tandas convolucionales, elija donde situarlo, luego de la primera convolución, después de la segunda o solamente después del *pooling*. La idea es que se forme una idea de dónde conviene colocar el regularizador y porqué.\n",
    "\n",
    "> c) Como pre entrenamiento de redes neuronales de manera no supervisada se trabajará con un autoencoder convolucional, el cual no necesita etiqueta de los datos por lo que se puede aprovechar de transferir lo aprendido con datos sin conocer si pertenecen a la misma categoría o no.\n",
    "\n",
    "Pre entrenamiento a traves de autoencoder\n",
    "\n",
    "> d) Otra forma de hacer lo que se conoce como *transfer learning* es utilizar el conocimiento (los parámetros) aprendido por una red entrenada con millones de imágenes, y tomar estos parámetros como los pre entrenados. Para esto se utilizará el modelo VGG16 [7] proporcionado a través de la interfaz de keras. Visualice el modelo y sus 23 capas. Para esta instancia se utilizará todo lo aprendido por las capas convolucionales, es decir, se eliminan las capas densas del modelo y se agregan unas nuevas a ser entrenadas desde cero.\n",
    "```python\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Input,Flatten,Dropout\n",
    "from keras.optimizers import SGD\n",
    "from keras.applications import VGG16\n",
    "#LOAD PRETRAINED MODEL \n",
    "input_tensor=Input(shape=x_train.shape[1:])\n",
    "modelVGG = VGG16(weights='imagenet', include_top=False,input_tensor=input_tensor )\n",
    "features_train = modelVGG.predict(x_train)\n",
    "features_test = modelVGG.predict(x_test)\n",
    "modelVGG.summary()\n",
    "```\n",
    "\n",
    "> e) Entrene esta red agregando una capa densa de 1024 neuronas y seguido de un dropout de 0.5, finalmente es necesario agregar la capa de clasificación para las 100 clases. *Se recomienda que la tasa de aprendizaje sea pequeña*. Entrene unicamente por 10 *epochs* ¿Qué sucede?\n",
    "```python\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=features_train.shape[1:]))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "...#clasification\n",
    "model.compile(optimizer=optimizer_,loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(features_train, y_train,nb_epoch=0, batch_size=128,verbose=1,validation_data=(features_test,y_test))\n",
    "```\n",
    "\n",
    "> f) Agregue una capa de normalización (*Batch Normalization* [8]) de las activaciones en las capas densas, esto es, restar por la media del batch y dividir por la desviación estándar. Vuelva a entrenar el modelo con la misma configuración pero ahora por **15 *epochs***. Comente lo observado y compare las curvas de convergencia con los modelos anteriores ¿Por qué esto mejora a lo presentado en e)? Realice los mismos gráficos que en a) a través del número de *epochs* y comente sobre el tiempo de ejecución de este entrenamiento.\n",
    "```python\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=features_train.shape[1:]))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "...\n",
    "```\n",
    "\n",
    "> g) Anteriormente se dejaron fijas las capas de convolución de VGG16, ahora experimente comentando sobre la convergencia y el tiempo de ejecución el entrenar la última tanda de convoluciones de VGG16, es decir, tome como punto inicial los pesos pre entrenados de esta red en *Imagenet* y entrenelos para este problema.\n",
    "```python\n",
    "#LOAD PRETRAINED MODEL \n",
    "input_tensor=Input(shape=x_train.shape[1:])\n",
    "modelVGG = VGG16(weights='imagenet', include_top=False,input_tensor=input_tensor )\n",
    "salida_vgg = modelVGG.get_layer('block4_pool').output_shape\n",
    "model = Sequential()\n",
    "model.add(Conv2D(512,(3, 3),input_shape=salida_vgg[1:],activation='relu',padding='same'))\n",
    "model.add(Conv2D(512,(3, 3),activation='relu',padding='same'))\n",
    "model.add(Conv2D(512,(3, 3),activation='relu',padding='same'))\n",
    "model.add(MaxPooling2D((2, 2),strides=(2,2)))    \n",
    "##dense section\n",
    "model.add(Flatten())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(100, activation='softmax'))\n",
    "#delete last 4 layers of VGG16 and transfer the weight to new model\n",
    "modelVGG.layers.pop() #delete last maxpooling\n",
    "for i in np.arange(2,-1,-1):\n",
    "    last = modelVGG.layers.pop() #delete convolutional layers\n",
    "    model.layers[i].set_weights(last.get_weights())\n",
    "from keras.models import Model\n",
    "crop_modelVGG = Model(inputs=modelVGG.input, outputs=modelVGG.layers[-1].output)\n",
    "features_train = crop_modelVGG.predict(x_train)\n",
    "features_test = crop_modelVGG.predict(x_test)\n",
    "#train it\n",
    "model.compile(optimizer=optimizer_,loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(features_train, y_train,nb_epoch=15, batch_size=128,verbose=1,validation_data=(features_test,y_test))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"refs\"></a>\n",
    "## Referencias\n",
    "[1] Vincent, P., Larochelle, H., Lajoie, I., Bengio, Y., Manzagol, P. A. *Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion*. Journal of Machine Learning Research 11. pp 3371–3408, 2010.  \n",
    "[3]  Bishop, Christopher M. (1995), *Neural Networks for Pattern Recognition,* Clarendon Press.  \n",
    "[4] Krizhevsky, A., Hinton, G. (2009). Learning multiple layers of features from tiny images.  \n",
    "[5] *Scikit-learn: Machine Learning in Python.* http://scikit-learn.org/stable/  \n",
    "[6] referencia a convolutional autoencoder  \n",
    "[7] Simonyan, K., & Zisserman, A. (2014). *Very deep convolutional networks for large-scale image recognition.* arXiv preprint arXiv:1409.1556.\n",
    "[8] Ioffe, S., & Szegedy, C. (2015). Batch normalization: *Accelerating deep network training by reducing internal covariate shift*. arXiv preprint arXiv:1502.03167.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
