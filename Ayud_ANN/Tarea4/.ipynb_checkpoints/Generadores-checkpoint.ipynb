{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* VAE: https://blog.keras.io/building-autoencoders-in-keras.html\n",
    "* GAN: https://www.kdnuggets.com/2016/07/mnist-generative-adversarial-model-keras.html\n",
    "* GAN2: https://towardsdatascience.com/gan-by-example-using-keras-on-tensorflow-backend-1a6d515a60d0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imagens leidas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE\n",
    "entender un poco  \n",
    "\n",
    "Min la reconstrucción (aleatoria) con una medición de error (*mse* o binary cross entropy para datos entre 0 y 1) + una regularización que se impone que la distribución aprendida (z) sea una Normal con media bla y sigma bla\n",
    "\n",
    "FO (modelela?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###FEED FORWARD VAE\n",
    "import keras\n",
    "from keras.layers import Input,Dense,Lambda\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "\n",
    "batch_size = 16\n",
    "original_dim = X_train.shape[1:][0]\n",
    "intermediate_dim = 1000\n",
    "a## Encoder\n",
    "x = Input(batch_shape=(batch_size, original_dim))\n",
    "h = Dense(intermediate_dim, activation='relu')(x)\n",
    "z_mean = Dense(latent_dim)(h)\n",
    "z_log_sigma = Dense(latent_dim)(h)\n",
    "def sampling(args): #from normal\n",
    "    epsilon_std = 1\n",
    "    z_mean, z_log_sigma = args\n",
    "    epsilon = K.random_normal(shape=(batch_size, latent_dim),mean=0., stddev=epsilon_std)\n",
    "    return z_mean + K.exp(z_log_sigma) * epsilon\n",
    "\n",
    "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "# so you could write `Lambda(sampling)([z_mean, z_log_sigma])`\n",
    "z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_sigma])\n",
    "## Decoder\n",
    "decoder_h = Dense(intermediate_dim, activation='relu')\n",
    "decoder_mean = Dense(original_dim, activation='sigmoid')\n",
    "h_decoded = decoder_h(z)\n",
    "x_decoded_mean = decoder_mean(h_decoded)\n",
    "\n",
    "# end-to-end autoencoder\n",
    "vae = Model(x, x_decoded_mean)\n",
    "\n",
    "# encoder, from inputs to latent space\n",
    "encoder = Model(x, z_mean)\n",
    "\n",
    "# generator, from latent space to reconstructed inputs\n",
    "decoder_input = Input(shape=(latent_dim,))\n",
    "_h_decoded = decoder_h(decoder_input)\n",
    "_x_decoded_mean = decoder_mean(_h_decoded)\n",
    "generator = Model(decoder_input, _x_decoded_mean)\n",
    "\n",
    "def vae_loss(x, x_decoded_mean):\n",
    "    xent_loss = keras.metrics.binary_crossentropy(x, x_decoded_mean)\n",
    "    kl_loss = - 0.5 * K.mean(1 + z_log_sigma - K.square(z_mean) - K.exp(z_log_sigma), axis=-1) #entre z y N(0,1)\n",
    "    return xent_loss + kl_loss\n",
    "\n",
    "vae.compile(optimizer='rmsprop', loss=vae_loss)\n",
    "\n",
    "vae.summary()\n",
    "\n",
    "vae.fit(X_train, X_train,\n",
    "        shuffle=True,\n",
    "        epochs=10,\n",
    "        batch_size=batch_size,validation_data=(X_test, X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input image dimensions\n",
    "img_rows, img_cols, img_chns = 100, 100, 3\n",
    "original_img_size = (img_rows, img_cols,img_chns)\n",
    "\n",
    "# number of convolutional filters to use\n",
    "filters = 32\n",
    "# convolution kernel size\n",
    "num_conv = 3\n",
    "batch_size = 10\n",
    "\n",
    "latent_dim = 32\n",
    "intermediate_dim = 256\n",
    "epsilon_std = 1.0\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Encoder\n",
    "x = Input(shape=original_img_size)\n",
    "conv_1 = Conv2D(filters,kernel_size=(2, 2),padding='same', activation='relu',strides=(2, 2))(x)\n",
    "conv_2 = Conv2D(filters*2, kernel_size=(2, 2),padding='same', activation='relu',strides=1)(conv_1)\n",
    "#conv_3 = Conv2D(filters, kernel_size=num_conv, padding='same', activation='relu', strides=1)(conv_2)\n",
    "conv_4 = Conv2D(filters*3,kernel_size=num_conv, padding='same', activation='relu', strides=(2,2))(conv_2)\n",
    "flat = Flatten()(conv_4)\n",
    "hidden = Dense(intermediate_dim, activation='relu')(flat)\n",
    "\n",
    "z_mean = Dense(latent_dim)(hidden)\n",
    "z_log_var = Dense(latent_dim)(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(args):\n",
    "    epsilon_std = 1.0\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim),mean=0., stddev=epsilon_std)\n",
    "    return z_mean + K.exp(z_log_var) * epsilon\n",
    "\n",
    "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "# so you could write `Lambda(sampling)([z_mean, z_log_var])`\n",
    "z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Decoder\n",
    "\n",
    "# we instantiate these layers separately so as to reuse them later\n",
    "decoder_hid = Dense(intermediate_dim, activation='relu')\n",
    "decoder_upsample = Dense(filters*3 * img_rows/4 * img_cols/4, activation='relu')\n",
    "\n",
    "output_shape = (batch_size, img_rows/4, img_cols/4, filters*3)\n",
    "decoder_reshape = Reshape(output_shape[1:])\n",
    "\n",
    "decoder_deconv_1 = Conv2DTranspose(filters*2,kernel_size=num_conv,padding='same',strides=(2,2), activation='relu')\n",
    "#decoder_deconv_2 = Conv2DTranspose(filters,kernel_size=num_conv,padding='same',strides=1, activation='relu')\n",
    "\n",
    "output_shape = (batch_size, 29, 29, filters)\n",
    "#decoder_deconv_3_upsamp = Conv2DTranspose(filters,kernel_size=(3, 3),strides=(2, 2), padding='valid',activation='relu')\n",
    "decoder_deconv_3_upsamp = Conv2DTranspose(filters,kernel_size=(2, 2),strides=1, padding='same',activation='relu')\n",
    "\n",
    "decoder_mean_squash = Conv2DTranspose(img_chns, kernel_size=(2,2), strides=(2,2),padding='valid', activation='sigmoid')\n",
    "\n",
    "hid_decoded = decoder_hid(z)\n",
    "up_decoded = decoder_upsample(hid_decoded)\n",
    "reshape_decoded = decoder_reshape(up_decoded)\n",
    "deconv_1_decoded = decoder_deconv_1(reshape_decoded)\n",
    "#deconv_2_decoded = decoder_deconv_2(deconv_1_decoded)\n",
    "x_decoded_relu = decoder_deconv_3_upsamp(deconv_1_decoded)\n",
    "x_decoded_mean_squash = decoder_mean_squash(x_decoded_relu)\n",
    "#modelo generador:\n",
    "decoder_input = Input(shape=(latent_dim,))\n",
    "generator = Model(decoder_input, _x_decoded_mean_squash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate VAE model\n",
    "vae = Model(x, x_decoded_mean_squash)\n",
    "\n",
    "# Compute VAE loss\n",
    "xent_loss = keras.metrics.binary_crossentropy(K.flatten(x),K.flatten(x_decoded_mean_squash))\n",
    "kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "vae_loss = K.mean(xent_loss + kl_loss)\n",
    "vae.add_loss(vae_loss)\n",
    "\n",
    "vae.compile(optimizer='rmsprop')\n",
    "vae.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.fit(X_train,\n",
    "        shuffle=True,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(X_test, None))\n",
    "#se demoraba 200 segs por epoc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a digit generator that can sample from the learned distribution\n",
    "##NOES NECESARIO SI SE CREA EL MODELO ARRIBA\n",
    "decoder_input = Input(shape=(latent_dim,))\n",
    "_hid_decoded = decoder_hid(decoder_input)\n",
    "_up_decoded = decoder_upsample(_hid_decoded)\n",
    "_reshape_decoded = decoder_reshape(_up_decoded)\n",
    "_deconv_1_decoded = decoder_deconv_1(_reshape_decoded)\n",
    "#_deconv_2_decoded = decoder_deconv_2(_deconv_1_decoded)\n",
    "_x_decoded_relu = decoder_deconv_3_upsamp(_deconv_1_decoded)\n",
    "_x_decoded_mean_squash = decoder_mean_squash(_x_decoded_relu)\n",
    "generator = Model(decoder_input, _x_decoded_mean_squash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5  # figure with 15x15 images\n",
    "image_size = 100\n",
    "figure = np.zeros((image_size * n, image_size * n,3))\n",
    "\n",
    "from scipy.stats import norm\n",
    "grid_x = norm.ppf(np.linspace(0.05, 0.95, n))\n",
    "grid_y = norm.ppf(np.linspace(0.05, 0.95, n))\n",
    "\n",
    "#grid_x = np.linspace(-15, 15, n)\n",
    "#grid_y = np.linspace(-15, 15, n)\n",
    "        \n",
    "for i, yi in enumerate(grid_x):\n",
    "    for j, xi in enumerate(grid_y):\n",
    "        #zmean_random = np.random.rand(1)*30-15\n",
    "        #zlogsigma_random = np.random.rand(1)*30-15\n",
    "        #z_sample = np.array([[zmean_random, zlogsigma_random]])\n",
    "        \n",
    "        a = norm.ppf(np.linspace(0.05, 0.95, 32))\n",
    "        b = norm.ppf(np.linspace(0.05, 0.95, 32))\n",
    "        \n",
    "        z_sample = np.array([a,b])\n",
    "        \n",
    "        #z_sample = np.array([[xi, yi]])\n",
    "\n",
    "        #z_sample = np.tile(z_sample, batch_size).reshape(batch_size, 2)\n",
    "            \n",
    "        \n",
    "        x_decoded = generator.predict(z_sample,batch_size=batch_size)\n",
    "        \n",
    "        #image = x_decoded[0].reshape(3,image_size, image_size)\n",
    "        #image = image.transpose([1,2,0])\n",
    "        figure[i * image_size: (i + 1) * image_size,\n",
    "               j * image_size: (j + 1) * image_size,:] = x_decoded[0]\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.imshow(figure)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN\n",
    "\n",
    "entender un poco  \n",
    "\n",
    "un prior en $p_z$\n",
    "\n",
    "MAX para D la probabilidad que un dato viene de la distribución original de los datos minimizando la probabilidad de que un dato viene de una distribución corrupta  \n",
    "MAX para G la probabilidad de que D asigne un dato de G como de los datos reales==Min para G la probabilidad de que D asigne un dato de G como corrupto\n",
    "\n",
    "$$\n",
    "min_G \\ max_G = E_{x\\sim p_{data}(x) }[logD(x)] + E_{z\\sim p_z(z)}[log(1-D(G(z))]\n",
    "$$\n",
    "\n",
    "En el paper muestran qu el óptimo tórico está en $p_g = p_{data}$\n",
    "\n",
    "El entrenamiento es iterativo, primero D con G fixed, luego G con D fixed, como un minimax game le llaman."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LeakyReLU\n",
    "## Discriminator\n",
    "D = Sequential()\n",
    "depth = 64\n",
    "dropout = 0.4\n",
    "# In: 28 x 28 x 1, depth = 1\n",
    "# Out: 14 x 14 x 1, depth=64\n",
    "img_rows = img_cols = 100\n",
    "channel=3\n",
    "input_shape = (img_rows, img_cols, channel)\n",
    "D.add(Conv2D(depth*1, (5,5), strides=2, input_shape=input_shape,padding='same', activation=LeakyReLU(alpha=0.2)))\n",
    "D.add(Dropout(dropout))\n",
    "D.add(Conv2D(depth*2, (5,5), strides=2, padding='same',activation=LeakyReLU(alpha=0.2)))\n",
    "D.add(Dropout(dropout))\n",
    "D.add(Conv2D(depth*4, (5,5), strides=2, padding='same',activation=LeakyReLU(alpha=0.2)))\n",
    "D.add(Dropout(dropout))\n",
    "D.add(Conv2D(depth*8, (5,5), strides=1, padding='same',activation=LeakyReLU(alpha=0.2)))\n",
    "D.add(Dropout(dropout))\n",
    "# Out: 1-dim probability\n",
    "D.add(Flatten())\n",
    "D.add(Dense(1))\n",
    "D.add(Activation('sigmoid'))\n",
    "D.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import BatchNormalization,Reshape,UpSampling2D,Conv2DTranspose\n",
    "\n",
    "## Generator\n",
    "G = Sequential()\n",
    "dropout = 0.4\n",
    "#layers = 4\n",
    "dim = 25 #img_cols/layers\n",
    "depth = 64*4\n",
    "#_,dim,dim,depth = D.layers[-5].output_shape #for tensorflow dim #64+64+64+64\n",
    "# In: 100\n",
    "# Out: dim x dim x depth\n",
    "G.add(Dense(dim*dim*depth, input_dim=100))\n",
    "G.add(BatchNormalization(momentum=0.9))\n",
    "G.add(Activation('relu'))\n",
    "G.add(Reshape((dim, dim, depth)))\n",
    "G.add(Dropout(dropout))\n",
    "# In: dim x dim x depth\n",
    "# Out: 2*dim x 2*dim x depth/2\n",
    "G.add(UpSampling2D())\n",
    "G.add(Conv2DTranspose(int(depth/2), (5,5), padding='same'))\n",
    "G.add(BatchNormalization(momentum=0.9))\n",
    "G.add(Activation('relu'))\n",
    "G.add(UpSampling2D())\n",
    "G.add(Conv2DTranspose(int(depth/4), (5,5), padding='same'))\n",
    "G.add(BatchNormalization(momentum=0.9))\n",
    "G.add(Activation('relu'))\n",
    "#G.add(UpSampling2D())\n",
    "G.add(Conv2DTranspose(int(depth/8), (5,5), padding='same'))\n",
    "G.add(BatchNormalization(momentum=0.9))\n",
    "G.add(Activation('relu'))\n",
    "# Out: 28 x 28 x 1 grayscale image [0.0,1.0] per pix\n",
    "G.add(Conv2DTranspose(channel, (5,5), padding='same'))\n",
    "G.add(Activation('sigmoid'))\n",
    "G.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import RMSprop\n",
    "## Discriminator model (police) o directamente ocupar D (no porke se usa en AM)\n",
    "optimizer = RMSprop(lr=0.0008, clipvalue=1.0, decay=6e-8)\n",
    "DM = Sequential()\n",
    "DM.add(D)\n",
    "DM.compile(loss='binary_crossentropy', optimizer=optimizer,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adversarial model (Generator->Discriminator)\n",
    "D.trainable=False #set the discriminator freeze  (fixed params)\n",
    "optimizer = RMSprop(lr=0.0004, clipvalue=1.0, decay=3e-8)\n",
    "AM = Sequential()\n",
    "AM.add(G)\n",
    "AM.add(D)\n",
    "AM.compile(loss='binary_crossentropy', optimizer=optimizer,metrics=['accuracy'])\n",
    "AM.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=8\n",
    "#sample images from real data\n",
    "images_train = X_train[np.random.randint(0,X_train.shape[0], size=batch_size), :, :, :]\n",
    "\n",
    "#sample image from generated data\n",
    "noise = np.random.uniform(-1.0, 1.0, size=[batch_size, 100])\n",
    "images_fake = G.predict(noise) #se actualizara por ref?\n",
    "\n",
    "x = np.concatenate((images_train, images_fake))\n",
    "\n",
    "#create labels\n",
    "y = np.ones([2*batch_size, 1])\n",
    "y[batch_size:, :] = 0\n",
    "\n",
    "#train discriminator\n",
    "d_loss = DM.train_on_batch(x, y)\n",
    "\n",
    "#train generator (or adversarial)\n",
    "y = np.ones([batch_size, 1])\n",
    "noise = np.random.uniform(-1.0, 1.0, size=[batch_size, 100])\n",
    "a_loss = AM.train_on_batch(noise, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_input = None\n",
    "if save_interval>0:\n",
    "    noise_input = np.random.uniform(-1.0, 1.0, size=[16, 100])\n",
    "for i in range(train_steps):\n",
    "    ##CODIGO DE ARRIBA\n",
    "    \n",
    "    log_mesg = \"%d: [D loss: %f, acc: %f]\" % (i, d_loss[0], d_loss[1])\n",
    "    log_mesg = \"%s  [A loss: %f, acc: %f]\" % (log_mesg, a_loss[0], a_loss[1])\n",
    "    print(log_mesg)\n",
    "    if save_interval>0:\n",
    "        if (i+1)%save_interval==0:\n",
    "            self.plot_images(save2file=True, samples=noise_input.shape[0],\\\n",
    "                noise=noise_input, step=(i+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'mnist.png'\n",
    "if fake:\n",
    "    if noise is None:\n",
    "        noise = np.random.uniform(-1.0, 1.0, size=[samples, 100])\n",
    "    else:\n",
    "        filename = \"mnist_%d.png\" % step\n",
    "    images = self.generator.predict(noise)\n",
    "else:\n",
    "    i = np.random.randint(0, self.x_train.shape[0], samples)\n",
    "    images = self.x_train[i, :, :, :]\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(images.shape[0]):\n",
    "    plt.subplot(4, 4, i+1)\n",
    "    image = images[i, :, :, :]\n",
    "    image = np.reshape(image, [self.img_rows, self.img_cols])\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "if save2file:\n",
    "    plt.savefig(filename)\n",
    "    plt.close('all')\n",
    "else:\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
