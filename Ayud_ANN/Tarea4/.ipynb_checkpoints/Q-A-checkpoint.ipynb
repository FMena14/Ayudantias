{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://rajpurkar.github.io/SQuAD-explorer/\n",
    "\n",
    "https://www.kaggle.com/quora/question-pairs-dataset/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev-v2.0.json  evaluate-v2.0.py  OldVersion  train-v2.0.json\r\n"
     ]
    }
   ],
   "source": [
    "!ls SQuAD/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Cantidad de preguntas realizadas: ', 130319)\n",
      "('Cantidad de respuestas realizadas a todas las preguntas: ', 86821)\n",
      "('Cantidad de preguntas diferentes: ', 86769)\n",
      "('Cantidad de respuestas diferentes: ', 64763)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "archivo = open(\"SQuAD/train-v2.0.json\")\n",
    "f = archivo.read()\n",
    "jsonToPython = json.loads(f)\n",
    "data = jsonToPython[\"data\"]\n",
    "archivo.close()\n",
    "\n",
    "train_questions = []\n",
    "train_answers = []\n",
    "train_ids = []\n",
    "questions = 0\n",
    "answers = 0\n",
    "\n",
    "for doc in data:\n",
    "    for paragraph in doc[\"paragraphs\"]: #cada contexto\n",
    "        for pairs in paragraph[\"qas\"]: #cada par pregunta y respuestas en ese contexto\n",
    "            questions+=1\n",
    "            for answer in pairs[\"answers\"]: #por cada respuesta a esa pregunta en este contexto\n",
    "                train_questions.append(pairs[\"question\"])        \n",
    "                train_answers.append(answer[\"text\"])\n",
    "                train_ids.append(pairs[\"id\"])\n",
    "                answers+=1\n",
    "del data,f\n",
    "print(\"Cantidad de preguntas realizadas: \",questions)\n",
    "print(\"Cantidad de respuestas realizadas a todas las preguntas: \",answers)\n",
    "\n",
    "print(\"Cantidad de preguntas diferentes: \",len(set(train_questions)))\n",
    "print(\"Cantidad de respuestas diferentes: \",len(set(train_answers)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86821"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86821"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'When did Beyonce start becoming popular?',\n",
       " u'What areas did Beyonce compete in when she was growing up?',\n",
       " u\"When did Beyonce leave Destiny's Child and become a solo singer?\",\n",
       " u'In what city and state did Beyonce  grow up? ',\n",
       " u'In which decade did Beyonce become famous?']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_questions[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'in the late 1990s',\n",
       " u'singing and dancing',\n",
       " u'2003',\n",
       " u'Houston, Texas',\n",
       " u'late 1990s']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_answers[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Cantidad de preguntas realizadas: ', 11873)\n",
      "('Cantidad de preguntas diferentes: ', 11864)\n"
     ]
    }
   ],
   "source": [
    "archivo = open(\"SQuAD/dev-v2.0.json\")\n",
    "f = archivo.read()\n",
    "jsonToPython = json.loads(f)\n",
    "data = jsonToPython[\"data\"]\n",
    "archivo.close()\n",
    "\n",
    "\n",
    "test_questions = []\n",
    "test_ids = []\n",
    "questions = 0\n",
    "\n",
    "for doc in data:\n",
    "    for paragraph in doc[\"paragraphs\"]: #cada contexto\n",
    "        for pairs in paragraph[\"qas\"]: #cada par pregunta y respuestas en ese contexto\n",
    "            questions+=1\n",
    "            test_questions.append(pairs[\"question\"])\n",
    "            test_ids.append(pairs[\"id\"])\n",
    "del data,f\n",
    "print(\"Cantidad de preguntas realizadas: \",questions)\n",
    "print(\"Cantidad de preguntas diferentes: \",len(set(test_questions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf8')\n",
    "print(\"Hola\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "d = {'id': train_ids, 'question': train_questions,'answer': train_answers}\n",
    "entrega = pd.DataFrame(data=d,columns=['id','question','answer'])\n",
    "entrega.head()\n",
    "entrega.to_csv('train_Q-A.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "d = {'id': test_ids, 'question': test_questions}\n",
    "entrega = pd.DataFrame(data=d,columns=['id','question'])\n",
    "entrega.head()\n",
    "entrega.to_csv('test_Q.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USE DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(86821, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_train = pd.read_csv('SQuAD/train_Q-A.csv')\n",
    "df_test = pd.read_csv('SQuAD/test_Q.csv')\n",
    "\n",
    "df_train.head()\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "any(pd.isnull(df_test)) #df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/fmena/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19026\n",
      "St. Luke's–Roosevelt Hospital Center\n",
      "32038\n",
      "42 U.S.C. § 1983\n",
      "36200\n",
      "C. Préaux\n",
      "40351\n",
      "George F. E. Rudé\n",
      "51035\n",
      "Nürnberg. On Carnival Thursday\n",
      "52433\n",
      "near St. Sophia’s Cathedral\n",
      "53487\n",
      "c. 1037–1137\n",
      "56907\n",
      "c. 320–550 CE\n",
      "62336\n",
      "bushi (武士?, [bu.ɕi]) or buke (武家?)\n",
      "71651\n",
      "nan\n",
      "75670\n",
      "the Lower Paleolithic era, c. 800,000–200,000 BC\n",
      "75949\n",
      "c. 488–444 Ma and early Silurian period\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "for i,dato in enumerate(df_train[\"answer\"]):\n",
    "    try:\n",
    "        word_tokenize(dato)\n",
    "        #print(\"hola\")\n",
    "    except:\n",
    "        print(i)\n",
    "        print(dato)\n",
    "        #asd\n",
    "        #pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.loc[19026,\"answer\"] = \"St. Luke's-Roosevelt Hospital Center\"\n",
    "df_train.loc[32038,\"answer\"] = \"42 U.S.C. # 1983\"\n",
    "df_train.loc[36200,\"answer\"] = \"C. Preaux\"\n",
    "df_train.loc[40351,\"answer\"] = \"George F. E. Rude\"\n",
    "df_train.loc[51035,\"answer\"] = \"Nurnberg. On Carnival Thursday\"\n",
    "df_train.loc[52433,\"answer\"] = \"near St. Sophia's Cathedral\"\n",
    "df_train.loc[53487,\"answer\"] = \"c. 1037-1137\"\n",
    "df_train.loc[56907,\"answer\"] = \"c. 320-550 CE\"\n",
    "df_train.loc[62336,\"answer\"] = \"bushi or buke\"\n",
    "df_train.loc[71651,\"answer\"] = \"dont know\"\n",
    "df_train.loc[75670,\"answer\"] = \"the Lower Paleolithic era, c. 800,000-200,000 BC\"\n",
    "df_train.loc[75949,\"answer\"] = \"c. 488-444 Ma and early Silurian period\"\n",
    "\n",
    "#con null clean\n",
    "df_train.loc[29373,\"question\"] = \"What is the meaning of the Sanskrit loanword 'bhasa?'\"\n",
    "df_train.loc[37565,\"question\"] = \"What year was J. C. Flugel's Psychology of Clothes published?\"\n",
    "df_train.loc[75950,\"question\"] = \"What happened during the c. 488-444 Ma and early Silurian period?\"\n",
    "\n",
    "df_train.to_csv('train_Q-A.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['when', 'did', 'beyonce', 'start', 'becoming', 'popular', '?'], ['what', 'areas', 'did', 'beyonce', 'compete', 'in', 'when', 'she', 'was', 'growing', 'up', '?']]\n",
      "[['in', 'what', 'country', 'is', 'normandy', 'located', '?'], ['when', 'were', 'the', 'normans', 'in', 'normandy', '?']]\n",
      "[['in', 'the', 'late', '1990s'], ['singing', 'and', 'dancing']]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "train_questions = [word_tokenize(sentence.lower()) for sentence in df_train[\"question\"]] #or processing\n",
    "test_questions = [word_tokenize(sentence.lower()) for sentence in df_test[\"question\"]]\n",
    "train_answers = [word_tokenize(sentence) for sentence in df_train[\"answer\"]]\n",
    "print(train_questions[:2])\n",
    "print(test_questions[:2])\n",
    "print(train_answers[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('posibles palabras para respuestas :', 47506)\n",
      "('posibles palabras para preguntas :', 42088)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Shapur': 2,\n",
       " 'pro-Soviet': 3,\n",
       " '1,800': 4,\n",
       " 'Poetry': 5,\n",
       " 'Laboustre': 6,\n",
       " 'Larsen': 7,\n",
       " 'pantheistic': 8,\n",
       " 'hanging': 9,\n",
       " 'woody': 10,\n",
       " 'localized': 11,\n",
       " 'Schuster': 12,\n",
       " 'Warfare/Jamming': 82,\n",
       " 'Franco-Prussian': 31698,\n",
       " 'orthographies': 16,\n",
       " \"Ba'al\": 17,\n",
       " 'Archuleta': 18,\n",
       " 'Journey': 15767,\n",
       " 'mutinies': 20,\n",
       " 'Western': 21,\n",
       " 'Shakhanov': 23,\n",
       " 'Caen': 24,\n",
       " 'Euro': 25,\n",
       " 'co-operation': 27,\n",
       " 'Valle': 28,\n",
       " 'Jornado': 29,\n",
       " 'Blade': 30,\n",
       " '\\xe2\\x9f\\xa8t\\xe2\\x9f\\xa9': 31,\n",
       " 'eugenics': 32,\n",
       " 'appropriation': 33,\n",
       " 'rawhide': 34,\n",
       " 'DiscoVision': 39477,\n",
       " 'Morten': 35,\n",
       " 'fractal': 220,\n",
       " 'wooded': 37,\n",
       " 'vibrational': 38,\n",
       " 'Rayleigh': 39,\n",
       " 'wooden': 40,\n",
       " 'Pergamon': 41,\n",
       " 'salination': 42,\n",
       " 'woods': 43,\n",
       " 'Multilateral': 44,\n",
       " 'stereotypical': 45,\n",
       " 'south-west': 46,\n",
       " 'spiders': 47,\n",
       " 'non-violent': 48,\n",
       " 'Pinkerton': 49,\n",
       " 'insular': 15508,\n",
       " '43:10': 51,\n",
       " 'Sponsorship': 52,\n",
       " 'feasibility': 53,\n",
       " '271': 54,\n",
       " '272': 55,\n",
       " '273': 56,\n",
       " '275': 57,\n",
       " '276': 58,\n",
       " '278': 59,\n",
       " '279': 60,\n",
       " 'corv\\xc3\\xa9e': 61,\n",
       " 'prosody': 62,\n",
       " 'inanimate': 63,\n",
       " 'errors': 64,\n",
       " 'deferred': 65,\n",
       " 'Initially': 66,\n",
       " 'cooking': 67,\n",
       " 'Sebasti\\xc3\\xa1n': 43328,\n",
       " '1612': 34607,\n",
       " 'Kilimanjaro': 69,\n",
       " 'Hamilton': 70,\n",
       " 'aggression': 23753,\n",
       " 'designing': 72,\n",
       " 'numeral': 73,\n",
       " 'consular': 31708,\n",
       " 'Croat': 75,\n",
       " 'Logging': 76,\n",
       " 'moksha': 77,\n",
       " 'MonoSpace': 78,\n",
       " 'Hooge': 79,\n",
       " 'affiliates': 80,\n",
       " 'mg/day': 81,\n",
       " 'mutinied': 14,\n",
       " 'china': 83,\n",
       " '11,900': 31710,\n",
       " 'affiliated': 85,\n",
       " 'Azkaban': 15776,\n",
       " '1670s': 87,\n",
       " 'Bandier': 88,\n",
       " 'Moltacqua': 36542,\n",
       " 'pajh\\xc4\\x81nas': 90,\n",
       " 'Downfall': 91,\n",
       " 'Picturephone': 92,\n",
       " 'Manufacturers': 23756,\n",
       " 'Dhimmis': 93,\n",
       " 'Dubose': 94,\n",
       " 'controversy': 95,\n",
       " 'kachee': 96,\n",
       " '144,600': 97,\n",
       " 'natures': 98,\n",
       " 'neurologist': 99,\n",
       " 'Isles': 100,\n",
       " 'Isabelle': 29331,\n",
       " 'Stand': 101,\n",
       " 'millimetres': 102,\n",
       " 'golden': 103,\n",
       " 'topography': 104,\n",
       " 'projection': 105,\n",
       " 'Harvey': 106,\n",
       " 'Sackler': 107,\n",
       " 'Przebiegi': 108,\n",
       " 'rifting': 109,\n",
       " \"Baha'i\": 110,\n",
       " 'originality': 111,\n",
       " 'Sandstone': 37608,\n",
       " 'constitutio': 112,\n",
       " 'insecurity': 113,\n",
       " 'Okeechobee': 114,\n",
       " 'Akechi': 115,\n",
       " 'music': 116,\n",
       " 'therefore': 117,\n",
       " 'distortions': 118,\n",
       " 'Buyid': 119,\n",
       " 'sermons': 120,\n",
       " '2.48\\xc3\\x97105': 121,\n",
       " 'Nohant': 122,\n",
       " 'Tribhuwan': 123,\n",
       " 'populations': 124,\n",
       " 'Chelomey': 125,\n",
       " '43': 32633,\n",
       " 'Buol': 126,\n",
       " 'UNITE': 127,\n",
       " 'Pyotr': 128,\n",
       " '5,416': 129,\n",
       " 'milestones': 130,\n",
       " 'Nazarbaev': 131,\n",
       " 'schoolboys': 132,\n",
       " 'circumstances': 133,\n",
       " 'Dolbear': 134,\n",
       " 'slash-and-burn': 135,\n",
       " 'intake': 136,\n",
       " 'morally': 137,\n",
       " 'locked': 138,\n",
       " 'Occitano-Romance': 139,\n",
       " '40': 32634,\n",
       " 'Madrid': 8352,\n",
       " 'Colombia': 141,\n",
       " 'MMTS': 39495,\n",
       " 'RLV': 142,\n",
       " 'Ficus': 143,\n",
       " 'Le': 144,\n",
       " 'irrationalist': 145,\n",
       " '11172-3': 41591,\n",
       " 'Thupten': 866,\n",
       " 'unjust': 872,\n",
       " '.ts': 148,\n",
       " 'Lk': 149,\n",
       " 'Li': 150,\n",
       " 'glassblowers': 151,\n",
       " 'Lt': 152,\n",
       " 'Lu': 153,\n",
       " 'kingdoms': 39498,\n",
       " 'want': 155,\n",
       " 'cookers': 156,\n",
       " 'Lilienfeld': 157,\n",
       " 'rayon': 158,\n",
       " 'Biafra': 159,\n",
       " 'Dominicans': 160,\n",
       " 'absolute': 161,\n",
       " 'LF': 162,\n",
       " 'LD': 921,\n",
       " 'LC': 164,\n",
       " 'LA': 165,\n",
       " 'travel': 166,\n",
       " '78,000': 167,\n",
       " 'Ismenius': 168,\n",
       " 'playback': 169,\n",
       " 'Songbirds': 170,\n",
       " 'LP': 171,\n",
       " 'Falcon': 969,\n",
       " 'LZ': 173,\n",
       " '1177': 174,\n",
       " '1175': 175,\n",
       " 'Bates': 176,\n",
       " '1170': 177,\n",
       " 'L.': 178,\n",
       " 'PenAir': 303,\n",
       " 'Lierde': 180,\n",
       " 'century\\xe2\\x80\\x931489': 1022,\n",
       " '1179': 182,\n",
       " 'Shukeiri': 184,\n",
       " 'dinosaurs': 185,\n",
       " 'modest': 186,\n",
       " 'sentencing': 187,\n",
       " 'pigment': 188,\n",
       " 'Punjabis': 189,\n",
       " '*d\\xc5\\xba': 190,\n",
       " 'recombination': 191,\n",
       " 'CFB': 192,\n",
       " 'Nidhal': 193,\n",
       " 'CFA': 194,\n",
       " '18th': 195,\n",
       " 'Mabel': 196,\n",
       " 'rheumatism': 31728,\n",
       " 'Federative': 199,\n",
       " 'Navigator': 200,\n",
       " 'CFR': 201,\n",
       " 'CFS': 202,\n",
       " 'Willy': 203,\n",
       " 'welcomed': 204,\n",
       " 'Wills': 205,\n",
       " 'canyon-like': 1138,\n",
       " 'Badajoz': 207,\n",
       " 'Asim': 208,\n",
       " 'Willi': 209,\n",
       " 'Robertson': 212,\n",
       " 'pain-related': 213,\n",
       " 'mid-1850s': 214,\n",
       " 'Hip': 216,\n",
       " 'Abbott': 217,\n",
       " 'His': 218,\n",
       " '40,000,000': 219,\n",
       " 'wiretapping': 31721,\n",
       " 'bringing': 36,\n",
       " 'Nieuwegein': 1209,\n",
       " '624': 34409,\n",
       " 'Romanised': 19389,\n",
       " 'Hananuntasuk': 224,\n",
       " 'fig': 225,\n",
       " 'omitted': 15799,\n",
       " 'Bladderball': 227,\n",
       " 'Hagenau': 228,\n",
       " 'images\\xe2\\x80\\x94typically': 229,\n",
       " 'guaila': 37044,\n",
       " 'paterfamilias': 230,\n",
       " 'NJPW': 42047,\n",
       " 'Concave': 231,\n",
       " 'Muteferrika': 233,\n",
       " '41,285': 234,\n",
       " 'intraradices': 235,\n",
       " 'hypoxia': 236,\n",
       " 'effects': 237,\n",
       " 'multidimensional': 238,\n",
       " 'sixteen': 239,\n",
       " 'Mechanics': 240,\n",
       " 'Lipopolysaccharides': 241,\n",
       " 'Shams': 242,\n",
       " '1,344': 1359,\n",
       " '1,342': 244,\n",
       " 'Shame': 245,\n",
       " 'arrow': 246,\n",
       " 'arroz': 247,\n",
       " '302nd': 248,\n",
       " 'Uthmanic': 249,\n",
       " \"B'Day\": 250,\n",
       " 'burial': 251,\n",
       " 'telescope': 252,\n",
       " 'parasites': 253,\n",
       " '393': 254,\n",
       " '391': 255,\n",
       " '390': 20453,\n",
       " '397': 257,\n",
       " '396': 258,\n",
       " '395': 259,\n",
       " 'Jamestown': 260,\n",
       " 'Byng': 261,\n",
       " 'Tyrnavos': 211,\n",
       " 'Pomerania': 263,\n",
       " '120,000': 264,\n",
       " 'toleration': 265,\n",
       " 'self-': 266,\n",
       " 'deerskin': 267,\n",
       " 'antiheroes': 268,\n",
       " 'specialist': 35637,\n",
       " 'Val\\xc3\\xa9rie': 269,\n",
       " 'combinatorial': 1513,\n",
       " 'encourage': 271,\n",
       " 'Vaishnava': 272,\n",
       " 'adapt': 273,\n",
       " 'Zeuxis': 274,\n",
       " 'gate': 44618,\n",
       " 'stamping': 275,\n",
       " 'abbots': 276,\n",
       " 'strata': 277,\n",
       " 'Monaghan': 278,\n",
       " 'Penrose\\xe2\\x80\\x93Hawking': 39520,\n",
       " 'estimate': 280,\n",
       " 'Egg': 281,\n",
       " 'universally': 282,\n",
       " 'chlorine': 283,\n",
       " 'jugs': 284,\n",
       " 'Governator': 285,\n",
       " 'Nigel': 286,\n",
       " 'Kleve': 287,\n",
       " 'ministries': 288,\n",
       " 'disturbed': 289,\n",
       " 'competed': 290,\n",
       " 'Facts': 291,\n",
       " 'Niger': 292,\n",
       " 'hostels': 16910,\n",
       " '\\xc2\\xa3640': 294,\n",
       " 'Mendarozqueta': 295,\n",
       " '787\\xe2\\x80\\x93797': 296,\n",
       " 'feldspar': 298,\n",
       " 'seizures': 299,\n",
       " 'service': 300,\n",
       " 'Gropegate': 301,\n",
       " 'Fearless': 302,\n",
       " 'Provisional': 304,\n",
       " 'needed': 305,\n",
       " 'master': 307,\n",
       " 'genesis': 309,\n",
       " 'rewards': 310,\n",
       " 'Stadtteilschule': 311,\n",
       " 'mid-June': 312,\n",
       " 'Nicolaus': 313,\n",
       " 'Tarvis': 1831,\n",
       " 'Schechter': 25983,\n",
       " 'Oracle': 315,\n",
       " 'Daddy': 316,\n",
       " 'positively': 317,\n",
       " '270': 318,\n",
       " 'Colleges': 319,\n",
       " 'Catalunya': 320,\n",
       " 'tail-on': 321,\n",
       " 'regulator': 322,\n",
       " 'idle': 323,\n",
       " 'Mithridates': 324,\n",
       " 'feeling': 326,\n",
       " '365.26-day': 327,\n",
       " 'Chicago': 329,\n",
       " 'redesigned': 31760,\n",
       " 'spectrum': 331,\n",
       " 'increment': 332,\n",
       " '2012-2015': 23801,\n",
       " 'arousal': 334,\n",
       " 'resignations': 23802,\n",
       " 'dozen': 336,\n",
       " 'Theo': 337,\n",
       " 'affairs': 338,\n",
       " 'Them': 339,\n",
       " 'wholesome': 340,\n",
       " 'Britney': 15825,\n",
       " 'Im\\xc4\\x81m': 31762,\n",
       " 'Jiabao': 342,\n",
       " 'Component': 343,\n",
       " '2.76': 344,\n",
       " 'toothed': 345,\n",
       " 'They': 346,\n",
       " 'Lakshmi': 347,\n",
       " 'Missionary': 348,\n",
       " 'Iliopoulos': 349,\n",
       " 'shipments': 350,\n",
       " '45th': 351,\n",
       " 'exposure-index': 352,\n",
       " 'committing': 353,\n",
       " 'Chaldean': 354,\n",
       " 'sugarcane': 355,\n",
       " '1570s': 356,\n",
       " 'retroviruses': 357,\n",
       " 'simplify': 358,\n",
       " 'mouth': 359,\n",
       " 'conceded': 360,\n",
       " 'Ciclov\\xc3\\xadas': 361,\n",
       " 'transverse': 23073,\n",
       " 'Sein': 363,\n",
       " 'singer': 364,\n",
       " '4,472': 365,\n",
       " 'Ujelang': 39529,\n",
       " 'Vauxhall': 367,\n",
       " 'Romaniot': 368,\n",
       " 'multiracial': 369,\n",
       " 't\\xca\\xb0': 370,\n",
       " 'tech': 371,\n",
       " 'scream': 372,\n",
       " 'non-EU': 373,\n",
       " 'saying': 374,\n",
       " 'Curly': 375,\n",
       " 'U-2': 376,\n",
       " 'gibberelins': 377,\n",
       " 'Helvetica': 378,\n",
       " 'Jacobson': 379,\n",
       " 'padded': 380,\n",
       " 'Cunha': 381,\n",
       " 'Marks': 382,\n",
       " 'Daryl': 383,\n",
       " '35\\xe2\\x80\\x9355': 384,\n",
       " 'Resources': 385,\n",
       " 'Olivares': 23809,\n",
       " 'Titoists': 387,\n",
       " 'rich': 388,\n",
       " 'rice': 389,\n",
       " 'plate': 390,\n",
       " 'Panchen': 391,\n",
       " '4.5': 45947,\n",
       " '1521\\xe2\\x80\\x931567': 392,\n",
       " 'Branly': 393,\n",
       " 'Logitech': 394,\n",
       " 'photoelectric': 395,\n",
       " 'All-Star': 31774,\n",
       " 'altogether': 397,\n",
       " 'Landshut': 42358,\n",
       " 'meantone': 39508,\n",
       " 'droning': 399,\n",
       " 'HIPPO': 400,\n",
       " 'Ecology': 401,\n",
       " 'jaguar': 402,\n",
       " 'rectal': 403,\n",
       " '488,000': 39539,\n",
       " 'Rip': 405,\n",
       " 'patch': 406,\n",
       " 'Rio': 407,\n",
       " 'Dances': 408,\n",
       " 'Rim': 409,\n",
       " 'Rif': 410,\n",
       " 'Ric': 411,\n",
       " 'Composers': 413,\n",
       " '73,464': 414,\n",
       " 'clarified': 415,\n",
       " 'sensitivity': 416,\n",
       " 'Albano': 417,\n",
       " 'Anderson': 418,\n",
       " 'Uighur': 419,\n",
       " 'Heren': 420,\n",
       " 'Netherfield': 422,\n",
       " 'Occupational': 423,\n",
       " '196.9': 424,\n",
       " 'I-95': 425,\n",
       " 'Kampfgruppe': 426,\n",
       " 'I-91': 427,\n",
       " 'I-90': 428,\n",
       " 'lots': 429,\n",
       " 'conductive': 431,\n",
       " 'Ominous': 432,\n",
       " 'solenoid': 433,\n",
       " 'Advanced': 434,\n",
       " 'discipline': 31779,\n",
       " 'redistricting': 436,\n",
       " 'extend': 437,\n",
       " 'nature': 438,\n",
       " 'BYUtv': 439,\n",
       " 'Cassaro': 2658,\n",
       " 'GameTrailers': 441,\n",
       " 'Saussure': 443,\n",
       " 'extent': 444,\n",
       " 'accessed': 39545,\n",
       " 'Bronx': 446,\n",
       " 'disciplina': 31782,\n",
       " 'Charbonneau': 448,\n",
       " 'inflorescence': 2686,\n",
       " 'tyranny': 450,\n",
       " 'semi-state': 451,\n",
       " 'heating': 452,\n",
       " 'incense': 453,\n",
       " 'Bras\\xc3\\xadlia': 454,\n",
       " 'Laura': 455,\n",
       " '1592-1665': 456,\n",
       " 'Neuch\\xc3\\xa2tel': 457,\n",
       " 'Pebble': 458,\n",
       " 'Amundsen\\xe2\\x80\\x93Scott': 15844,\n",
       " 'southeastern': 460,\n",
       " 'eradicate': 461,\n",
       " 'Terrestrial': 39550,\n",
       " 'twenty-three': 463,\n",
       " 'ARCUK': 15846,\n",
       " '11.15': 465,\n",
       " 'Balkh': 466,\n",
       " 'Gymboree': 467,\n",
       " 'Ruggero': 468,\n",
       " '4.2': 45954,\n",
       " 'mid-8th': 470,\n",
       " 'pericardial': 471,\n",
       " 'Amor\\xc3\\xb3s': 472,\n",
       " 'blonde': 473,\n",
       " 'organisational': 474,\n",
       " '3.82': 475,\n",
       " 'union': 477,\n",
       " '3.88': 478,\n",
       " '.': 480,\n",
       " 'Mahdist': 481,\n",
       " 'much': 482,\n",
       " 'sommelier': 483,\n",
       " '1678': 442,\n",
       " 'Lerdista': 485,\n",
       " 'Zarafshan': 486,\n",
       " 'tallest': 487,\n",
       " 'Adventist': 2876,\n",
       " 'obese': 489,\n",
       " 'Remington': 490,\n",
       " 'Poseidon': 491,\n",
       " 'spit': 492,\n",
       " 'Arnaud': 493,\n",
       " 'conifers': 494,\n",
       " 'freehold': 495,\n",
       " 'low-water': 496,\n",
       " 'Donovan': 497,\n",
       " 'phlox': 498,\n",
       " 'doubts': 499,\n",
       " 'business-related': 500,\n",
       " 'Cipolla': 20084,\n",
       " 'spin': 502,\n",
       " 'Elgin': 503,\n",
       " 'Transposable': 504,\n",
       " '1670': 31793,\n",
       " 'Ben-Gurion': 506,\n",
       " '1671': 31794,\n",
       " 'employ': 508,\n",
       " 'misconstrued': 2976,\n",
       " 'Crusaders': 510,\n",
       " 'moraines': 512,\n",
       " 'Blofeld': 513,\n",
       " 'McLanahan': 27720,\n",
       " 'riparian': 514,\n",
       " 'Feathers': 515,\n",
       " 'Skye': 516,\n",
       " 'Clarksburg': 3037,\n",
       " 'conditioned': 518,\n",
       " \"d'Alembert\": 519,\n",
       " 'Potomac': 520,\n",
       " 'eighteen': 521,\n",
       " 'London-based': 522,\n",
       " 'Jewels': 523,\n",
       " 'Sword': 524,\n",
       " 'Brookings': 9385,\n",
       " 'MOSFETs': 526,\n",
       " 'musicianship': 46148,\n",
       " 'breakfast': 527,\n",
       " 'transformative': 34310,\n",
       " '3\\xc2\\xb0': 3087,\n",
       " 'Valley': 39561,\n",
       " 'Elders': 530,\n",
       " 'acquire': 531,\n",
       " 'Tahitian': 3120,\n",
       " 'Cosmati': 533,\n",
       " 'split': 535,\n",
       " 'Chinese': 32044,\n",
       " 'inadvertently': 537,\n",
       " 'marcher': 538,\n",
       " 'Indo-Australian': 539,\n",
       " 'qualifications': 540,\n",
       " 'workforce': 541,\n",
       " 'Kre\\xc5\\xa1imir': 542,\n",
       " 'marched': 543,\n",
       " 'boiler': 544,\n",
       " 'Persia': 545,\n",
       " 'Inter-Cities': 546,\n",
       " 'cost-effectiveness': 547,\n",
       " 'altar': 45333,\n",
       " 'peremptory': 548,\n",
       " 'Eisner': 31799,\n",
       " 'Gavin': 550,\n",
       " 'Stearn': 551,\n",
       " 'EBU': 3243,\n",
       " 'academia': 553,\n",
       " 'Flexible-goals': 554,\n",
       " 'Table': 555,\n",
       " 'Martyr': 557,\n",
       " 'corporate': 558,\n",
       " 'Hellene': 559,\n",
       " 'plaque': 560,\n",
       " 'letra': 561,\n",
       " 'Rapid': 562,\n",
       " 'appropriately': 563,\n",
       " 'homogeneity': 564,\n",
       " 'portrayed': 565,\n",
       " 'Happy': 566,\n",
       " 'Eternity': 567,\n",
       " 'tree-ring': 568,\n",
       " 'Rouget': 569,\n",
       " 'ham': 570,\n",
       " 'Oscar': 571,\n",
       " 'Coele-Syria': 572,\n",
       " 'had': 573,\n",
       " 'advancement': 574,\n",
       " 'second-wealthiest': 575,\n",
       " 'hay': 576,\n",
       " 'Baghdad': 3392,\n",
       " 'beloved': 578,\n",
       " 'has': 579,\n",
       " 'espoused': 580,\n",
       " 'Wikiproject': 581,\n",
       " 'Fono': 31806,\n",
       " 'Housekeeping': 3435,\n",
       " 'elders': 585,\n",
       " 'survival': 586,\n",
       " 'Hochbunker': 587,\n",
       " 'pranks': 18875,\n",
       " 'Three': 23839,\n",
       " 'indicative': 590,\n",
       " 'clustered': 591,\n",
       " 'shadow': 592,\n",
       " 'Muscovy': 593,\n",
       " 'longbowmen': 594,\n",
       " 'Monge': 595,\n",
       " 'Lascaux': 596,\n",
       " 'Steven': 597,\n",
       " 'Leghorn': 598,\n",
       " 'Supplementum': 600,\n",
       " 'Pentagon': 601,\n",
       " 'misdemeanors': 602,\n",
       " 'Swedish': 603,\n",
       " 'Vladan': 604,\n",
       " 'crowd': 605,\n",
       " 'mosques': 606,\n",
       " 'crown': 607,\n",
       " 'proportionally': 608,\n",
       " 'deflection': 609,\n",
       " 'captive': 610,\n",
       " 'Between': 611,\n",
       " '52nd': 612,\n",
       " 'Clopas': 613,\n",
       " 'bottom': 614,\n",
       " 'Garde': 615,\n",
       " 'inhuman': 616,\n",
       " 'plucked': 617,\n",
       " 'Facebook': 619,\n",
       " '1398\\xe2\\x80\\x931402': 620,\n",
       " 'syphilis': 621,\n",
       " 'Devanagari': 622,\n",
       " 'Breakage': 623,\n",
       " 'monogamy': 624,\n",
       " 'Watts': 625,\n",
       " 'Allard': 39891,\n",
       " 'Calls': 626,\n",
       " 'Tussauds': 627,\n",
       " 'Akhenaten': 628,\n",
       " 'top-one-percent': 629,\n",
       " 'binder': 630,\n",
       " 'brigades': 631,\n",
       " 'neo-Gothic': 632,\n",
       " 'filaments': 633,\n",
       " '1644\\xe2\\x80\\x931911': 634,\n",
       " '1644\\xe2\\x80\\x931912': 635,\n",
       " 'Zener': 636,\n",
       " 'restlessness': 637,\n",
       " 'two-channel': 638,\n",
       " 'Bulblins': 639,\n",
       " 'anomalous': 640,\n",
       " 'officeholders': 641,\n",
       " 'oneness': 3805,\n",
       " 'Fredericks': 643,\n",
       " 'abbreviations': 644,\n",
       " 'non-normative': 646,\n",
       " 'Guangnan': 647,\n",
       " 'infraction': 648,\n",
       " 'administer': 649,\n",
       " 'Sitka': 650,\n",
       " 'beings': 651,\n",
       " 'one-fourth': 3888,\n",
       " 'waxwings': 653,\n",
       " 'Gojri': 654,\n",
       " 'a\\xc3\\xa9ronef': 655,\n",
       " 'shoots': 657,\n",
       " 'despised': 659,\n",
       " 'Mac': 31943,\n",
       " 'fabric': 660,\n",
       " 'altitude': 661,\n",
       " 'Mallet': 662,\n",
       " 'XETV-TV': 663,\n",
       " 'grasping': 3971,\n",
       " '1880\\xe2\\x80\\x931973': 665,\n",
       " 'Endo': 666,\n",
       " 'Marie-Antoinette': 667,\n",
       " '70\\xe2\\x80\\x9390': 668,\n",
       " 'Immink': 670,\n",
       " 'strike': 23852,\n",
       " 'Soda': 672,\n",
       " 'spokesperson': 673,\n",
       " 'safeguard': 674,\n",
       " 'Interferon': 676,\n",
       " 'Amenhotep': 677,\n",
       " 'arrays': 678,\n",
       " 'Jaya': 679,\n",
       " 'Household': 680,\n",
       " 'non-discrimination': 681,\n",
       " 'County': 682,\n",
       " 'open-air': 683,\n",
       " 'unanimous': 684,\n",
       " 'pesos': 685,\n",
       " 'soldering': 686,\n",
       " 'Brest': 687,\n",
       " 'Counts': 688,\n",
       " 'dues': 689,\n",
       " 'refillable': 690,\n",
       " 'Anahat': 691,\n",
       " 'passenger': 692,\n",
       " 'la\\xc3\\xafk\\xc3\\xb3': 693,\n",
       " 'disgrace': 694,\n",
       " 'moderne': 695,\n",
       " 'Pedieos': 39590,\n",
       " 'Tales': 697,\n",
       " 'sanatoria': 698,\n",
       " 'Crater': 699,\n",
       " 'bats': 9452,\n",
       " 'decapitation': 700,\n",
       " 'Taleb': 701,\n",
       " 'triangles': 702,\n",
       " 'scritto': 703,\n",
       " 'eventual': 705,\n",
       " 'Surayj': 706,\n",
       " 'serous': 707,\n",
       " 'role': 708,\n",
       " 'arquebus': 14841,\n",
       " 'transgender': 710,\n",
       " 'mutually': 37879,\n",
       " 'roll': 711,\n",
       " 'E': 712,\n",
       " 'docrtrine': 713,\n",
       " 'palms': 714,\n",
       " 'Buceo': 715,\n",
       " 'emisor/': 716,\n",
       " 'Melbourne': 717,\n",
       " 'Bodybuilder': 718,\n",
       " 'intent': 719,\n",
       " 'Dakhini': 720,\n",
       " 'Bucer': 721,\n",
       " 'Idi': 15839,\n",
       " 'variable': 723,\n",
       " 'Ealing': 724,\n",
       " 'Gardens': 725,\n",
       " 'explosions': 726,\n",
       " 'Ida': 727,\n",
       " 'Almaz': 729,\n",
       " 'ordination': 39598,\n",
       " 'Sudanese': 731,\n",
       " 'Senorina': 31840,\n",
       " 'Dinapanah': 734,\n",
       " 'gown': 735,\n",
       " 'corps': 736,\n",
       " 'Northeast': 15879,\n",
       " 'Escudier': 739,\n",
       " 'bandits': 740,\n",
       " 'Wabash': 741,\n",
       " 'chair': 742,\n",
       " 'ballet': 743,\n",
       " 'amplification': 744,\n",
       " 'Hyperlinks': 44122,\n",
       " 'Salvarsan': 15880,\n",
       " 'Leovigild': 746,\n",
       " '12,884': 747,\n",
       " 'freelance': 749,\n",
       " 'quasi-judicial': 750,\n",
       " 'megawatts': 751,\n",
       " 'Nightmares': 4505,\n",
       " 'silencers': 753,\n",
       " 'flat-top': 754,\n",
       " 'Struggle': 755,\n",
       " 'Danelaw': 756,\n",
       " 'Tourism': 757,\n",
       " '30M': 31843,\n",
       " 'downloading': 760,\n",
       " '3,150': 761,\n",
       " 'Political': 39602,\n",
       " 'Gayowsky': 762,\n",
       " 'non-corruptible': 763,\n",
       " 'optronics': 764,\n",
       " 'choice': 765,\n",
       " 'embark': 766,\n",
       " 'mullions': 767,\n",
       " 'blackwood': 46509,\n",
       " 'stays': 768,\n",
       " 'monetisation': 769,\n",
       " 'exact': 770,\n",
       " 'minute': 771,\n",
       " 'centrasia.ru': 772,\n",
       " 'Fifteen': 31845,\n",
       " 'Tax': 775,\n",
       " 'Tay': 776,\n",
       " 'pursue': 23871,\n",
       " 'platinized': 778,\n",
       " 'Katrina': 779,\n",
       " 'defaults': 780,\n",
       " 'Tac': 781,\n",
       " 'skewer': 782,\n",
       " 'Tan': 783,\n",
       " '1,400,000': 784,\n",
       " 'Tai': 785,\n",
       " 'Taj': 786,\n",
       " 'Copernicus': 787,\n",
       " 'Silvio': 788,\n",
       " 'Roddy': 789,\n",
       " 'roundworm': 790,\n",
       " 'trails': 791,\n",
       " 'Silvia': 4699,\n",
       " 'M.': 793,\n",
       " 'heavyweight': 794,\n",
       " 'M6': 795,\n",
       " 'M1': 796,\n",
       " 'shirts': 797,\n",
       " 'M3': 798,\n",
       " '22\\xe2\\x80\\x9327': 43112,\n",
       " 'agonists': 799,\n",
       " 'Reflective': 800,\n",
       " 'M9': 801,\n",
       " 'M8': 802,\n",
       " 'headset': 803,\n",
       " '5,200': 804,\n",
       " 'Roman-Christian': 806,\n",
       " 'celebrated': 807,\n",
       " 'Landau': 809,\n",
       " 'polygonal': 810,\n",
       " 'Adichie': 31852,\n",
       " 'Panama-California': 812,\n",
       " 'Lopes': 813,\n",
       " 'allegorizing': 814,\n",
       " 'Magna': 815,\n",
       " 'ground': 816,\n",
       " 'bovis': 817,\n",
       " 'Lopez': 818,\n",
       " 'Me': 819,\n",
       " 'drafted': 4837,\n",
       " '303': 24352,\n",
       " 'Ma': 822,\n",
       " 'oldies': 824,\n",
       " 'Seleucids': 825,\n",
       " 'honour': 826,\n",
       " 'Mk': 827,\n",
       " '305': 31855,\n",
       " 'Mu': 829,\n",
       " 'Mt': 830,\n",
       " 'Mw': 831,\n",
       " 'Latham': 832,\n",
       " 'Ms': 833,\n",
       " 'Mr': 834,\n",
       " 'My': 835,\n",
       " 'SouthPark': 836,\n",
       " 'boleros': 837,\n",
       " 'Webb': 838,\n",
       " 'MD': 839,\n",
       " 'MG': 840,\n",
       " 'MC': 841,\n",
       " 'MB': 842,\n",
       " 'Essence': 843,\n",
       " 'title': 39611,\n",
       " 'MW': 844,\n",
       " 'MV': 845,\n",
       " 'MP': 846,\n",
       " 'MS': 847,\n",
       " 'Bofors': 848,\n",
       " '12-month': 849,\n",
       " 'Solan': 850,\n",
       " 'influx': 851,\n",
       " 'Bom': 852,\n",
       " 'Antes': 853,\n",
       " 'Deccani': 854,\n",
       " 'Giulio': 855,\n",
       " 'POWs': 856,\n",
       " 'Enough': 857,\n",
       " 'Jong': 858,\n",
       " 'Davidson': 859,\n",
       " 'myoglobin': 860,\n",
       " 'undergone': 861,\n",
       " 'August': 862,\n",
       " 'handbags': 863,\n",
       " 'perished': 864,\n",
       " 'DVD-RAM': 865,\n",
       " 'La': 146,\n",
       " 'optimize': 31858,\n",
       " 'Rime': 868,\n",
       " 'cockfighting': 869,\n",
       " 'opposed': 870,\n",
       " 'Bulldogs': 871,\n",
       " 'Lo': 147,\n",
       " 'Those': 873,\n",
       " 'Irir': 874,\n",
       " '164th': 875,\n",
       " 'Klaus': 876,\n",
       " 'Laurussia': 877,\n",
       " 'Ekier': 878,\n",
       " 'assimilation': 5147,\n",
       " 'tundra': 880,\n",
       " 'Henley': 881,\n",
       " 'consoles': 882,\n",
       " 'goods': 31860,\n",
       " 'Burdine': 884,\n",
       " 'riders': 885,\n",
       " 'Bacchanalia': 886,\n",
       " 'lowercase': 887,\n",
       " 'self-termination': 888,\n",
       " 'opioids': 889,\n",
       " 'pre-echo': 890,\n",
       " 'originally': 891,\n",
       " 'Glenn': 31862,\n",
       " 'abortion': 5214,\n",
       " 'following': 896,\n",
       " 'Shantipath': 897,\n",
       " 'Protestantism': 898,\n",
       " 'Magnoliidae': 899,\n",
       " 'mirrors': 900,\n",
       " 'Hecht': 901,\n",
       " 'parachute': 902,\n",
       " 'locks': 903,\n",
       " 'Symphony': 904,\n",
       " 'Amandus': 905,\n",
       " 'Hume': 906,\n",
       " \"d'\\xc3\\x8ele-de-France\": 907,\n",
       " 'Broglie': 908,\n",
       " 'hydrogenases': 909,\n",
       " 'septic': 910,\n",
       " 'Mandyali': 911,\n",
       " 'Plimpton': 912,\n",
       " 'Beichuan': 913,\n",
       " 'Ashkenazim': 5357,\n",
       " 'Skinner': 915,\n",
       " 'Notman': 916,\n",
       " 'Pradhan': 917,\n",
       " '51.7': 918,\n",
       " 'DSV': 40383,\n",
       " 'STA': 919,\n",
       " '356,000': 920,\n",
       " 'Zucker': 163,\n",
       " 'convincingly': 922,\n",
       " 'primordial': 44178,\n",
       " 'STV': 923,\n",
       " 'Wiesel': 924,\n",
       " 'Schr\\xc3\\xb6der': 925,\n",
       " 'sociolinguistics': 926,\n",
       " 'sulfonamides': 927,\n",
       " 'patuet': 928,\n",
       " 'temperate': 930,\n",
       " 'kleertjes': 931,\n",
       " 'conscious': 932,\n",
       " 'Kesselring': 5489,\n",
       " '3,984': 934,\n",
       " 'inhabiting': 935,\n",
       " 'subdivisions': 936,\n",
       " '\\xc3\\x9c-Tsang': 937,\n",
       " 'middle-income': 938,\n",
       " 'swollen': 939,\n",
       " 'Kanye': 940,\n",
       " 'wolves': 941,\n",
       " 'pulled': 942,\n",
       " 'manga': 943,\n",
       " 'Momigliano': 944,\n",
       " 'third-best': 945,\n",
       " 'Peterborough': 946,\n",
       " 'Bowie': 947,\n",
       " 'years': 948,\n",
       " 'professors': 949,\n",
       " 'episodes': 950,\n",
       " 'Trustees': 951,\n",
       " 'Elias': 952,\n",
       " 'de-carbonizing': 953,\n",
       " 'multipath': 5590,\n",
       " 'Docks': 956,\n",
       " 'delayed': 26330,\n",
       " 'Benedictines': 957,\n",
       " 'nomenclature': 18967,\n",
       " 'Assisi': 958,\n",
       " 'suspension': 959,\n",
       " 'troubled': 960,\n",
       " 'Piers': 33213,\n",
       " 'apron': 961,\n",
       " 'school-based': 962,\n",
       " 'civilian': 963,\n",
       " 'Lychee': 964,\n",
       " 'Deutsche': 966,\n",
       " 'indigenous': 967,\n",
       " 'Gordon': 968,\n",
       " 'sorted': 970,\n",
       " 'Guido': 46054,\n",
       " 'Visigothic': 971,\n",
       " '24.9': 972,\n",
       " 'hickory': 973,\n",
       " '24.4': 974,\n",
       " '24.5': 975,\n",
       " '24.6': 976,\n",
       " '24.7': 977,\n",
       " '24.0': 978,\n",
       " 'Yellow': 979,\n",
       " '24.3': 980,\n",
       " 'Bernadette': 981,\n",
       " 'Lesotho': 33518,\n",
       " 'Funafuti': 983,\n",
       " 'fisherman': 984,\n",
       " 'dignified': 7047,\n",
       " 'battleships': 986,\n",
       " 'Anthrax': 987,\n",
       " 'quarter': 989,\n",
       " 'quartet': 990,\n",
       " 'Felix': 39629,\n",
       " 'retrieve': 992,\n",
       " 'bursting': 993,\n",
       " 'n\\xe2\\x80\\x93p\\xe2\\x80\\x93n': 994,\n",
       " 'Mecelle': 995,\n",
       " 'Livy': 996,\n",
       " 'SYN': 39630,\n",
       " 'frames': 31880,\n",
       " 'Live': 999,\n",
       " 'm\\xc3\\xa9thode': 1000,\n",
       " 'entering': 1001,\n",
       " 'Loophole': 1002,\n",
       " 'disasters': 1003,\n",
       " 'POW/MIA': 1004,\n",
       " 'troll': 1005,\n",
       " 'Jiangsu': 1006,\n",
       " 'Brendon': 1007,\n",
       " '792': 1008,\n",
       " 'Michigan': 1010,\n",
       " 'Immelt': 1011,\n",
       " 'seriously': 1012,\n",
       " 'trauma': 1013,\n",
       " 'assimilated': 179,\n",
       " ...}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_answer = set()\n",
    "for sentence in train_answers:\n",
    "    for word in sentence:\n",
    "        vocab_answer.add(word)\n",
    "vocab_answer = [\"#start\",\"#end\"]+ list(vocab_answer)\n",
    "print('posibles palabras para respuestas :', len(vocab_answer))\n",
    "vocabA_indices = {c: i for i, c in enumerate(vocab_answer)}\n",
    "indices_vocabA = {i: c for i, c in enumerate(vocab_answer)}\n",
    "#sameforquestions\n",
    "vocab_question = set()\n",
    "for sentence in train_questions+test_questions:\n",
    "    for word in sentence:\n",
    "        vocab_question.add(word)\n",
    "vocab_question = list(vocab_question)\n",
    "print('posibles palabras para preguntas :', len(vocab_question))\n",
    "vocabQ_indices = {c: i for i, c in enumerate(vocab_question)}\n",
    "              \n",
    "vocabA_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Dimensionalidades input: ', (87598, 60))\n",
      "('Dimensionalidades output: ', (87598, 47))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0, ..., 40772, 29628, 26366],\n",
       "       [    0,     0,     0, ..., 40772, 29628, 26366],\n",
       "       [    0,     0,     0, ..., 11457, 29973, 26366],\n",
       "       ...,\n",
       "       [    0,     0,     0, ...,  3683, 23434, 26366],\n",
       "       [    0,     0,     0, ..., 19859, 37028, 26366],\n",
       "       [    0,     0,     0, ..., 11592, 18350, 26366]], dtype=int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#input and output to onehotvector\n",
    "Xtrain_question = [[vocabQ_indices[palabra] for palabra in sentence] for sentence in train_questions]\n",
    "Xtest_question = [[vocabQ_indices[palabra] for palabra in sentence] for sentence in test_questions]\n",
    "X_answers = [[vocabA_indices[palabra] for palabra in sentence] for sentence in train_answers]\n",
    "\n",
    "import numpy as np\n",
    "max_input_lenght = np.max(list(map(len,train_questions)))\n",
    "max_output_lenght = np.max(list(map(len,train_answers)))+1\n",
    "from keras.preprocessing import sequence\n",
    "Xtrain_question = sequence.pad_sequences(Xtrain_question,maxlen=max_input_lenght,padding='pre',value=0)\n",
    "Xtest_question = sequence.pad_sequences(Xtest_question,maxlen=max_input_lenght,padding='pre',value=0)\n",
    "X_answers = sequence.pad_sequences(X_answers,maxlen=max_output_lenght,padding='post',value=vocabA_indices[\"#end\"])\n",
    "\n",
    "print(\"Dimensionalidades input: \",Xtrain_question.shape)\n",
    "print(\"Dimensionalidades output: \",X_answers.shape)\n",
    "\n",
    "Xtest_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(87598, 47, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_answers = X_answers.reshape(X_answers.shape[0],X_answers.shape[1],1)\n",
    "X_answers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_45 (Embedding)     (None, 60, 32)            1352768   \n",
      "_________________________________________________________________\n",
      "cu_dnngru_5 (CuDNNGRU)       (None, 128)               62208     \n",
      "_________________________________________________________________\n",
      "repeat_vector_16 (RepeatVect (None, 47, 128)           0         \n",
      "_________________________________________________________________\n",
      "cu_dnngru_6 (CuDNNGRU)       (None, 47, 128)           99072     \n",
      "_________________________________________________________________\n",
      "time_distributed_33 (TimeDis (None, 47, 47620)         6142980   \n",
      "=================================================================\n",
      "Total params: 7,657,028\n",
      "Trainable params: 7,657,028\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(\n",
    "    input_dim=len(vocabQ_indices),\n",
    "    output_dim=embedding_vector,\n",
    "    input_length=max_input_lenght))\n",
    "#encoder\n",
    "model.add(CuDNNGRU(hidden_dim,return_sequences=False))\n",
    "model.add(RepeatVector(lenght_output))\n",
    "#decoder\n",
    "model.add(CuDNNGRU(hidden_dim, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(len(vocab_answer), activation='softmax')))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 60)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_9 (Embedding)         (None, 60, 64)       2705536     input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "cu_dnngru_13 (CuDNNGRU)         (None, 60, 128)      74496       embedding_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_13 (TimeDistri (None, 60, 47)       6063        cu_dnngru_13[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "permute_3 (Permute)             (None, 47, 60)       0           time_distributed_13[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 47, 60)       0           permute_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "permute_4 (Permute)             (None, 60, 47)       0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, 47, 128)      0           cu_dnngru_13[0][0]               \n",
      "                                                                 permute_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "cu_dnngru_14 (CuDNNGRU)         (None, 47, 128)      99072       lambda_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_14 (TimeDistri (None, 47, 47620)    6142980     cu_dnngru_14[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 9,028,147\n",
      "Trainable params: 9,028,147\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Encoder-Decoder modelo\n",
    "from keras.layers import Input,CuDNNGRU,RepeatVector,TimeDistributed,Dense,Embedding,Flatten,Activation,Permute,Lambda\n",
    "from keras.models import Sequential,Model\n",
    "from keras import backend as K\n",
    "\n",
    "lenght_output = max_output_lenght\n",
    "hidden_dim = 128\n",
    "embedding_vector = 64\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "from keras.layers import LSTM,Input\n",
    "from keras.models import Model\n",
    "encoder_input = Input(shape=(max_input_lenght,))\n",
    "encoder = Embedding(input_dim=len(vocabQ_indices),output_dim=embedding_vector,input_length=max_input_lenght, mask_zero=True)(encoder_input)\n",
    "encoder = LSTM(hidden_dim, return_sequences=True, unroll=True)(encoder)\n",
    "encoder_last = encoder[:,-1,:]\n",
    "\n",
    "decoder_input = Input(shape=(max_output_lenght,))\n",
    "decoder = Embedding(input_dim=len(vocabA_indices),output_dim=embedding_vector,input_length=max_output_lenght, mask_zero=True)(decoder_input)\n",
    "decoder = LSTM(hidden_dim, return_sequences=True, unroll=True)(decoder, initial_state=[encoder_last, encoder_last])\n",
    "decoder = TimeDistributed(Dense(len(vocab_answer), activation=\"softmax\"))(decoder)\n",
    "\n",
    "model = Model(inputs=[encoder_input, decoder_input], outputs=[decoder])\n",
    "\"\"\"\n",
    "\n",
    "encoder_input = Input(shape=(max_input_lenght,))\n",
    "embedded = Embedding(input_dim=len(vocabQ_indices),output_dim=embedding_vector,input_length=max_input_lenght)(encoder_input)\n",
    "encoder = CuDNNGRU(hidden_dim, return_sequences=True)(embedded)\n",
    "\n",
    "#bidirectional...\n",
    "\n",
    "# compute T' importance for each step T\n",
    "attention = TimeDistributed(Dense(max_output_lenght, activation='tanh'))(encoder)  #weight regularized\n",
    "\n",
    "#softmax a las antenciones sobre todo T\n",
    "attention = Permute([2, 1])(attention)\n",
    "attention = Activation('softmax')(attention) \n",
    "attention = Permute([2, 1])(attention)\n",
    "\n",
    "\n",
    "# apply the attention to encoder\n",
    "def attention_multiply(vects):\n",
    "    encoder, attention = vects\n",
    "    return K.batch_dot(attention,encoder, axes=1)\n",
    "sent_representation = Lambda(attention_multiply)([encoder, attention])\n",
    "\n",
    "\n",
    "decoder = CuDNNGRU(hidden_dim, return_sequences=True)(sent_representation)\n",
    "probabilities = TimeDistributed(Dense(len(vocab_answer), activation=\"softmax\"))(decoder) #weight regularized\n",
    "\n",
    "model = Model(encoder_input,probabilities)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"556pt\" viewBox=\"0.00 0.00 430.00 556.00\" width=\"430pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 552)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-552 426,-552 426,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 140393697495952 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>140393697495952</title>\n",
       "<polygon fill=\"none\" points=\"192,-511.5 192,-547.5 324,-547.5 324,-511.5 192,-511.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"258\" y=\"-525.8\">input_41: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140393697495824 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>140393697495824</title>\n",
       "<polygon fill=\"none\" points=\"174,-438.5 174,-474.5 342,-474.5 342,-438.5 174,-438.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"258\" y=\"-452.8\">embedding_43: Embedding</text>\n",
       "</g>\n",
       "<!-- 140393697495952&#45;&gt;140393697495824 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>140393697495952-&gt;140393697495824</title>\n",
       "<path d=\"M258,-511.4551C258,-503.3828 258,-493.6764 258,-484.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"261.5001,-484.5903 258,-474.5904 254.5001,-484.5904 261.5001,-484.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140393697496144 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>140393697496144</title>\n",
       "<polygon fill=\"none\" points=\"205.5,-365.5 205.5,-401.5 310.5,-401.5 310.5,-365.5 205.5,-365.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"258\" y=\"-379.8\">lstm_39: LSTM</text>\n",
       "</g>\n",
       "<!-- 140393697495824&#45;&gt;140393697496144 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>140393697495824-&gt;140393697496144</title>\n",
       "<path d=\"M258,-438.4551C258,-430.3828 258,-420.6764 258,-411.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"261.5001,-411.5903 258,-401.5904 254.5001,-411.5904 261.5001,-411.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140393696306192 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>140393696306192</title>\n",
       "<polygon fill=\"none\" points=\"0,-292.5 0,-328.5 326,-328.5 326,-292.5 0,-292.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"163\" y=\"-306.8\">time_distributed_29(dense_28): TimeDistributed(Dense)</text>\n",
       "</g>\n",
       "<!-- 140393697496144&#45;&gt;140393696306192 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>140393697496144-&gt;140393696306192</title>\n",
       "<path d=\"M234.5168,-365.4551C222.5275,-356.2422 207.7679,-344.9006 194.7769,-334.918\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"196.6042,-331.9082 186.5422,-328.5904 192.339,-337.4587 196.6042,-331.9082\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140393696308752 -->\n",
       "<g class=\"node\" id=\"node6\">\n",
       "<title>140393696308752</title>\n",
       "<polygon fill=\"none\" points=\"197.5,-146.5 197.5,-182.5 320.5,-182.5 320.5,-146.5 197.5,-146.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"259\" y=\"-160.8\">lambda_5: Lambda</text>\n",
       "</g>\n",
       "<!-- 140393697496144&#45;&gt;140393696308752 -->\n",
       "<g class=\"edge\" id=\"edge5\">\n",
       "<title>140393697496144-&gt;140393696308752</title>\n",
       "<path d=\"M297.7062,-365.4221C312.1748,-356.6392 326.9477,-344.5646 335,-329 342.5562,-314.3944 339.2442,-307.8873 335,-292 324.767,-253.6944 299.3818,-215.378 280.7542,-190.9004\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"283.3869,-188.5845 274.4831,-182.8422 277.8626,-192.8836 283.3869,-188.5845\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140394048682960 -->\n",
       "<g class=\"node\" id=\"node5\">\n",
       "<title>140394048682960</title>\n",
       "<polygon fill=\"none\" points=\"128.5,-219.5 128.5,-255.5 283.5,-255.5 283.5,-219.5 128.5,-219.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"206\" y=\"-233.8\">activation_19: Activation</text>\n",
       "</g>\n",
       "<!-- 140393696306192&#45;&gt;140394048682960 -->\n",
       "<g class=\"edge\" id=\"edge4\">\n",
       "<title>140393696306192-&gt;140394048682960</title>\n",
       "<path d=\"M173.6292,-292.4551C178.6425,-283.9441 184.7259,-273.6165 190.2597,-264.2219\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"193.2843,-265.9831 195.344,-255.5904 187.2529,-262.4303 193.2843,-265.9831\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140394048682960&#45;&gt;140393696308752 -->\n",
       "<g class=\"edge\" id=\"edge6\">\n",
       "<title>140394048682960-&gt;140393696308752</title>\n",
       "<path d=\"M219.1011,-219.4551C225.344,-210.8564 232.9332,-200.4034 239.8099,-190.9316\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"242.823,-192.7388 245.8659,-182.5904 237.1585,-188.6262 242.823,-192.7388\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140393696309072 -->\n",
       "<g class=\"node\" id=\"node7\">\n",
       "<title>140393696309072</title>\n",
       "<polygon fill=\"none\" points=\"206.5,-73.5 206.5,-109.5 311.5,-109.5 311.5,-73.5 206.5,-73.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"259\" y=\"-87.8\">lstm_40: LSTM</text>\n",
       "</g>\n",
       "<!-- 140393696308752&#45;&gt;140393696309072 -->\n",
       "<g class=\"edge\" id=\"edge7\">\n",
       "<title>140393696308752-&gt;140393696309072</title>\n",
       "<path d=\"M259,-146.4551C259,-138.3828 259,-128.6764 259,-119.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"262.5001,-119.5903 259,-109.5904 255.5001,-119.5904 262.5001,-119.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140393695266832 -->\n",
       "<g class=\"node\" id=\"node8\">\n",
       "<title>140393695266832</title>\n",
       "<polygon fill=\"none\" points=\"96,-.5 96,-36.5 422,-36.5 422,-.5 96,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"259\" y=\"-14.8\">time_distributed_30(dense_29): TimeDistributed(Dense)</text>\n",
       "</g>\n",
       "<!-- 140393696309072&#45;&gt;140393695266832 -->\n",
       "<g class=\"edge\" id=\"edge8\">\n",
       "<title>140393696309072-&gt;140393695266832</title>\n",
       "<path d=\"M259,-73.4551C259,-65.3828 259,-55.6764 259,-46.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"262.5001,-46.5903 259,-36.5904 255.5001,-46.5904 262.5001,-46.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(87598, 47, 1)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_decoded = np.zeros_like(X_answers[:,:,0])\n",
    "X_decoded[:, 1:] = X_answers[:,:-1]\n",
    "X_decoded[:, 0] = vocabA_indices[\"#start\"] #START_CHAR_CODE\n",
    "X_decoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(87598, 47)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_decoded[:,:,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 70078 samples, validate on 17520 samples\n",
      "Epoch 1/10\n",
      "70078/70078 [==============================] - 365s 5ms/step - loss: 1.2451 - val_loss: 0.7577\n",
      "Epoch 2/10\n",
      "70078/70078 [==============================] - 341s 5ms/step - loss: 0.5991 - val_loss: 0.7111\n",
      "Epoch 3/10\n",
      "70078/70078 [==============================] - 340s 5ms/step - loss: 0.4861 - val_loss: 0.5744\n",
      "Epoch 4/10\n",
      "70078/70078 [==============================] - 339s 5ms/step - loss: 0.4393 - val_loss: 0.5597\n",
      "Epoch 5/10\n",
      "70078/70078 [==============================] - 338s 5ms/step - loss: 0.4209 - val_loss: 0.5534\n",
      "Epoch 6/10\n",
      "70078/70078 [==============================] - 338s 5ms/step - loss: 0.4109 - val_loss: 0.5497\n",
      "Epoch 7/10\n",
      "70078/70078 [==============================] - 338s 5ms/step - loss: 0.4033 - val_loss: 0.5471\n",
      "Epoch 8/10\n",
      "70078/70078 [==============================] - 338s 5ms/step - loss: 0.3954 - val_loss: 0.5447\n",
      "Epoch 9/10\n",
      "70078/70078 [==============================] - 338s 5ms/step - loss: 0.3864 - val_loss: 0.5412\n",
      "Epoch 10/10\n",
      "70078/70078 [==============================] - 338s 5ms/step - loss: 0.3761 - val_loss: 0.5375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb01e085b90>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([Xtrain_question, X_decoded],X_answers,epochs=10,batch_size=128,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 70078 samples, validate on 17520 samples\n",
      "Epoch 1/10\n",
      "70078/70078 [==============================] - 300s 4ms/step - loss: 1.2198 - val_loss: 0.8321\n",
      "Epoch 2/10\n",
      "70078/70078 [==============================] - 290s 4ms/step - loss: 0.7037 - val_loss: 0.8359\n",
      "Epoch 3/10\n",
      "70078/70078 [==============================] - 290s 4ms/step - loss: 0.6977 - val_loss: 0.8355\n",
      "Epoch 4/10\n",
      "70078/70078 [==============================] - 290s 4ms/step - loss: 0.6885 - val_loss: 0.8342\n",
      "Epoch 5/10\n",
      "70078/70078 [==============================] - 289s 4ms/step - loss: 0.6711 - val_loss: 0.8384\n",
      "Epoch 6/10\n",
      "70078/70078 [==============================] - 290s 4ms/step - loss: 0.6573 - val_loss: 0.8471\n",
      "Epoch 7/10\n",
      "70078/70078 [==============================] - 289s 4ms/step - loss: 0.6468 - val_loss: 0.8470\n",
      "Epoch 8/10\n",
      "70078/70078 [==============================] - 289s 4ms/step - loss: 0.6369 - val_loss: 0.8484\n",
      "Epoch 9/10\n",
      "70078/70078 [==============================] - 288s 4ms/step - loss: 0.6275 - val_loss: 0.8572\n",
      "Epoch 10/10\n",
      "70078/70078 [==============================] - 289s 4ms/step - loss: 0.6185 - val_loss: 0.8585\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2f06a9e910>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(Xtrain_question,X_answers,epochs=10,batch_size=128,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 70078 samples, validate on 17520 samples\n",
      "Epoch 1/10\n",
      "70078/70078 [==============================] - 410s 6ms/step - loss: 1.2264 - val_loss: 0.8371\n",
      "Epoch 2/10\n",
      "70078/70078 [==============================] - 403s 6ms/step - loss: 0.7088 - val_loss: 0.8408\n",
      "Epoch 3/10\n",
      "70078/70078 [==============================] - 404s 6ms/step - loss: 0.7033 - val_loss: 0.8437\n",
      "Epoch 4/10\n",
      "70078/70078 [==============================] - 404s 6ms/step - loss: 0.7000 - val_loss: 0.8429\n",
      "Epoch 5/10\n",
      "70078/70078 [==============================] - 404s 6ms/step - loss: 0.6970 - val_loss: 0.8446\n",
      "Epoch 6/10\n",
      "70078/70078 [==============================] - 403s 6ms/step - loss: 0.6943 - val_loss: 0.8440\n",
      "Epoch 7/10\n",
      "70078/70078 [==============================] - 404s 6ms/step - loss: 0.6928 - val_loss: 0.8493\n",
      "Epoch 8/10\n",
      "70078/70078 [==============================] - 404s 6ms/step - loss: 0.6905 - val_loss: 0.8475\n",
      "Epoch 9/10\n",
      "70078/70078 [==============================] - 403s 6ms/step - loss: 0.6885 - val_loss: 0.8492\n",
      "Epoch 10/10\n",
      "70078/70078 [==============================] - 404s 6ms/step - loss: 0.6866 - val_loss: 0.8479\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7faff42d0ed0>"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(Xtrain_question,X_answers,epochs=10,batch_size=128,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##con word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predecir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ha predecido 1000\n",
      "Ha predecido 1000\n",
      "Ha predecido 1000\n",
      "Ha predecido 1000\n",
      "Ha predecido 1000\n",
      "Ha predecido 1000\n",
      "Ha predecido 1000\n",
      "Ha predecido 1000\n",
      "Ha predecido 1000\n",
      "Ha predecido 1000\n",
      "Los ha predecido todos!\n"
     ]
    }
   ],
   "source": [
    "def predict_words(model,example,diversity=0.9):\n",
    "    probas = model.predict(np.asarray([example]))[0]\n",
    "    probas = np.log(probas) /diversity\n",
    "    exp_preds = np.exp(probas)\n",
    "    probas = exp_preds/np.sum(exp_preds,axis=1)[:,None]  \n",
    "    word_indexes = []\n",
    "    for prob_word in probas:\n",
    "        word_indexes.append(np.random.choice(np.arange(prob_word.shape[0]),p=prob_word) )\n",
    "    return np.asarray(word_indexes)\n",
    "\n",
    "\n",
    "dic_predictions = {}\n",
    "\n",
    "contador=1\n",
    "for example,id_e in zip(Xtest_question,df_test[\"id\"]):\n",
    "    if contador%1000==0:\n",
    "        print(\"Ha predecido 1000\")\n",
    "    indexes_answer = predict_words(model,example) #predice palabra en cada instante\n",
    "    answer = \"\"\n",
    "    for index in indexes_answer:\n",
    "        if indices_vocabA[index]==\"#end\": # el final de la oracion\n",
    "            continue\n",
    "        else:\n",
    "            answer+=indices_vocabA[index]+\" \"\n",
    "    dic_predictions[id_e] = answer\n",
    "    contador+=1\n",
    "print(\"Los ha predecido todos!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf8' codec can't decode byte 0xc4 in position 9: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-85685ec616f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mjson_save\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdic_predictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0marchivo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"predictions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0marchivo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_save\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0marchivo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/casapanshop/anaconda2/lib/python2.7/json/__init__.pyc\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, encoding, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mindent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mseparators\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         encoding == 'utf-8' and default is None and not sort_keys and not kw):\n\u001b[0;32m--> 244\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/casapanshop/anaconda2/lib/python2.7/json/encoder.pyc\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;31m# exceptions aren't as detailed.  The list call should be roughly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0;31m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m         \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_one_shot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m             \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/casapanshop/anaconda2/lib/python2.7/json/encoder.pyc\u001b[0m in \u001b[0;36miterencode\u001b[0;34m(self, o, _one_shot)\u001b[0m\n\u001b[1;32m    268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey_separator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem_separator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m                 self.skipkeys, _one_shot)\n\u001b[0;32m--> 270\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m def _make_iterencode(markers, _default, _encoder, _indent, _floatstr,\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf8' codec can't decode byte 0xc4 in position 9: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "import json\n",
    "json_save = json.dumps(dic_predictions)\n",
    "archivo = open(\"predictions\",\"w\")\n",
    "archivo.write(json_save)\n",
    "archivo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluar resultados\n",
    "!python SQuAD/evaluate-v1.1.py SQuAD/dev-v1.1.json predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Pregunta: ', 'How many NFL teams have gone 15-1 in one season?')\n",
      "('Respuesta: ', '1974 ')\n",
      "('Pregunta: ', 'What network is Newcastle a member of?')\n",
      "('Respuesta: ', '1435\\xe2\\x80\\x931449 government and ')\n",
      "('Pregunta: ', \"How are peridinin-type chloroplasts' thylakoids arranged?\")\n",
      "('Respuesta: ', 'Tuareg or ')\n",
      "('Pregunta: ', 'Other than the US and Britain what was the other main country that Tesla had patents granted?')\n",
      "('Respuesta: ', 'Barack ')\n",
      "('Pregunta: ', 'What is the name of the TV scrambling system BSkyB uses?')\n",
      "('Respuesta: ', 'the ')\n",
      "('Pregunta: ', 'In cases with shared medium how is it delivered ')\n",
      "('Respuesta: ', 'the a Thirteen , during treaties ')\n",
      "('Pregunta: ', 'Each deacon in full connection is a member of what?')\n",
      "('Respuesta: ', 'The Everest Epirus ')\n",
      "('Pregunta: ', 'Who besides the british colonized Africa?')\n",
      "('Respuesta: ', 'Marc constitution ')\n",
      "('Pregunta: ', 'Which two leading roles did Audra McDonald perform when she was in high school?')\n",
      "('Respuesta: ', 'the ')\n",
      "('Pregunta: ', 'Who do clinical pharmacists work with much of the time?')\n",
      "('Respuesta: ', 'Balkan for ')\n",
      "Los ha predecido todos!\n"
     ]
    }
   ],
   "source": [
    "#ejemplo de prediccion\n",
    "n=10\n",
    "\n",
    "for i in range(n):\n",
    "    indexs = np.random.randint(0,len(Xtest_question))\n",
    "    example = Xtest_question[indexs]\n",
    "    indexes_answer = predict_words(model,example,0.85)\n",
    "\n",
    "    question = df_test[\"question\"][indexs]\n",
    "    print(\"Pregunta: \",question)\n",
    "    answer = \"\"\n",
    "    for index in indexes_answer:\n",
    "        if indices_vocabA[index]==\"#end\": # el final de la oracion\n",
    "            continue\n",
    "        else:\n",
    "            answer+=indices_vocabA[index]+\" \"\n",
    "    print(\"Respuesta: \",answer)\n",
    "print(\"Los ha predecido todos!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'input_17:0' shape=(?, 60) dtype=float32>,\n",
       " <tf.Tensor 'input_18:0' shape=(?, 47) dtype=float32>]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.inputs\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
