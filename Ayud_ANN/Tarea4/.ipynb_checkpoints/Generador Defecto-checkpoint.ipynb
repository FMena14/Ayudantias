{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/casapanshop/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 1.0\n",
      "('X_train shape:', (60000, 28, 28, 1))\n",
      "(60000, 'train samples')\n",
      "(10000, 'test samples')\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "import keras\n",
    "img_rows, img_cols,channel = 28, 28, 1\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols,channel)\n",
    "X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols,channel)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "print np.min(X_train), np.max(X_train)\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opcion 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model,Sequential\n",
    "from keras.layers import Input,Dense,BatchNormalization,Convolution2D,UpSampling2D,Activation,LeakyReLU,Reshape,Dropout,Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "opt = Adam(lr=1e-4)\n",
    "dopt = Adam(lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/casapanshop/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:3: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(39200, kernel_initializer=\"glorot_normal\")`\n",
      "  app.launch_new_instance()\n",
      "/home/casapanshop/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:8: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(100, (3, 3), padding=\"same\", kernel_initializer=\"glorot_uniform\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 39200)             3959200   \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 39200)             156800    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 39200)             0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 14, 14, 200)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 28, 28, 200)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 100)       180100    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 28, 28, 100)       400       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 28, 28, 100)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 28, 28, 50)        45050     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 28, 28, 50)        200       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 28, 28, 50)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 28, 28, 1)         51        \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 4,341,801\n",
      "Trainable params: 4,263,101\n",
      "Non-trainable params: 78,700\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/casapanshop/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:11: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(50, (3, 3), padding=\"same\", kernel_initializer=\"glorot_uniform\")`\n",
      "/home/casapanshop/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:14: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(1, (1, 1), padding=\"same\", kernel_initializer=\"glorot_uniform\")`\n"
     ]
    }
   ],
   "source": [
    "nch = 200\n",
    "g_input = Input(shape=[100])\n",
    "H = Dense(nch*14*14, init='glorot_normal')(g_input)\n",
    "H = BatchNormalization()(H)\n",
    "H = Activation('relu')(H)\n",
    "H = Reshape( [14, 14,nch] )(H)\n",
    "H = UpSampling2D(size=(2, 2))(H)\n",
    "H = Convolution2D(nch/2, 3, 3, border_mode='same', init='glorot_uniform')(H)\n",
    "H = BatchNormalization()(H)\n",
    "H = Activation('relu')(H)\n",
    "H = Convolution2D(nch/4, 3, 3, border_mode='same', init='glorot_uniform')(H)\n",
    "H = BatchNormalization()(H)\n",
    "H = Activation('relu')(H)\n",
    "H = Convolution2D(1, 1, 1, border_mode='same', init='glorot_uniform')(H)\n",
    "g_V = Activation('sigmoid')(H)\n",
    "generator = Model(g_input,g_V)\n",
    "generator.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 14, 14, 256)       6656      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 14, 14, 256)       0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 14, 14, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 7, 7, 512)         3277312   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               6422784   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 9,707,009\n",
      "Trainable params: 9,707,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/casapanshop/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:5: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (5, 5), padding=\"same\", strides=(2, 2), activation=\"relu\")`\n",
      "/home/casapanshop/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:8: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (5, 5), padding=\"same\", strides=(2, 2), activation=\"relu\")`\n"
     ]
    }
   ],
   "source": [
    "# Build Discriminative model ...\n",
    "dropout_rate = 0.25\n",
    "shp = X_train.shape[1:]\n",
    "d_input = Input(shape=shp)\n",
    "H = Convolution2D(256, 5, 5, subsample=(2, 2), border_mode = 'same', activation='relu')(d_input)\n",
    "H = LeakyReLU(0.2)(H)\n",
    "H = Dropout(dropout_rate)(H)\n",
    "H = Convolution2D(512, 5, 5, subsample=(2, 2), border_mode = 'same', activation='relu')(H)\n",
    "H = LeakyReLU(0.2)(H)\n",
    "H = Dropout(dropout_rate)(H)\n",
    "H = Flatten()(H)\n",
    "H = Dense(256)(H)\n",
    "H = LeakyReLU(0.2)(H)\n",
    "H = Dropout(dropout_rate)(H)\n",
    "d_V = Dense(1,activation='sigmoid')(H)\n",
    "discriminator = Model(d_input,d_V)\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=dopt)\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "model_1 (Model)              (None, 28, 28, 1)         4341801   \n",
      "_________________________________________________________________\n",
      "model_2 (Model)              (None, 1)                 9707009   \n",
      "=================================================================\n",
      "Total params: 14,048,810\n",
      "Trainable params: 4,263,101\n",
      "Non-trainable params: 9,785,709\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator.trainable = False\n",
    "for l in discriminator.layers:\n",
    "    l.trainable = False\n",
    "\n",
    "# Build stacked GAN model\n",
    "gan_input = Input(shape=[100])\n",
    "H = generator(gan_input)\n",
    "gan_V = discriminator(H)\n",
    "GAN = Model(gan_input, gan_V)\n",
    "GAN.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "GAN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/casapanshop/anaconda2/lib/python2.7/site-packages/keras/engine/training.py:973: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: [D loss: 0.692122, acc: 0.692122]  [G loss: 2.155517, acc: 2.155517]\n",
      "1: [D loss: 0.387036, acc: 0.387036]  [G loss: 0.146086, acc: 0.146086]\n",
      "2: [D loss: 1.146713, acc: 1.146713]  [G loss: 12.450276, acc: 12.450276]\n",
      "3: [D loss: 0.402736, acc: 0.402736]  [G loss: 15.917855, acc: 15.917855]\n",
      "4: [D loss: 0.725004, acc: 0.725004]  [G loss: 13.477327, acc: 13.477327]\n",
      "5: [D loss: 0.046851, acc: 0.046851]  [G loss: 8.673269, acc: 8.673269]\n",
      "6: [D loss: 0.001241, acc: 0.001241]  [G loss: 2.839534, acc: 2.839534]\n",
      "7: [D loss: 0.006813, acc: 0.006813]  [G loss: 0.275466, acc: 0.275466]\n",
      "8: [D loss: 0.329416, acc: 0.329416]  [G loss: 6.747838, acc: 6.747838]\n",
      "9: [D loss: 0.000029, acc: 0.000029]  [G loss: 11.911262, acc: 11.911262]\n",
      "10: [D loss: 0.000061, acc: 0.000061]  [G loss: 15.298594, acc: 15.298594]\n",
      "11: [D loss: 0.000351, acc: 0.000351]  [G loss: 16.087963, acc: 16.087963]\n",
      "12: [D loss: 0.001730, acc: 0.001730]  [G loss: 16.118095, acc: 16.118095]\n",
      "13: [D loss: 0.001632, acc: 0.001632]  [G loss: 16.118095, acc: 16.118095]\n",
      "14: [D loss: 0.034296, acc: 0.034296]  [G loss: 16.118095, acc: 16.118095]\n",
      "15: [D loss: 0.028522, acc: 0.028522]  [G loss: 16.118095, acc: 16.118095]\n",
      "16: [D loss: 0.022145, acc: 0.022145]  [G loss: 16.118095, acc: 16.118095]\n",
      "17: [D loss: 0.001377, acc: 0.001377]  [G loss: 16.118095, acc: 16.118095]\n",
      "18: [D loss: 0.000848, acc: 0.000848]  [G loss: 16.118095, acc: 16.118095]\n",
      "19: [D loss: 0.000034, acc: 0.000034]  [G loss: 16.117411, acc: 16.117411]\n",
      "20: [D loss: 0.000016, acc: 0.000016]  [G loss: 16.078161, acc: 16.078161]\n",
      "21: [D loss: 0.000003, acc: 0.000003]  [G loss: 15.911392, acc: 15.911392]\n",
      "22: [D loss: 0.000000, acc: 0.000000]  [G loss: 15.616585, acc: 15.616585]\n",
      "23: [D loss: 0.000000, acc: 0.000000]  [G loss: 14.919785, acc: 14.919785]\n",
      "24: [D loss: 0.000000, acc: 0.000000]  [G loss: 13.604240, acc: 13.604240]\n",
      "25: [D loss: 0.000000, acc: 0.000000]  [G loss: 12.391581, acc: 12.391581]\n",
      "26: [D loss: 0.000000, acc: 0.000000]  [G loss: 9.944100, acc: 9.944100]\n",
      "27: [D loss: 0.000000, acc: 0.000000]  [G loss: 7.615621, acc: 7.615621]\n",
      "28: [D loss: 0.000000, acc: 0.000000]  [G loss: 5.787283, acc: 5.787283]\n",
      "29: [D loss: 0.000000, acc: 0.000000]  [G loss: 4.163796, acc: 4.163796]\n",
      "30: [D loss: 0.000000, acc: 0.000000]  [G loss: 3.405274, acc: 3.405274]\n",
      "31: [D loss: 0.000000, acc: 0.000000]  [G loss: 2.054159, acc: 2.054159]\n",
      "32: [D loss: 0.000000, acc: 0.000000]  [G loss: 0.859292, acc: 0.859292]\n",
      "33: [D loss: 0.000000, acc: 0.000000]  [G loss: 1.039596, acc: 1.039596]\n",
      "34: [D loss: 0.000000, acc: 0.000000]  [G loss: 0.321195, acc: 0.321195]\n",
      "35: [D loss: 0.000000, acc: 0.000000]  [G loss: 0.440868, acc: 0.440868]\n",
      "36: [D loss: 0.000007, acc: 0.000007]  [G loss: 0.032095, acc: 0.032095]\n",
      "37: [D loss: 0.000012, acc: 0.000012]  [G loss: 0.053650, acc: 0.053650]\n",
      "38: [D loss: 0.006294, acc: 0.006294]  [G loss: 1.077652, acc: 1.077652]\n",
      "39: [D loss: 0.000000, acc: 0.000000]  [G loss: 3.001861, acc: 3.001861]\n",
      "40: [D loss: 0.000000, acc: 0.000000]  [G loss: 4.253119, acc: 4.253119]\n",
      "41: [D loss: 0.000000, acc: 0.000000]  [G loss: 7.219868, acc: 7.219868]\n",
      "42: [D loss: 0.000000, acc: 0.000000]  [G loss: 8.302436, acc: 8.302436]\n",
      "43: [D loss: 0.000000, acc: 0.000000]  [G loss: 9.270718, acc: 9.270718]\n",
      "44: [D loss: 0.000000, acc: 0.000000]  [G loss: 9.028685, acc: 9.028685]\n",
      "45: [D loss: 0.000000, acc: 0.000000]  [G loss: 9.460032, acc: 9.460032]\n",
      "46: [D loss: 0.000000, acc: 0.000000]  [G loss: 9.929876, acc: 9.929876]\n",
      "47: [D loss: 0.000000, acc: 0.000000]  [G loss: 10.153214, acc: 10.153214]\n",
      "48: [D loss: 0.000000, acc: 0.000000]  [G loss: 10.512605, acc: 10.512605]\n",
      "49: [D loss: 0.000000, acc: 0.000000]  [G loss: 9.667903, acc: 9.667903]\n",
      "50: [D loss: 0.000001, acc: 0.000001]  [G loss: 9.808176, acc: 9.808176]\n",
      "51: [D loss: 0.000000, acc: 0.000000]  [G loss: 9.342545, acc: 9.342545]\n",
      "52: [D loss: 0.000148, acc: 0.000148]  [G loss: 8.726404, acc: 8.726404]\n",
      "53: [D loss: 0.000000, acc: 0.000000]  [G loss: 8.300209, acc: 8.300209]\n",
      "54: [D loss: 0.000000, acc: 0.000000]  [G loss: 9.165062, acc: 9.165062]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-b2e7a594a7c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m#make_trainable(discriminator,False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGAN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"g\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/casapanshop/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1881\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1883\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1884\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1885\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/casapanshop/anaconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2478\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2479\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/casapanshop/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/casapanshop/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/casapanshop/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/casapanshop/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/casapanshop/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/casapanshop/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_steps = 1000\n",
    "batch_size = 64\n",
    "history = {\"d\":[],\"g\":[]}\n",
    "for e in range(train_steps):\n",
    "    # Make generative images\n",
    "    image_batch = X_train[np.random.randint(0,X_train.shape[0],size=batch_size),:,:,:] #sample images from real data\n",
    "    noise_gen = np.random.uniform(-1,1,size=[batch_size,input_dim]) #sample image from generated data\n",
    "    generated_images = generator.predict(noise_gen) #fake images\n",
    "\n",
    "    # Train discriminator on generated images\n",
    "    X = np.concatenate((image_batch, generated_images))\n",
    "    #create labels\n",
    "    y = np.ones([2*batch_size,1])\n",
    "    y[batch_size:,:] = 0\n",
    "    \n",
    "    #train discriminator\n",
    "    #make_trainable(discriminator,True)\n",
    "    d_loss  = discriminator.train_on_batch(X,y)\n",
    "    history[\"d\"].append(d_loss)\n",
    "\n",
    "    # train Generator-Discriminator stack on input noise to non-generated output class\n",
    "    noise_tr = np.random.uniform(-1,1,size=[batch_size,input_dim])\n",
    "    y = np.ones([batch_size, 1])\n",
    "\n",
    "    #make_trainable(discriminator,False)\n",
    "    g_loss = GAN.train_on_batch(noise_tr, y)\n",
    "    history[\"g\"].append(g_loss)\n",
    "\n",
    "    log_mesg = \"%d: [D loss: %f, acc: %f]\" % (e, d_loss, d_loss)\n",
    "    log_mesg = \"%s  [G loss: %f, acc: %f]\" % (log_mesg, g_loss, g_loss)\n",
    "    print(log_mesg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f45e3f69050>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N = 10\n",
    "noise = np.random.uniform(-1.0, 1.0, size=[N, input_dim]) \n",
    "\n",
    "images = generator.predict(noise)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(images.shape[0]):\n",
    "    plt.subplot(4, 4, i+1)\n",
    "    image = images[i, :, :, :]\n",
    "    image = np.reshape(image, [img_rows, img_cols])\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opcion2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_10 (Conv2D)           (None, 14, 14, 64)        1664      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 7, 7, 128)         204928    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 4, 4, 256)         819456    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 4, 4, 512)         3277312   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 8193      \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 4,311,553\n",
      "Trainable params: 4,311,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D\n",
    "D = Sequential()\n",
    "depth = 64\n",
    "dropout = 0.4\n",
    "# In: 28 x 28 x 1, depth = 1\n",
    "# Out: 14 x 14 x 1, depth=64\n",
    "input_shape = (img_rows, img_cols, channel)\n",
    "D.add(Conv2D(depth*1, 5, strides=2, input_shape=input_shape,padding='same'))\n",
    "D.add(LeakyReLU(alpha=0.2))\n",
    "D.add(Dropout(dropout))\n",
    "\n",
    "D.add(Conv2D(depth*2, 5, strides=2, padding='same'))\n",
    "D.add(LeakyReLU(alpha=0.2))\n",
    "D.add(Dropout(dropout))\n",
    "\n",
    "D.add(Conv2D(depth*4, 5, strides=2, padding='same'))\n",
    "D.add(LeakyReLU(alpha=0.2))\n",
    "D.add(Dropout(dropout))\n",
    "\n",
    "D.add(Conv2D(depth*8, 5, strides=1, padding='same'))\n",
    "D.add(LeakyReLU(alpha=0.2))\n",
    "D.add(Dropout(dropout))\n",
    "\n",
    "# Out: 1-dim probability\n",
    "D.add(Flatten())\n",
    "D.add(Dense(1))\n",
    "D.add(Activation('sigmoid'))\n",
    "D.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 12544)             1266944   \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 12544)             50176     \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "reshape_4 (Reshape)          (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_5 (UpSampling2 (None, 14, 14, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_5 (Conv2DTr (None, 14, 14, 128)       819328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_6 (UpSampling2 (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_6 (Conv2DTr (None, 28, 28, 64)        204864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_7 (Conv2DTr (None, 28, 28, 32)        51232     \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_8 (Conv2DTr (None, 28, 28, 1)         801       \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 2,394,241\n",
      "Trainable params: 2,368,705\n",
      "Non-trainable params: 25,536\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2DTranspose\n",
    "G = Sequential()\n",
    "dropout = 0.4\n",
    "depth = 64+64+64+64\n",
    "dim = 7\n",
    "# In: 100\n",
    "# Out: dim x dim x depth\n",
    "G.add(Dense(dim*dim*depth, input_dim=100))\n",
    "G.add(BatchNormalization(momentum=0.9))\n",
    "G.add(Activation('relu'))\n",
    "G.add(Reshape((dim, dim, depth)))\n",
    "G.add(Dropout(dropout))\n",
    "\n",
    "# In: dim x dim x depth\n",
    "# Out: 2*dim x 2*dim x depth/2\n",
    "G.add(UpSampling2D())\n",
    "G.add(Conv2DTranspose(int(depth/2), 5, padding='same'))\n",
    "G.add(BatchNormalization(momentum=0.9))\n",
    "G.add(Activation('relu'))\n",
    "\n",
    "G.add(UpSampling2D())\n",
    "G.add(Conv2DTranspose(int(depth/4), 5, padding='same'))\n",
    "G.add(BatchNormalization(momentum=0.9))\n",
    "G.add(Activation('relu'))\n",
    "\n",
    "G.add(Conv2DTranspose(int(depth/8), 5, padding='same'))\n",
    "G.add(BatchNormalization(momentum=0.9))\n",
    "G.add(Activation('relu'))\n",
    "\n",
    "# Out: 28 x 28 x 1 grayscale image [0.0,1.0] per pix\n",
    "G.add(Conv2DTranspose(1, 5, padding='same'))\n",
    "G.add(Activation('sigmoid'))\n",
    "G.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import RMSprop\n",
    "optimizer = RMSprop(lr=0.0002, decay=6e-8)\n",
    "DM = Sequential()\n",
    "DM.add(discriminator)\n",
    "DM.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "generator.trainable = False\n",
    "optimizer = RMSprop(lr=0.0001, decay=3e-8)\n",
    "AM = Sequential()\n",
    "AM.add(generator)\n",
    "AM.add(discriminator)\n",
    "AM.compile(loss='binary_crossentropy', optimizer=optimizer,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: [D loss: 0.000000, acc: 0.000000]  [G loss: 9.228220, acc: 9.228220]\n",
      "1: [D loss: 0.128368, acc: 0.128368]  [G loss: 9.054298, acc: 9.054298]\n",
      "2: [D loss: 0.066198, acc: 0.066198]  [G loss: 11.143276, acc: 11.143276]\n",
      "3: [D loss: 0.000589, acc: 0.000589]  [G loss: 12.562723, acc: 12.562723]\n",
      "4: [D loss: 0.016898, acc: 0.016898]  [G loss: 13.863437, acc: 13.863437]\n",
      "5: [D loss: 0.231520, acc: 0.231520]  [G loss: 12.406153, acc: 12.406153]\n",
      "6: [D loss: 0.068270, acc: 0.068270]  [G loss: 11.245646, acc: 11.245646]\n",
      "7: [D loss: 0.058253, acc: 0.058253]  [G loss: 11.135263, acc: 11.135263]\n",
      "8: [D loss: 0.010805, acc: 0.010805]  [G loss: 10.993715, acc: 10.993715]\n",
      "9: [D loss: 0.000393, acc: 0.000393]  [G loss: 9.836522, acc: 9.836522]\n",
      "10: [D loss: 0.057455, acc: 0.057455]  [G loss: 9.465425, acc: 9.465425]\n",
      "11: [D loss: 0.013105, acc: 0.013105]  [G loss: 10.842360, acc: 10.842360]\n",
      "12: [D loss: 0.000069, acc: 0.000069]  [G loss: 10.686257, acc: 10.686257]\n",
      "13: [D loss: 0.030808, acc: 0.030808]  [G loss: 10.518062, acc: 10.518062]\n",
      "14: [D loss: 0.009336, acc: 0.009336]  [G loss: 10.671994, acc: 10.671994]\n",
      "15: [D loss: 0.017396, acc: 0.017396]  [G loss: 11.809661, acc: 11.809661]\n",
      "16: [D loss: 0.012790, acc: 0.012790]  [G loss: 11.769932, acc: 11.769932]\n",
      "17: [D loss: 0.028334, acc: 0.028334]  [G loss: 11.684784, acc: 11.684784]\n",
      "18: [D loss: 0.008330, acc: 0.008330]  [G loss: 10.904720, acc: 10.904720]\n",
      "19: [D loss: 0.001654, acc: 0.001654]  [G loss: 10.632320, acc: 10.632320]\n",
      "20: [D loss: 0.001564, acc: 0.001564]  [G loss: 11.073997, acc: 11.073997]\n",
      "21: [D loss: 0.000540, acc: 0.000540]  [G loss: 9.906213, acc: 9.906213]\n",
      "22: [D loss: 0.071664, acc: 0.071664]  [G loss: 10.859085, acc: 10.859085]\n",
      "23: [D loss: 0.043344, acc: 0.043344]  [G loss: 10.829835, acc: 10.829835]\n",
      "24: [D loss: 0.022597, acc: 0.022597]  [G loss: 11.181720, acc: 11.181720]\n",
      "25: [D loss: 0.002045, acc: 0.002045]  [G loss: 11.111444, acc: 11.111444]\n",
      "26: [D loss: 0.026015, acc: 0.026015]  [G loss: 11.826920, acc: 11.826920]\n",
      "27: [D loss: 0.041653, acc: 0.041653]  [G loss: 10.836069, acc: 10.836069]\n",
      "28: [D loss: 0.054504, acc: 0.054504]  [G loss: 11.342348, acc: 11.342348]\n",
      "29: [D loss: 0.040997, acc: 0.040997]  [G loss: 10.423472, acc: 10.423472]\n",
      "30: [D loss: 0.038362, acc: 0.038362]  [G loss: 10.342166, acc: 10.342166]\n",
      "31: [D loss: 0.063943, acc: 0.063943]  [G loss: 10.957123, acc: 10.957123]\n",
      "32: [D loss: 0.042879, acc: 0.042879]  [G loss: 11.416449, acc: 11.416449]\n",
      "33: [D loss: 0.021989, acc: 0.021989]  [G loss: 10.821391, acc: 10.821391]\n",
      "34: [D loss: 0.037282, acc: 0.037282]  [G loss: 10.783358, acc: 10.783358]\n",
      "35: [D loss: 0.050930, acc: 0.050930]  [G loss: 9.995433, acc: 9.995433]\n",
      "36: [D loss: 0.051935, acc: 0.051935]  [G loss: 9.977291, acc: 9.977291]\n",
      "37: [D loss: 0.012574, acc: 0.012574]  [G loss: 9.319735, acc: 9.319735]\n",
      "38: [D loss: 0.061151, acc: 0.061151]  [G loss: 10.128972, acc: 10.128972]\n",
      "39: [D loss: 0.004167, acc: 0.004167]  [G loss: 10.827637, acc: 10.827637]\n",
      "40: [D loss: 0.072684, acc: 0.072684]  [G loss: 10.620959, acc: 10.620959]\n",
      "41: [D loss: 0.047482, acc: 0.047482]  [G loss: 10.935166, acc: 10.935166]\n",
      "42: [D loss: 0.021239, acc: 0.021239]  [G loss: 9.708172, acc: 9.708172]\n",
      "43: [D loss: 0.003347, acc: 0.003347]  [G loss: 8.929300, acc: 8.929300]\n",
      "44: [D loss: 0.004330, acc: 0.004330]  [G loss: 9.434525, acc: 9.434525]\n",
      "45: [D loss: 0.029121, acc: 0.029121]  [G loss: 8.340874, acc: 8.340874]\n",
      "46: [D loss: 0.107807, acc: 0.107807]  [G loss: 9.472296, acc: 9.472296]\n",
      "47: [D loss: 0.007211, acc: 0.007211]  [G loss: 10.036455, acc: 10.036455]\n",
      "48: [D loss: 0.006277, acc: 0.006277]  [G loss: 10.354372, acc: 10.354372]\n",
      "49: [D loss: 0.032738, acc: 0.032738]  [G loss: 9.988140, acc: 9.988140]\n",
      "50: [D loss: 0.039026, acc: 0.039026]  [G loss: 9.302307, acc: 9.302307]\n",
      "51: [D loss: 0.013024, acc: 0.013024]  [G loss: 8.776971, acc: 8.776971]\n",
      "52: [D loss: 0.117085, acc: 0.117085]  [G loss: 8.540808, acc: 8.540808]\n",
      "53: [D loss: 0.063640, acc: 0.063640]  [G loss: 8.473309, acc: 8.473309]\n",
      "54: [D loss: 0.008650, acc: 0.008650]  [G loss: 8.068258, acc: 8.068258]\n",
      "55: [D loss: 0.057198, acc: 0.057198]  [G loss: 8.857080, acc: 8.857080]\n",
      "56: [D loss: 0.011741, acc: 0.011741]  [G loss: 9.056092, acc: 9.056092]\n",
      "57: [D loss: 0.147739, acc: 0.147739]  [G loss: 8.201139, acc: 8.201139]\n",
      "58: [D loss: 0.123672, acc: 0.123672]  [G loss: 7.576458, acc: 7.576458]\n",
      "59: [D loss: 0.120223, acc: 0.120223]  [G loss: 8.104218, acc: 8.104218]\n",
      "60: [D loss: 0.152616, acc: 0.152616]  [G loss: 7.979383, acc: 7.979383]\n",
      "61: [D loss: 0.091623, acc: 0.091623]  [G loss: 8.764788, acc: 8.764788]\n",
      "62: [D loss: 0.141685, acc: 0.141685]  [G loss: 8.297548, acc: 8.297548]\n",
      "63: [D loss: 0.070013, acc: 0.070013]  [G loss: 7.569993, acc: 7.569993]\n",
      "64: [D loss: 0.071118, acc: 0.071118]  [G loss: 7.559777, acc: 7.559777]\n",
      "65: [D loss: 0.110704, acc: 0.110704]  [G loss: 7.127841, acc: 7.127841]\n",
      "66: [D loss: 0.089090, acc: 0.089090]  [G loss: 6.790284, acc: 6.790284]\n",
      "67: [D loss: 0.143188, acc: 0.143188]  [G loss: 7.073436, acc: 7.073436]\n",
      "68: [D loss: 0.125490, acc: 0.125490]  [G loss: 6.976015, acc: 6.976015]\n",
      "69: [D loss: 0.078773, acc: 0.078773]  [G loss: 6.855567, acc: 6.855567]\n",
      "70: [D loss: 0.052611, acc: 0.052611]  [G loss: 7.402116, acc: 7.402116]\n",
      "71: [D loss: 0.077389, acc: 0.077389]  [G loss: 7.640772, acc: 7.640772]\n",
      "72: [D loss: 0.043433, acc: 0.043433]  [G loss: 7.737761, acc: 7.737761]\n",
      "73: [D loss: 0.096689, acc: 0.096689]  [G loss: 7.322508, acc: 7.322508]\n",
      "74: [D loss: 0.022902, acc: 0.022902]  [G loss: 7.707803, acc: 7.707803]\n",
      "75: [D loss: 0.044876, acc: 0.044876]  [G loss: 7.847368, acc: 7.847368]\n",
      "76: [D loss: 0.054425, acc: 0.054425]  [G loss: 8.071849, acc: 8.071849]\n",
      "77: [D loss: 0.020260, acc: 0.020260]  [G loss: 8.672701, acc: 8.672701]\n",
      "78: [D loss: 0.030194, acc: 0.030194]  [G loss: 8.931964, acc: 8.931964]\n",
      "79: [D loss: 0.009197, acc: 0.009197]  [G loss: 8.616808, acc: 8.616808]\n",
      "80: [D loss: 0.002840, acc: 0.002840]  [G loss: 8.623394, acc: 8.623394]\n",
      "81: [D loss: 0.007521, acc: 0.007521]  [G loss: 8.395733, acc: 8.395733]\n",
      "82: [D loss: 0.000583, acc: 0.000583]  [G loss: 7.908062, acc: 7.908062]\n",
      "83: [D loss: 0.000382, acc: 0.000382]  [G loss: 7.734725, acc: 7.734725]\n",
      "84: [D loss: 0.000177, acc: 0.000177]  [G loss: 7.688506, acc: 7.688506]\n",
      "85: [D loss: 0.000233, acc: 0.000233]  [G loss: 7.074933, acc: 7.074933]\n",
      "86: [D loss: 0.000077, acc: 0.000077]  [G loss: 6.899899, acc: 6.899899]\n",
      "87: [D loss: 0.000036, acc: 0.000036]  [G loss: 6.985353, acc: 6.985353]\n",
      "88: [D loss: 0.000058, acc: 0.000058]  [G loss: 6.669734, acc: 6.669734]\n",
      "89: [D loss: 0.000005, acc: 0.000005]  [G loss: 6.016172, acc: 6.016172]\n",
      "90: [D loss: 0.000001, acc: 0.000001]  [G loss: 5.932535, acc: 5.932535]\n",
      "91: [D loss: 0.000001, acc: 0.000001]  [G loss: 6.561002, acc: 6.561002]\n",
      "92: [D loss: 0.000000, acc: 0.000000]  [G loss: 6.177150, acc: 6.177150]\n",
      "93: [D loss: 0.000000, acc: 0.000000]  [G loss: 6.107798, acc: 6.107798]\n",
      "94: [D loss: 0.000000, acc: 0.000000]  [G loss: 5.531930, acc: 5.531930]\n",
      "95: [D loss: 0.000000, acc: 0.000000]  [G loss: 5.028306, acc: 5.028306]\n",
      "96: [D loss: 0.000000, acc: 0.000000]  [G loss: 5.565153, acc: 5.565153]\n",
      "97: [D loss: 0.000000, acc: 0.000000]  [G loss: 5.500504, acc: 5.500504]\n",
      "98: [D loss: 0.000000, acc: 0.000000]  [G loss: 4.777987, acc: 4.777987]\n",
      "99: [D loss: 0.000000, acc: 0.000000]  [G loss: 5.503352, acc: 5.503352]\n",
      "100: [D loss: 0.000000, acc: 0.000000]  [G loss: 5.474723, acc: 5.474723]\n",
      "101: [D loss: 0.000000, acc: 0.000000]  [G loss: 5.158261, acc: 5.158261]\n",
      "102: [D loss: 0.000000, acc: 0.000000]  [G loss: 5.156309, acc: 5.156309]\n",
      "103: [D loss: 0.000000, acc: 0.000000]  [G loss: 4.277363, acc: 4.277363]\n",
      "104: [D loss: 0.000000, acc: 0.000000]  [G loss: 4.469570, acc: 4.469570]\n",
      "105: [D loss: 0.000000, acc: 0.000000]  [G loss: 5.172910, acc: 5.172910]\n",
      "106: [D loss: 0.000000, acc: 0.000000]  [G loss: 4.425767, acc: 4.425767]\n",
      "107: [D loss: 0.000000, acc: 0.000000]  [G loss: 4.526621, acc: 4.526621]\n",
      "108: [D loss: 0.000000, acc: 0.000000]  [G loss: 4.688405, acc: 4.688405]\n",
      "109: [D loss: 0.000000, acc: 0.000000]  [G loss: 4.482738, acc: 4.482738]\n",
      "110: [D loss: 0.000000, acc: 0.000000]  [G loss: 4.133900, acc: 4.133900]\n",
      "111: [D loss: 0.000000, acc: 0.000000]  [G loss: 3.574810, acc: 3.574810]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112: [D loss: 0.000000, acc: 0.000000]  [G loss: 4.179914, acc: 4.179914]\n",
      "113: [D loss: 0.000000, acc: 0.000000]  [G loss: 4.267969, acc: 4.267969]\n",
      "114: [D loss: 0.000000, acc: 0.000000]  [G loss: 3.682711, acc: 3.682711]\n",
      "115: [D loss: 0.000000, acc: 0.000000]  [G loss: 3.690500, acc: 3.690500]\n",
      "116: [D loss: 0.000000, acc: 0.000000]  [G loss: 3.318107, acc: 3.318107]\n",
      "117: [D loss: 0.000000, acc: 0.000000]  [G loss: 3.881400, acc: 3.881400]\n",
      "118: [D loss: 0.000000, acc: 0.000000]  [G loss: 3.438669, acc: 3.438669]\n",
      "119: [D loss: 0.000000, acc: 0.000000]  [G loss: 2.736917, acc: 2.736917]\n",
      "120: [D loss: 0.000000, acc: 0.000000]  [G loss: 3.373306, acc: 3.373306]\n",
      "121: [D loss: 0.000000, acc: 0.000000]  [G loss: 3.910410, acc: 3.910410]\n",
      "122: [D loss: 0.000000, acc: 0.000000]  [G loss: 2.998569, acc: 2.998569]\n",
      "123: [D loss: 0.000000, acc: 0.000000]  [G loss: 3.048373, acc: 3.048373]\n",
      "124: [D loss: 0.000000, acc: 0.000000]  [G loss: 3.449547, acc: 3.449547]\n",
      "125: [D loss: 0.000000, acc: 0.000000]  [G loss: 3.194478, acc: 3.194478]\n",
      "126: [D loss: 0.000000, acc: 0.000000]  [G loss: 2.182513, acc: 2.182513]\n",
      "127: [D loss: 0.000000, acc: 0.000000]  [G loss: 2.417175, acc: 2.417175]\n",
      "128: [D loss: 0.000000, acc: 0.000000]  [G loss: 2.790293, acc: 2.790293]\n",
      "129: [D loss: 0.000000, acc: 0.000000]  [G loss: 2.762190, acc: 2.762190]\n",
      "130: [D loss: 0.000000, acc: 0.000000]  [G loss: 2.002633, acc: 2.002633]\n",
      "131: [D loss: 0.000000, acc: 0.000000]  [G loss: 2.016593, acc: 2.016593]\n",
      "132: [D loss: 0.000000, acc: 0.000000]  [G loss: 2.081451, acc: 2.081451]\n",
      "133: [D loss: 0.000000, acc: 0.000000]  [G loss: 1.988167, acc: 1.988167]\n",
      "134: [D loss: 0.000000, acc: 0.000000]  [G loss: 1.794026, acc: 1.794026]\n",
      "135: [D loss: 0.000000, acc: 0.000000]  [G loss: 1.467670, acc: 1.467670]\n",
      "136: [D loss: 0.000000, acc: 0.000000]  [G loss: 1.229476, acc: 1.229476]\n",
      "137: [D loss: 0.000000, acc: 0.000000]  [G loss: 1.439152, acc: 1.439152]\n",
      "138: [D loss: 0.000000, acc: 0.000000]  [G loss: 1.562363, acc: 1.562363]\n",
      "139: [D loss: 0.000001, acc: 0.000001]  [G loss: 1.364027, acc: 1.364027]\n",
      "140: [D loss: 0.000003, acc: 0.000003]  [G loss: 1.090488, acc: 1.090488]\n",
      "141: [D loss: 0.000032, acc: 0.000032]  [G loss: 1.536495, acc: 1.536495]\n",
      "142: [D loss: 0.000034, acc: 0.000034]  [G loss: 0.962633, acc: 0.962633]\n",
      "143: [D loss: 0.000458, acc: 0.000458]  [G loss: 1.182533, acc: 1.182533]\n",
      "144: [D loss: 0.000560, acc: 0.000560]  [G loss: 1.085091, acc: 1.085091]\n",
      "145: [D loss: 0.006585, acc: 0.006585]  [G loss: 1.585545, acc: 1.585545]\n",
      "146: [D loss: 0.000216, acc: 0.000216]  [G loss: 3.206578, acc: 3.206578]\n",
      "147: [D loss: 0.000149, acc: 0.000149]  [G loss: 3.993845, acc: 3.993845]\n",
      "148: [D loss: 0.000028, acc: 0.000028]  [G loss: 4.780484, acc: 4.780484]\n",
      "149: [D loss: 0.000006, acc: 0.000006]  [G loss: 6.009667, acc: 6.009667]\n",
      "150: [D loss: 0.000005, acc: 0.000005]  [G loss: 7.097522, acc: 7.097522]\n",
      "151: [D loss: 0.000001, acc: 0.000001]  [G loss: 7.741680, acc: 7.741680]\n",
      "152: [D loss: 0.000001, acc: 0.000001]  [G loss: 7.978142, acc: 7.978142]\n",
      "153: [D loss: 0.000001, acc: 0.000001]  [G loss: 8.524768, acc: 8.524768]\n",
      "154: [D loss: 0.000001, acc: 0.000001]  [G loss: 8.975298, acc: 8.975298]\n",
      "155: [D loss: 0.000013, acc: 0.000013]  [G loss: 9.614326, acc: 9.614326]\n",
      "156: [D loss: 0.000002, acc: 0.000002]  [G loss: 9.908045, acc: 9.908045]\n",
      "157: [D loss: 0.000002, acc: 0.000002]  [G loss: 9.716350, acc: 9.716350]\n",
      "158: [D loss: 0.000023, acc: 0.000023]  [G loss: 9.840261, acc: 9.840261]\n",
      "159: [D loss: 0.000029, acc: 0.000029]  [G loss: 9.809618, acc: 9.809618]\n",
      "160: [D loss: 0.000009, acc: 0.000009]  [G loss: 10.001764, acc: 10.001764]\n",
      "161: [D loss: 0.000010, acc: 0.000010]  [G loss: 10.103317, acc: 10.103317]\n",
      "162: [D loss: 0.000119, acc: 0.000119]  [G loss: 9.959261, acc: 9.959261]\n",
      "163: [D loss: 0.000005, acc: 0.000005]  [G loss: 10.408813, acc: 10.408813]\n",
      "164: [D loss: 0.000027, acc: 0.000027]  [G loss: 9.899348, acc: 9.899348]\n",
      "165: [D loss: 0.000130, acc: 0.000130]  [G loss: 9.922080, acc: 9.922080]\n",
      "166: [D loss: 0.000182, acc: 0.000182]  [G loss: 9.698828, acc: 9.698828]\n",
      "167: [D loss: 0.000072, acc: 0.000072]  [G loss: 9.862219, acc: 9.862219]\n",
      "168: [D loss: 0.000014, acc: 0.000014]  [G loss: 9.634516, acc: 9.634516]\n",
      "169: [D loss: 0.000060, acc: 0.000060]  [G loss: 9.407101, acc: 9.407101]\n",
      "170: [D loss: 0.000028, acc: 0.000028]  [G loss: 8.817019, acc: 8.817019]\n",
      "171: [D loss: 0.000299, acc: 0.000299]  [G loss: 10.135822, acc: 10.135822]\n",
      "172: [D loss: 0.000081, acc: 0.000081]  [G loss: 10.108108, acc: 10.108108]\n",
      "173: [D loss: 0.000011, acc: 0.000011]  [G loss: 8.919359, acc: 8.919359]\n",
      "174: [D loss: 0.000254, acc: 0.000254]  [G loss: 8.574447, acc: 8.574447]\n",
      "175: [D loss: 0.000031, acc: 0.000031]  [G loss: 8.773479, acc: 8.773479]\n",
      "176: [D loss: 0.000007, acc: 0.000007]  [G loss: 8.474621, acc: 8.474621]\n",
      "177: [D loss: 0.000005, acc: 0.000005]  [G loss: 8.603139, acc: 8.603139]\n",
      "178: [D loss: 0.000012, acc: 0.000012]  [G loss: 8.721119, acc: 8.721119]\n",
      "179: [D loss: 0.000060, acc: 0.000060]  [G loss: 9.107367, acc: 9.107367]\n",
      "180: [D loss: 0.000007, acc: 0.000007]  [G loss: 8.571630, acc: 8.571630]\n",
      "181: [D loss: 0.000036, acc: 0.000036]  [G loss: 8.109489, acc: 8.109489]\n",
      "182: [D loss: 0.000044, acc: 0.000044]  [G loss: 8.429688, acc: 8.429688]\n",
      "183: [D loss: 0.000021, acc: 0.000021]  [G loss: 8.240225, acc: 8.240225]\n",
      "184: [D loss: 0.000216, acc: 0.000216]  [G loss: 7.708773, acc: 7.708773]\n",
      "185: [D loss: 0.000003, acc: 0.000003]  [G loss: 7.914918, acc: 7.914918]\n",
      "186: [D loss: 0.000018, acc: 0.000018]  [G loss: 8.329289, acc: 8.329289]\n",
      "187: [D loss: 0.000029, acc: 0.000029]  [G loss: 8.315180, acc: 8.315180]\n",
      "188: [D loss: 0.001544, acc: 0.001544]  [G loss: 6.967750, acc: 6.967750]\n",
      "189: [D loss: 0.000016, acc: 0.000016]  [G loss: 7.467869, acc: 7.467869]\n",
      "190: [D loss: 0.000034, acc: 0.000034]  [G loss: 7.111614, acc: 7.111614]\n",
      "191: [D loss: 0.000004, acc: 0.000004]  [G loss: 7.292109, acc: 7.292109]\n",
      "192: [D loss: 0.000005, acc: 0.000005]  [G loss: 6.569621, acc: 6.569621]\n",
      "193: [D loss: 0.000009, acc: 0.000009]  [G loss: 6.835402, acc: 6.835402]\n",
      "194: [D loss: 0.000055, acc: 0.000055]  [G loss: 7.059542, acc: 7.059542]\n",
      "195: [D loss: 0.000092, acc: 0.000092]  [G loss: 6.841321, acc: 6.841321]\n",
      "196: [D loss: 0.000041, acc: 0.000041]  [G loss: 6.329889, acc: 6.329889]\n",
      "197: [D loss: 0.000209, acc: 0.000209]  [G loss: 6.255700, acc: 6.255700]\n",
      "198: [D loss: 0.000067, acc: 0.000067]  [G loss: 5.922992, acc: 5.922992]\n",
      "199: [D loss: 0.000353, acc: 0.000353]  [G loss: 6.789173, acc: 6.789173]\n",
      "200: [D loss: 0.002046, acc: 0.002046]  [G loss: 5.317958, acc: 5.317958]\n",
      "201: [D loss: 0.000215, acc: 0.000215]  [G loss: 6.060324, acc: 6.060324]\n",
      "202: [D loss: 0.001921, acc: 0.001921]  [G loss: 5.681499, acc: 5.681499]\n",
      "203: [D loss: 0.002433, acc: 0.002433]  [G loss: 6.899873, acc: 6.899873]\n",
      "204: [D loss: 0.002436, acc: 0.002436]  [G loss: 7.104999, acc: 7.104999]\n",
      "205: [D loss: 0.001202, acc: 0.001202]  [G loss: 8.215845, acc: 8.215845]\n",
      "206: [D loss: 0.000443, acc: 0.000443]  [G loss: 9.999756, acc: 9.999756]\n",
      "207: [D loss: 0.010723, acc: 0.010723]  [G loss: 10.581966, acc: 10.581966]\n",
      "208: [D loss: 0.000725, acc: 0.000725]  [G loss: 9.138939, acc: 9.138939]\n",
      "209: [D loss: 0.007968, acc: 0.007968]  [G loss: 10.266222, acc: 10.266222]\n",
      "210: [D loss: 0.001718, acc: 0.001718]  [G loss: 11.678283, acc: 11.678283]\n",
      "211: [D loss: 0.039751, acc: 0.039751]  [G loss: 8.938234, acc: 8.938234]\n",
      "212: [D loss: 0.006519, acc: 0.006519]  [G loss: 7.764223, acc: 7.764223]\n",
      "213: [D loss: 0.075938, acc: 0.075938]  [G loss: 10.625860, acc: 10.625860]\n",
      "214: [D loss: 0.001436, acc: 0.001436]  [G loss: 14.056466, acc: 14.056466]\n",
      "215: [D loss: 0.322513, acc: 0.322513]  [G loss: 9.024382, acc: 9.024382]\n",
      "216: [D loss: 0.031016, acc: 0.031016]  [G loss: 6.959244, acc: 6.959244]\n",
      "217: [D loss: 0.503468, acc: 0.503468]  [G loss: 8.304873, acc: 8.304873]\n",
      "218: [D loss: 0.054813, acc: 0.054813]  [G loss: 11.452967, acc: 11.452967]\n",
      "219: [D loss: 0.003055, acc: 0.003055]  [G loss: 12.878386, acc: 12.878386]\n",
      "220: [D loss: 0.013831, acc: 0.013831]  [G loss: 14.089096, acc: 14.089096]\n",
      "221: [D loss: 0.165027, acc: 0.165027]  [G loss: 13.437221, acc: 13.437221]\n",
      "222: [D loss: 0.019339, acc: 0.019339]  [G loss: 11.469742, acc: 11.469742]\n",
      "223: [D loss: 0.004356, acc: 0.004356]  [G loss: 10.978142, acc: 10.978142]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224: [D loss: 0.004581, acc: 0.004581]  [G loss: 10.328914, acc: 10.328914]\n",
      "225: [D loss: 0.000450, acc: 0.000450]  [G loss: 9.163323, acc: 9.163323]\n",
      "226: [D loss: 0.005121, acc: 0.005121]  [G loss: 8.444456, acc: 8.444456]\n",
      "227: [D loss: 0.007198, acc: 0.007198]  [G loss: 8.480998, acc: 8.480998]\n",
      "228: [D loss: 0.023275, acc: 0.023275]  [G loss: 8.171905, acc: 8.171905]\n",
      "229: [D loss: 0.025953, acc: 0.025953]  [G loss: 7.994589, acc: 7.994589]\n",
      "230: [D loss: 0.011061, acc: 0.011061]  [G loss: 8.304440, acc: 8.304440]\n",
      "231: [D loss: 0.001810, acc: 0.001810]  [G loss: 7.833671, acc: 7.833671]\n",
      "232: [D loss: 0.011300, acc: 0.011300]  [G loss: 7.715368, acc: 7.715368]\n",
      "233: [D loss: 0.000311, acc: 0.000311]  [G loss: 8.098452, acc: 8.098452]\n",
      "234: [D loss: 0.001005, acc: 0.001005]  [G loss: 7.428854, acc: 7.428854]\n",
      "235: [D loss: 0.001281, acc: 0.001281]  [G loss: 7.817883, acc: 7.817883]\n",
      "236: [D loss: 0.000181, acc: 0.000181]  [G loss: 8.134607, acc: 8.134607]\n",
      "237: [D loss: 0.000377, acc: 0.000377]  [G loss: 7.965560, acc: 7.965560]\n",
      "238: [D loss: 0.000047, acc: 0.000047]  [G loss: 8.146557, acc: 8.146557]\n",
      "239: [D loss: 0.000026, acc: 0.000026]  [G loss: 8.493761, acc: 8.493761]\n",
      "240: [D loss: 0.000090, acc: 0.000090]  [G loss: 7.226556, acc: 7.226556]\n",
      "241: [D loss: 0.000031, acc: 0.000031]  [G loss: 8.591269, acc: 8.591269]\n",
      "242: [D loss: 0.000043, acc: 0.000043]  [G loss: 7.735330, acc: 7.735330]\n",
      "243: [D loss: 0.000037, acc: 0.000037]  [G loss: 8.147975, acc: 8.147975]\n",
      "244: [D loss: 0.000009, acc: 0.000009]  [G loss: 7.667910, acc: 7.667910]\n",
      "245: [D loss: 0.000016, acc: 0.000016]  [G loss: 7.242204, acc: 7.242204]\n",
      "246: [D loss: 0.000008, acc: 0.000008]  [G loss: 7.504206, acc: 7.504206]\n",
      "247: [D loss: 0.000004, acc: 0.000004]  [G loss: 7.441051, acc: 7.441051]\n",
      "248: [D loss: 0.000007, acc: 0.000007]  [G loss: 7.630315, acc: 7.630315]\n",
      "249: [D loss: 0.000004, acc: 0.000004]  [G loss: 7.272632, acc: 7.272632]\n",
      "250: [D loss: 0.000003, acc: 0.000003]  [G loss: 7.605161, acc: 7.605161]\n",
      "251: [D loss: 0.000003, acc: 0.000003]  [G loss: 7.916057, acc: 7.916057]\n",
      "252: [D loss: 0.000003, acc: 0.000003]  [G loss: 7.731676, acc: 7.731676]\n",
      "253: [D loss: 0.000003, acc: 0.000003]  [G loss: 7.684054, acc: 7.684054]\n",
      "254: [D loss: 0.000004, acc: 0.000004]  [G loss: 6.832535, acc: 6.832535]\n",
      "255: [D loss: 0.000002, acc: 0.000002]  [G loss: 7.273499, acc: 7.273499]\n",
      "256: [D loss: 0.000001, acc: 0.000001]  [G loss: 7.186331, acc: 7.186331]\n",
      "257: [D loss: 0.000001, acc: 0.000001]  [G loss: 7.606643, acc: 7.606643]\n",
      "258: [D loss: 0.000001, acc: 0.000001]  [G loss: 7.017821, acc: 7.017821]\n",
      "259: [D loss: 0.000001, acc: 0.000001]  [G loss: 7.183043, acc: 7.183043]\n",
      "260: [D loss: 0.000001, acc: 0.000001]  [G loss: 7.373237, acc: 7.373237]\n",
      "261: [D loss: 0.000001, acc: 0.000001]  [G loss: 6.537008, acc: 6.537008]\n",
      "262: [D loss: 0.000001, acc: 0.000001]  [G loss: 7.289950, acc: 7.289950]\n",
      "263: [D loss: 0.000003, acc: 0.000003]  [G loss: 6.921978, acc: 6.921978]\n",
      "264: [D loss: 0.000001, acc: 0.000001]  [G loss: 6.642133, acc: 6.642133]\n",
      "265: [D loss: 0.000002, acc: 0.000002]  [G loss: 6.941693, acc: 6.941693]\n",
      "266: [D loss: 0.000002, acc: 0.000002]  [G loss: 6.383041, acc: 6.383041]\n",
      "267: [D loss: 0.000001, acc: 0.000001]  [G loss: 6.259073, acc: 6.259073]\n",
      "268: [D loss: 0.000002, acc: 0.000002]  [G loss: 6.158468, acc: 6.158468]\n",
      "269: [D loss: 0.000001, acc: 0.000001]  [G loss: 6.434446, acc: 6.434446]\n",
      "270: [D loss: 0.000000, acc: 0.000000]  [G loss: 6.125169, acc: 6.125169]\n",
      "271: [D loss: 0.000001, acc: 0.000001]  [G loss: 6.250289, acc: 6.250289]\n",
      "272: [D loss: 0.000001, acc: 0.000001]  [G loss: 6.319597, acc: 6.319597]\n",
      "273: [D loss: 0.000001, acc: 0.000001]  [G loss: 6.202930, acc: 6.202930]\n",
      "274: [D loss: 0.000004, acc: 0.000004]  [G loss: 6.801173, acc: 6.801173]\n",
      "275: [D loss: 0.000000, acc: 0.000000]  [G loss: 6.544595, acc: 6.544595]\n",
      "276: [D loss: 0.000001, acc: 0.000001]  [G loss: 5.944592, acc: 5.944592]\n",
      "277: [D loss: 0.000002, acc: 0.000002]  [G loss: 6.549660, acc: 6.549660]\n",
      "278: [D loss: 0.000001, acc: 0.000001]  [G loss: 5.883234, acc: 5.883234]\n",
      "279: [D loss: 0.000001, acc: 0.000001]  [G loss: 6.044880, acc: 6.044880]\n",
      "280: [D loss: 0.000001, acc: 0.000001]  [G loss: 5.978039, acc: 5.978039]\n",
      "281: [D loss: 0.000001, acc: 0.000001]  [G loss: 5.772143, acc: 5.772143]\n",
      "282: [D loss: 0.000001, acc: 0.000001]  [G loss: 5.758862, acc: 5.758862]\n",
      "283: [D loss: 0.000001, acc: 0.000001]  [G loss: 5.781277, acc: 5.781277]\n",
      "284: [D loss: 0.000001, acc: 0.000001]  [G loss: 5.858472, acc: 5.858472]\n",
      "285: [D loss: 0.000002, acc: 0.000002]  [G loss: 5.623664, acc: 5.623664]\n",
      "286: [D loss: 0.000002, acc: 0.000002]  [G loss: 5.279669, acc: 5.279669]\n",
      "287: [D loss: 0.000007, acc: 0.000007]  [G loss: 5.035962, acc: 5.035962]\n",
      "288: [D loss: 0.000002, acc: 0.000002]  [G loss: 5.125466, acc: 5.125466]\n",
      "289: [D loss: 0.000002, acc: 0.000002]  [G loss: 5.131413, acc: 5.131413]\n",
      "290: [D loss: 0.000004, acc: 0.000004]  [G loss: 4.839276, acc: 4.839276]\n",
      "291: [D loss: 0.000007, acc: 0.000007]  [G loss: 4.992163, acc: 4.992163]\n",
      "292: [D loss: 0.000005, acc: 0.000005]  [G loss: 5.539631, acc: 5.539631]\n",
      "293: [D loss: 0.000009, acc: 0.000009]  [G loss: 5.043869, acc: 5.043869]\n",
      "294: [D loss: 0.000005, acc: 0.000005]  [G loss: 5.164449, acc: 5.164449]\n",
      "295: [D loss: 0.000013, acc: 0.000013]  [G loss: 5.107264, acc: 5.107264]\n",
      "296: [D loss: 0.000008, acc: 0.000008]  [G loss: 4.864752, acc: 4.864752]\n",
      "297: [D loss: 0.000040, acc: 0.000040]  [G loss: 4.532593, acc: 4.532593]\n",
      "298: [D loss: 0.000022, acc: 0.000022]  [G loss: 4.501438, acc: 4.501438]\n",
      "299: [D loss: 0.000152, acc: 0.000152]  [G loss: 4.617326, acc: 4.617326]\n",
      "300: [D loss: 0.000191, acc: 0.000191]  [G loss: 4.907572, acc: 4.907572]\n",
      "301: [D loss: 0.000308, acc: 0.000308]  [G loss: 4.628075, acc: 4.628075]\n",
      "302: [D loss: 0.003064, acc: 0.003064]  [G loss: 4.931325, acc: 4.931325]\n",
      "303: [D loss: 0.000242, acc: 0.000242]  [G loss: 4.694773, acc: 4.694773]\n",
      "304: [D loss: 0.000653, acc: 0.000653]  [G loss: 4.843695, acc: 4.843695]\n",
      "305: [D loss: 0.000640, acc: 0.000640]  [G loss: 4.359001, acc: 4.359001]\n",
      "306: [D loss: 0.000912, acc: 0.000912]  [G loss: 5.054820, acc: 5.054820]\n",
      "307: [D loss: 0.008215, acc: 0.008215]  [G loss: 4.703777, acc: 4.703777]\n",
      "308: [D loss: 0.002708, acc: 0.002708]  [G loss: 4.575902, acc: 4.575902]\n",
      "309: [D loss: 0.001195, acc: 0.001195]  [G loss: 5.742658, acc: 5.742658]\n",
      "310: [D loss: 0.006836, acc: 0.006836]  [G loss: 5.428547, acc: 5.428547]\n",
      "311: [D loss: 0.000305, acc: 0.000305]  [G loss: 5.692716, acc: 5.692716]\n",
      "312: [D loss: 0.000261, acc: 0.000261]  [G loss: 6.759612, acc: 6.759612]\n",
      "313: [D loss: 0.000269, acc: 0.000269]  [G loss: 6.684028, acc: 6.684028]\n",
      "314: [D loss: 0.000318, acc: 0.000318]  [G loss: 6.786741, acc: 6.786741]\n",
      "315: [D loss: 0.000905, acc: 0.000905]  [G loss: 7.522915, acc: 7.522915]\n",
      "316: [D loss: 0.000496, acc: 0.000496]  [G loss: 7.980769, acc: 7.980769]\n",
      "317: [D loss: 0.000127, acc: 0.000127]  [G loss: 7.770998, acc: 7.770998]\n",
      "318: [D loss: 0.001185, acc: 0.001185]  [G loss: 7.912693, acc: 7.912693]\n",
      "319: [D loss: 0.001023, acc: 0.001023]  [G loss: 7.941687, acc: 7.941687]\n",
      "320: [D loss: 0.000500, acc: 0.000500]  [G loss: 8.926304, acc: 8.926304]\n",
      "321: [D loss: 0.000876, acc: 0.000876]  [G loss: 7.915083, acc: 7.915083]\n",
      "322: [D loss: 0.000695, acc: 0.000695]  [G loss: 8.187809, acc: 8.187809]\n",
      "323: [D loss: 0.000692, acc: 0.000692]  [G loss: 8.973341, acc: 8.973341]\n",
      "324: [D loss: 0.012561, acc: 0.012561]  [G loss: 8.573097, acc: 8.573097]\n",
      "325: [D loss: 0.002901, acc: 0.002901]  [G loss: 8.627956, acc: 8.627956]\n",
      "326: [D loss: 0.002098, acc: 0.002098]  [G loss: 9.916126, acc: 9.916126]\n",
      "327: [D loss: 0.003578, acc: 0.003578]  [G loss: 10.481987, acc: 10.481987]\n",
      "328: [D loss: 0.016494, acc: 0.016494]  [G loss: 9.434908, acc: 9.434908]\n",
      "329: [D loss: 0.004533, acc: 0.004533]  [G loss: 9.385678, acc: 9.385678]\n",
      "330: [D loss: 0.006659, acc: 0.006659]  [G loss: 8.906607, acc: 8.906607]\n",
      "331: [D loss: 0.007888, acc: 0.007888]  [G loss: 9.236543, acc: 9.236543]\n",
      "332: [D loss: 0.021404, acc: 0.021404]  [G loss: 9.302146, acc: 9.302146]\n",
      "333: [D loss: 0.003489, acc: 0.003489]  [G loss: 8.865604, acc: 8.865604]\n",
      "334: [D loss: 0.005811, acc: 0.005811]  [G loss: 10.307426, acc: 10.307426]\n",
      "335: [D loss: 0.024905, acc: 0.024905]  [G loss: 10.036087, acc: 10.036087]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "336: [D loss: 0.005273, acc: 0.005273]  [G loss: 9.885784, acc: 9.885784]\n",
      "337: [D loss: 0.012482, acc: 0.012482]  [G loss: 8.682762, acc: 8.682762]\n",
      "338: [D loss: 0.015499, acc: 0.015499]  [G loss: 11.387836, acc: 11.387836]\n",
      "339: [D loss: 0.009183, acc: 0.009183]  [G loss: 10.819653, acc: 10.819653]\n",
      "340: [D loss: 0.008945, acc: 0.008945]  [G loss: 10.878295, acc: 10.878295]\n",
      "341: [D loss: 0.023335, acc: 0.023335]  [G loss: 10.933128, acc: 10.933128]\n",
      "342: [D loss: 0.032027, acc: 0.032027]  [G loss: 11.949594, acc: 11.949594]\n",
      "343: [D loss: 0.008187, acc: 0.008187]  [G loss: 11.142960, acc: 11.142960]\n",
      "344: [D loss: 0.003614, acc: 0.003614]  [G loss: 13.741137, acc: 13.741137]\n",
      "345: [D loss: 0.006391, acc: 0.006391]  [G loss: 12.076709, acc: 12.076709]\n",
      "346: [D loss: 0.029632, acc: 0.029632]  [G loss: 10.035436, acc: 10.035436]\n",
      "347: [D loss: 0.064165, acc: 0.064165]  [G loss: 11.350819, acc: 11.350819]\n",
      "348: [D loss: 0.014231, acc: 0.014231]  [G loss: 12.578072, acc: 12.578072]\n",
      "349: [D loss: 0.015347, acc: 0.015347]  [G loss: 12.399525, acc: 12.399525]\n",
      "350: [D loss: 0.012476, acc: 0.012476]  [G loss: 12.031892, acc: 12.031892]\n",
      "351: [D loss: 0.007055, acc: 0.007055]  [G loss: 10.625288, acc: 10.625288]\n",
      "352: [D loss: 0.007507, acc: 0.007507]  [G loss: 10.969574, acc: 10.969574]\n",
      "353: [D loss: 0.036197, acc: 0.036197]  [G loss: 12.178738, acc: 12.178738]\n",
      "354: [D loss: 0.003622, acc: 0.003622]  [G loss: 13.489321, acc: 13.489321]\n",
      "355: [D loss: 0.077607, acc: 0.077607]  [G loss: 13.329273, acc: 13.329273]\n",
      "356: [D loss: 0.004837, acc: 0.004837]  [G loss: 12.670982, acc: 12.670982]\n",
      "357: [D loss: 0.029144, acc: 0.029144]  [G loss: 10.642975, acc: 10.642975]\n",
      "358: [D loss: 0.093516, acc: 0.093516]  [G loss: 12.906783, acc: 12.906783]\n",
      "359: [D loss: 0.046823, acc: 0.046823]  [G loss: 13.932713, acc: 13.932713]\n",
      "360: [D loss: 0.125746, acc: 0.125746]  [G loss: 10.034868, acc: 10.034868]\n",
      "361: [D loss: 0.045409, acc: 0.045409]  [G loss: 9.100409, acc: 9.100409]\n",
      "362: [D loss: 0.114934, acc: 0.114934]  [G loss: 10.942322, acc: 10.942322]\n",
      "363: [D loss: 0.023104, acc: 0.023104]  [G loss: 12.340796, acc: 12.340796]\n",
      "364: [D loss: 0.015511, acc: 0.015511]  [G loss: 13.747250, acc: 13.747250]\n",
      "365: [D loss: 0.056084, acc: 0.056084]  [G loss: 12.970947, acc: 12.970947]\n",
      "366: [D loss: 0.005600, acc: 0.005600]  [G loss: 12.036706, acc: 12.036706]\n",
      "367: [D loss: 0.004749, acc: 0.004749]  [G loss: 11.595295, acc: 11.595295]\n",
      "368: [D loss: 0.008231, acc: 0.008231]  [G loss: 11.051136, acc: 11.051136]\n",
      "369: [D loss: 0.055432, acc: 0.055432]  [G loss: 11.559532, acc: 11.559532]\n",
      "370: [D loss: 0.057533, acc: 0.057533]  [G loss: 13.161719, acc: 13.161719]\n",
      "371: [D loss: 0.026517, acc: 0.026517]  [G loss: 12.821544, acc: 12.821544]\n",
      "372: [D loss: 0.072299, acc: 0.072299]  [G loss: 11.402151, acc: 11.402151]\n",
      "373: [D loss: 0.022085, acc: 0.022085]  [G loss: 9.840836, acc: 9.840836]\n",
      "374: [D loss: 0.064757, acc: 0.064757]  [G loss: 11.171353, acc: 11.171353]\n",
      "375: [D loss: 0.075029, acc: 0.075029]  [G loss: 13.521622, acc: 13.521622]\n",
      "376: [D loss: 0.036840, acc: 0.036840]  [G loss: 13.150252, acc: 13.150252]\n",
      "377: [D loss: 0.124739, acc: 0.124739]  [G loss: 10.945814, acc: 10.945814]\n",
      "378: [D loss: 0.146019, acc: 0.146019]  [G loss: 10.079290, acc: 10.079290]\n",
      "379: [D loss: 0.172307, acc: 0.172307]  [G loss: 13.906236, acc: 13.906236]\n",
      "380: [D loss: 0.053656, acc: 0.053656]  [G loss: 15.297217, acc: 15.297217]\n",
      "381: [D loss: 0.373499, acc: 0.373499]  [G loss: 9.749737, acc: 9.749737]\n",
      "382: [D loss: 0.325618, acc: 0.325618]  [G loss: 9.239287, acc: 9.239287]\n",
      "383: [D loss: 0.460600, acc: 0.460600]  [G loss: 11.554914, acc: 11.554914]\n",
      "384: [D loss: 0.168995, acc: 0.168995]  [G loss: 13.354044, acc: 13.354044]\n",
      "385: [D loss: 0.413889, acc: 0.413889]  [G loss: 12.510598, acc: 12.510598]\n",
      "386: [D loss: 0.383174, acc: 0.383174]  [G loss: 11.352094, acc: 11.352094]\n",
      "387: [D loss: 0.089763, acc: 0.089763]  [G loss: 8.942341, acc: 8.942341]\n",
      "388: [D loss: 0.286362, acc: 0.286362]  [G loss: 6.960503, acc: 6.960503]\n",
      "389: [D loss: 0.495039, acc: 0.495039]  [G loss: 7.664211, acc: 7.664211]\n",
      "390: [D loss: 0.236767, acc: 0.236767]  [G loss: 8.479380, acc: 8.479380]\n",
      "391: [D loss: 0.130515, acc: 0.130515]  [G loss: 9.158420, acc: 9.158420]\n",
      "392: [D loss: 0.211822, acc: 0.211822]  [G loss: 9.714156, acc: 9.714156]\n",
      "393: [D loss: 0.348631, acc: 0.348631]  [G loss: 9.035242, acc: 9.035242]\n",
      "394: [D loss: 0.249001, acc: 0.249001]  [G loss: 8.498070, acc: 8.498070]\n",
      "395: [D loss: 0.143685, acc: 0.143685]  [G loss: 7.515098, acc: 7.515098]\n",
      "396: [D loss: 0.108073, acc: 0.108073]  [G loss: 6.018233, acc: 6.018233]\n",
      "397: [D loss: 0.174077, acc: 0.174077]  [G loss: 5.634973, acc: 5.634973]\n",
      "398: [D loss: 0.164697, acc: 0.164697]  [G loss: 5.434673, acc: 5.434673]\n",
      "399: [D loss: 0.199706, acc: 0.199706]  [G loss: 6.041630, acc: 6.041630]\n",
      "400: [D loss: 0.168531, acc: 0.168531]  [G loss: 6.575053, acc: 6.575053]\n",
      "401: [D loss: 0.119510, acc: 0.119510]  [G loss: 7.768669, acc: 7.768669]\n",
      "402: [D loss: 0.101092, acc: 0.101092]  [G loss: 7.861355, acc: 7.861355]\n",
      "403: [D loss: 0.128867, acc: 0.128867]  [G loss: 8.848019, acc: 8.848019]\n",
      "404: [D loss: 0.143769, acc: 0.143769]  [G loss: 8.575682, acc: 8.575682]\n",
      "405: [D loss: 0.101331, acc: 0.101331]  [G loss: 7.853263, acc: 7.853263]\n",
      "406: [D loss: 0.040755, acc: 0.040755]  [G loss: 6.877929, acc: 6.877929]\n",
      "407: [D loss: 0.066250, acc: 0.066250]  [G loss: 6.557512, acc: 6.557512]\n",
      "408: [D loss: 0.066378, acc: 0.066378]  [G loss: 6.906222, acc: 6.906222]\n",
      "409: [D loss: 0.064737, acc: 0.064737]  [G loss: 7.161673, acc: 7.161673]\n",
      "410: [D loss: 0.056839, acc: 0.056839]  [G loss: 8.898332, acc: 8.898332]\n",
      "411: [D loss: 0.036422, acc: 0.036422]  [G loss: 8.834007, acc: 8.834007]\n",
      "412: [D loss: 0.055228, acc: 0.055228]  [G loss: 9.198744, acc: 9.198744]\n",
      "413: [D loss: 0.050593, acc: 0.050593]  [G loss: 9.080974, acc: 9.080974]\n",
      "414: [D loss: 0.031099, acc: 0.031099]  [G loss: 8.284494, acc: 8.284494]\n",
      "415: [D loss: 0.022406, acc: 0.022406]  [G loss: 8.379677, acc: 8.379677]\n",
      "416: [D loss: 0.074643, acc: 0.074643]  [G loss: 7.662489, acc: 7.662489]\n",
      "417: [D loss: 0.045144, acc: 0.045144]  [G loss: 8.310139, acc: 8.310139]\n",
      "418: [D loss: 0.104655, acc: 0.104655]  [G loss: 10.953706, acc: 10.953706]\n",
      "419: [D loss: 0.091795, acc: 0.091795]  [G loss: 11.591194, acc: 11.591194]\n",
      "420: [D loss: 0.114078, acc: 0.114078]  [G loss: 11.846520, acc: 11.846520]\n",
      "421: [D loss: 0.055187, acc: 0.055187]  [G loss: 8.967493, acc: 8.967493]\n",
      "422: [D loss: 0.040012, acc: 0.040012]  [G loss: 7.830450, acc: 7.830450]\n",
      "423: [D loss: 0.104818, acc: 0.104818]  [G loss: 8.685661, acc: 8.685661]\n",
      "424: [D loss: 0.103298, acc: 0.103298]  [G loss: 10.086954, acc: 10.086954]\n",
      "425: [D loss: 0.039997, acc: 0.039997]  [G loss: 11.606511, acc: 11.606511]\n",
      "426: [D loss: 0.058808, acc: 0.058808]  [G loss: 11.798922, acc: 11.798922]\n",
      "427: [D loss: 0.079247, acc: 0.079247]  [G loss: 12.367037, acc: 12.367037]\n",
      "428: [D loss: 0.060086, acc: 0.060086]  [G loss: 11.547978, acc: 11.547978]\n",
      "429: [D loss: 0.015473, acc: 0.015473]  [G loss: 9.179430, acc: 9.179430]\n",
      "430: [D loss: 0.015628, acc: 0.015628]  [G loss: 8.470804, acc: 8.470804]\n",
      "431: [D loss: 0.019083, acc: 0.019083]  [G loss: 7.881520, acc: 7.881520]\n",
      "432: [D loss: 0.018238, acc: 0.018238]  [G loss: 7.810747, acc: 7.810747]\n",
      "433: [D loss: 0.008943, acc: 0.008943]  [G loss: 8.161441, acc: 8.161441]\n",
      "434: [D loss: 0.009718, acc: 0.009718]  [G loss: 7.868059, acc: 7.868059]\n",
      "435: [D loss: 0.007700, acc: 0.007700]  [G loss: 8.851617, acc: 8.851617]\n",
      "436: [D loss: 0.004860, acc: 0.004860]  [G loss: 8.566087, acc: 8.566087]\n",
      "437: [D loss: 0.003928, acc: 0.003928]  [G loss: 8.509375, acc: 8.509375]\n",
      "438: [D loss: 0.010235, acc: 0.010235]  [G loss: 8.723141, acc: 8.723141]\n",
      "439: [D loss: 0.001200, acc: 0.001200]  [G loss: 10.657512, acc: 10.657512]\n",
      "440: [D loss: 0.003028, acc: 0.003028]  [G loss: 9.177794, acc: 9.177794]\n",
      "441: [D loss: 0.008079, acc: 0.008079]  [G loss: 9.369703, acc: 9.369703]\n",
      "442: [D loss: 0.001399, acc: 0.001399]  [G loss: 10.261740, acc: 10.261740]\n",
      "443: [D loss: 0.001034, acc: 0.001034]  [G loss: 9.706696, acc: 9.706696]\n",
      "444: [D loss: 0.000855, acc: 0.000855]  [G loss: 8.624297, acc: 8.624297]\n",
      "445: [D loss: 0.001274, acc: 0.001274]  [G loss: 9.127777, acc: 9.127777]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "446: [D loss: 0.000690, acc: 0.000690]  [G loss: 8.451702, acc: 8.451702]\n",
      "447: [D loss: 0.000895, acc: 0.000895]  [G loss: 8.823352, acc: 8.823352]\n",
      "448: [D loss: 0.000456, acc: 0.000456]  [G loss: 7.637539, acc: 7.637539]\n",
      "449: [D loss: 0.002937, acc: 0.002937]  [G loss: 8.500250, acc: 8.500250]\n",
      "450: [D loss: 0.001104, acc: 0.001104]  [G loss: 7.668582, acc: 7.668582]\n",
      "451: [D loss: 0.001754, acc: 0.001754]  [G loss: 6.812075, acc: 6.812075]\n",
      "452: [D loss: 0.000379, acc: 0.000379]  [G loss: 6.800544, acc: 6.800544]\n",
      "453: [D loss: 0.000685, acc: 0.000685]  [G loss: 8.057172, acc: 8.057172]\n",
      "454: [D loss: 0.000168, acc: 0.000168]  [G loss: 6.445419, acc: 6.445419]\n",
      "455: [D loss: 0.000450, acc: 0.000450]  [G loss: 6.109908, acc: 6.109908]\n",
      "456: [D loss: 0.000574, acc: 0.000574]  [G loss: 6.333811, acc: 6.333811]\n",
      "457: [D loss: 0.000280, acc: 0.000280]  [G loss: 6.010811, acc: 6.010811]\n",
      "458: [D loss: 0.000711, acc: 0.000711]  [G loss: 6.291347, acc: 6.291347]\n",
      "459: [D loss: 0.000527, acc: 0.000527]  [G loss: 5.581758, acc: 5.581758]\n",
      "460: [D loss: 0.000867, acc: 0.000867]  [G loss: 6.303984, acc: 6.303984]\n",
      "461: [D loss: 0.000729, acc: 0.000729]  [G loss: 5.915981, acc: 5.915981]\n",
      "462: [D loss: 0.001898, acc: 0.001898]  [G loss: 5.954436, acc: 5.954436]\n",
      "463: [D loss: 0.000463, acc: 0.000463]  [G loss: 6.051254, acc: 6.051254]\n",
      "464: [D loss: 0.000156, acc: 0.000156]  [G loss: 5.376740, acc: 5.376740]\n",
      "465: [D loss: 0.000151, acc: 0.000151]  [G loss: 5.551787, acc: 5.551787]\n",
      "466: [D loss: 0.000414, acc: 0.000414]  [G loss: 5.845350, acc: 5.845350]\n",
      "467: [D loss: 0.000256, acc: 0.000256]  [G loss: 5.599671, acc: 5.599671]\n",
      "468: [D loss: 0.001010, acc: 0.001010]  [G loss: 5.704993, acc: 5.704993]\n",
      "469: [D loss: 0.000137, acc: 0.000137]  [G loss: 6.021981, acc: 6.021981]\n",
      "470: [D loss: 0.000254, acc: 0.000254]  [G loss: 6.087611, acc: 6.087611]\n",
      "471: [D loss: 0.000398, acc: 0.000398]  [G loss: 5.636819, acc: 5.636819]\n",
      "472: [D loss: 0.000277, acc: 0.000277]  [G loss: 5.641071, acc: 5.641071]\n",
      "473: [D loss: 0.000416, acc: 0.000416]  [G loss: 6.026079, acc: 6.026079]\n",
      "474: [D loss: 0.000242, acc: 0.000242]  [G loss: 6.002383, acc: 6.002383]\n",
      "475: [D loss: 0.000267, acc: 0.000267]  [G loss: 6.220981, acc: 6.220981]\n",
      "476: [D loss: 0.000265, acc: 0.000265]  [G loss: 5.812904, acc: 5.812904]\n",
      "477: [D loss: 0.000316, acc: 0.000316]  [G loss: 5.947268, acc: 5.947268]\n",
      "478: [D loss: 0.000489, acc: 0.000489]  [G loss: 5.741469, acc: 5.741469]\n",
      "479: [D loss: 0.006446, acc: 0.006446]  [G loss: 5.867889, acc: 5.867889]\n",
      "480: [D loss: 0.005342, acc: 0.005342]  [G loss: 6.310206, acc: 6.310206]\n",
      "481: [D loss: 0.022880, acc: 0.022880]  [G loss: 6.486846, acc: 6.486846]\n",
      "482: [D loss: 0.000591, acc: 0.000591]  [G loss: 6.090142, acc: 6.090142]\n",
      "483: [D loss: 0.003885, acc: 0.003885]  [G loss: 6.376742, acc: 6.376742]\n",
      "484: [D loss: 0.005886, acc: 0.005886]  [G loss: 6.266628, acc: 6.266628]\n",
      "485: [D loss: 0.000522, acc: 0.000522]  [G loss: 5.331141, acc: 5.331141]\n",
      "486: [D loss: 0.000579, acc: 0.000579]  [G loss: 5.311693, acc: 5.311693]\n",
      "487: [D loss: 0.000659, acc: 0.000659]  [G loss: 5.092937, acc: 5.092937]\n",
      "488: [D loss: 0.002659, acc: 0.002659]  [G loss: 4.960767, acc: 4.960767]\n",
      "489: [D loss: 0.002818, acc: 0.002818]  [G loss: 4.807928, acc: 4.807928]\n",
      "490: [D loss: 0.007211, acc: 0.007211]  [G loss: 5.218690, acc: 5.218690]\n",
      "491: [D loss: 0.001009, acc: 0.001009]  [G loss: 5.984921, acc: 5.984921]\n",
      "492: [D loss: 0.001467, acc: 0.001467]  [G loss: 6.502867, acc: 6.502867]\n",
      "493: [D loss: 0.000774, acc: 0.000774]  [G loss: 6.537347, acc: 6.537347]\n",
      "494: [D loss: 0.000738, acc: 0.000738]  [G loss: 7.272734, acc: 7.272734]\n",
      "495: [D loss: 0.009541, acc: 0.009541]  [G loss: 6.449000, acc: 6.449000]\n",
      "496: [D loss: 0.000526, acc: 0.000526]  [G loss: 6.722551, acc: 6.722551]\n",
      "497: [D loss: 0.002957, acc: 0.002957]  [G loss: 7.864149, acc: 7.864149]\n",
      "498: [D loss: 0.001233, acc: 0.001233]  [G loss: 7.268222, acc: 7.268222]\n",
      "499: [D loss: 0.018185, acc: 0.018185]  [G loss: 7.489483, acc: 7.489483]\n",
      "500: [D loss: 0.007611, acc: 0.007611]  [G loss: 6.546748, acc: 6.546748]\n",
      "501: [D loss: 0.000559, acc: 0.000559]  [G loss: 6.684996, acc: 6.684996]\n",
      "502: [D loss: 0.007877, acc: 0.007877]  [G loss: 5.607673, acc: 5.607673]\n",
      "503: [D loss: 0.024179, acc: 0.024179]  [G loss: 6.607821, acc: 6.607821]\n",
      "504: [D loss: 0.002062, acc: 0.002062]  [G loss: 7.133187, acc: 7.133187]\n",
      "505: [D loss: 0.002148, acc: 0.002148]  [G loss: 8.118034, acc: 8.118034]\n",
      "506: [D loss: 0.000224, acc: 0.000224]  [G loss: 8.985950, acc: 8.985950]\n",
      "507: [D loss: 0.003442, acc: 0.003442]  [G loss: 9.737057, acc: 9.737057]\n",
      "508: [D loss: 0.011431, acc: 0.011431]  [G loss: 9.385237, acc: 9.385237]\n",
      "509: [D loss: 0.003356, acc: 0.003356]  [G loss: 9.442244, acc: 9.442244]\n",
      "510: [D loss: 0.000310, acc: 0.000310]  [G loss: 8.574646, acc: 8.574646]\n",
      "511: [D loss: 0.000386, acc: 0.000386]  [G loss: 8.129396, acc: 8.129396]\n",
      "512: [D loss: 0.000387, acc: 0.000387]  [G loss: 8.188222, acc: 8.188222]\n",
      "513: [D loss: 0.000864, acc: 0.000864]  [G loss: 7.744399, acc: 7.744399]\n",
      "514: [D loss: 0.001127, acc: 0.001127]  [G loss: 7.030430, acc: 7.030430]\n",
      "515: [D loss: 0.001518, acc: 0.001518]  [G loss: 7.011465, acc: 7.011465]\n",
      "516: [D loss: 0.000642, acc: 0.000642]  [G loss: 6.994225, acc: 6.994225]\n",
      "517: [D loss: 0.000127, acc: 0.000127]  [G loss: 7.154099, acc: 7.154099]\n",
      "518: [D loss: 0.001674, acc: 0.001674]  [G loss: 6.592911, acc: 6.592911]\n",
      "519: [D loss: 0.000614, acc: 0.000614]  [G loss: 6.968624, acc: 6.968624]\n",
      "520: [D loss: 0.014254, acc: 0.014254]  [G loss: 8.153944, acc: 8.153944]\n",
      "521: [D loss: 0.000130, acc: 0.000130]  [G loss: 8.926016, acc: 8.926016]\n",
      "522: [D loss: 0.000208, acc: 0.000208]  [G loss: 9.527512, acc: 9.527512]\n",
      "523: [D loss: 0.003499, acc: 0.003499]  [G loss: 10.591021, acc: 10.591021]\n",
      "524: [D loss: 0.001173, acc: 0.001173]  [G loss: 10.443775, acc: 10.443775]\n",
      "525: [D loss: 0.013837, acc: 0.013837]  [G loss: 10.112417, acc: 10.112417]\n",
      "526: [D loss: 0.001861, acc: 0.001861]  [G loss: 9.525974, acc: 9.525974]\n",
      "527: [D loss: 0.000092, acc: 0.000092]  [G loss: 9.482890, acc: 9.482890]\n",
      "528: [D loss: 0.000083, acc: 0.000083]  [G loss: 9.139017, acc: 9.139017]\n",
      "529: [D loss: 0.000250, acc: 0.000250]  [G loss: 8.490057, acc: 8.490057]\n",
      "530: [D loss: 0.000112, acc: 0.000112]  [G loss: 8.102310, acc: 8.102310]\n",
      "531: [D loss: 0.000145, acc: 0.000145]  [G loss: 7.775594, acc: 7.775594]\n",
      "532: [D loss: 0.001591, acc: 0.001591]  [G loss: 7.520794, acc: 7.520794]\n",
      "533: [D loss: 0.000629, acc: 0.000629]  [G loss: 7.595699, acc: 7.595699]\n",
      "534: [D loss: 0.000294, acc: 0.000294]  [G loss: 7.798763, acc: 7.798763]\n",
      "535: [D loss: 0.000167, acc: 0.000167]  [G loss: 8.193474, acc: 8.193474]\n",
      "536: [D loss: 0.001229, acc: 0.001229]  [G loss: 7.142162, acc: 7.142162]\n",
      "537: [D loss: 0.009528, acc: 0.009528]  [G loss: 8.253769, acc: 8.253769]\n",
      "538: [D loss: 0.000476, acc: 0.000476]  [G loss: 9.222329, acc: 9.222329]\n",
      "539: [D loss: 0.000407, acc: 0.000407]  [G loss: 9.976305, acc: 9.976305]\n",
      "540: [D loss: 0.000115, acc: 0.000115]  [G loss: 10.457817, acc: 10.457817]\n",
      "541: [D loss: 0.002669, acc: 0.002669]  [G loss: 10.846663, acc: 10.846663]\n",
      "542: [D loss: 0.000806, acc: 0.000806]  [G loss: 11.075659, acc: 11.075659]\n",
      "543: [D loss: 0.000893, acc: 0.000893]  [G loss: 11.342051, acc: 11.342051]\n",
      "544: [D loss: 0.000705, acc: 0.000705]  [G loss: 11.329221, acc: 11.329221]\n",
      "545: [D loss: 0.000304, acc: 0.000304]  [G loss: 11.464585, acc: 11.464585]\n",
      "546: [D loss: 0.000339, acc: 0.000339]  [G loss: 11.655184, acc: 11.655184]\n",
      "547: [D loss: 0.000655, acc: 0.000655]  [G loss: 11.044715, acc: 11.044715]\n",
      "548: [D loss: 0.001440, acc: 0.001440]  [G loss: 10.838078, acc: 10.838078]\n",
      "549: [D loss: 0.003556, acc: 0.003556]  [G loss: 10.663688, acc: 10.663688]\n",
      "550: [D loss: 0.000464, acc: 0.000464]  [G loss: 10.525971, acc: 10.525971]\n",
      "551: [D loss: 0.004129, acc: 0.004129]  [G loss: 9.456517, acc: 9.456517]\n",
      "552: [D loss: 0.002508, acc: 0.002508]  [G loss: 10.154894, acc: 10.154894]\n",
      "553: [D loss: 0.000427, acc: 0.000427]  [G loss: 10.801799, acc: 10.801799]\n",
      "554: [D loss: 0.000944, acc: 0.000944]  [G loss: 10.547756, acc: 10.547756]\n",
      "555: [D loss: 0.000547, acc: 0.000547]  [G loss: 10.590597, acc: 10.590597]\n",
      "556: [D loss: 0.001546, acc: 0.001546]  [G loss: 10.720202, acc: 10.720202]\n",
      "557: [D loss: 0.009514, acc: 0.009514]  [G loss: 9.896684, acc: 9.896684]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "558: [D loss: 0.007835, acc: 0.007835]  [G loss: 10.676832, acc: 10.676832]\n",
      "559: [D loss: 0.002043, acc: 0.002043]  [G loss: 10.621893, acc: 10.621893]\n",
      "560: [D loss: 0.000915, acc: 0.000915]  [G loss: 11.365130, acc: 11.365130]\n",
      "561: [D loss: 0.001436, acc: 0.001436]  [G loss: 10.755640, acc: 10.755640]\n",
      "562: [D loss: 0.029954, acc: 0.029954]  [G loss: 11.943605, acc: 11.943605]\n",
      "563: [D loss: 0.002256, acc: 0.002256]  [G loss: 12.483030, acc: 12.483030]\n",
      "564: [D loss: 0.005596, acc: 0.005596]  [G loss: 12.013044, acc: 12.013044]\n",
      "565: [D loss: 0.003511, acc: 0.003511]  [G loss: 11.921928, acc: 11.921928]\n",
      "566: [D loss: 0.001085, acc: 0.001085]  [G loss: 11.326750, acc: 11.326750]\n",
      "567: [D loss: 0.025613, acc: 0.025613]  [G loss: 11.437885, acc: 11.437885]\n",
      "568: [D loss: 0.002352, acc: 0.002352]  [G loss: 12.007099, acc: 12.007099]\n",
      "569: [D loss: 0.040571, acc: 0.040571]  [G loss: 10.066347, acc: 10.066347]\n",
      "570: [D loss: 0.006997, acc: 0.006997]  [G loss: 10.152472, acc: 10.152472]\n",
      "571: [D loss: 0.032672, acc: 0.032672]  [G loss: 12.749464, acc: 12.749464]\n",
      "572: [D loss: 0.002406, acc: 0.002406]  [G loss: 13.550397, acc: 13.550397]\n",
      "573: [D loss: 0.015121, acc: 0.015121]  [G loss: 12.574823, acc: 12.574823]\n",
      "574: [D loss: 0.013708, acc: 0.013708]  [G loss: 11.237853, acc: 11.237853]\n",
      "575: [D loss: 0.023432, acc: 0.023432]  [G loss: 9.850951, acc: 9.850951]\n",
      "576: [D loss: 0.072080, acc: 0.072080]  [G loss: 11.053406, acc: 11.053406]\n",
      "577: [D loss: 0.002473, acc: 0.002473]  [G loss: 12.654160, acc: 12.654160]\n",
      "578: [D loss: 0.011993, acc: 0.011993]  [G loss: 12.816133, acc: 12.816133]\n",
      "579: [D loss: 0.005232, acc: 0.005232]  [G loss: 12.916718, acc: 12.916718]\n",
      "580: [D loss: 0.000597, acc: 0.000597]  [G loss: 11.561945, acc: 11.561945]\n",
      "581: [D loss: 0.006874, acc: 0.006874]  [G loss: 11.606800, acc: 11.606800]\n",
      "582: [D loss: 0.003752, acc: 0.003752]  [G loss: 11.771918, acc: 11.771918]\n",
      "583: [D loss: 0.011115, acc: 0.011115]  [G loss: 12.348623, acc: 12.348623]\n",
      "584: [D loss: 0.018163, acc: 0.018163]  [G loss: 11.665484, acc: 11.665484]\n",
      "585: [D loss: 0.009396, acc: 0.009396]  [G loss: 11.431213, acc: 11.431213]\n",
      "586: [D loss: 0.008620, acc: 0.008620]  [G loss: 11.605641, acc: 11.605641]\n",
      "587: [D loss: 0.008450, acc: 0.008450]  [G loss: 12.165187, acc: 12.165187]\n",
      "588: [D loss: 0.001449, acc: 0.001449]  [G loss: 12.963917, acc: 12.963917]\n",
      "589: [D loss: 0.014122, acc: 0.014122]  [G loss: 12.209458, acc: 12.209458]\n",
      "590: [D loss: 0.006095, acc: 0.006095]  [G loss: 11.445241, acc: 11.445241]\n",
      "591: [D loss: 0.002077, acc: 0.002077]  [G loss: 11.635605, acc: 11.635605]\n",
      "592: [D loss: 0.013225, acc: 0.013225]  [G loss: 11.450484, acc: 11.450484]\n",
      "593: [D loss: 0.013385, acc: 0.013385]  [G loss: 12.786970, acc: 12.786970]\n",
      "594: [D loss: 0.016136, acc: 0.016136]  [G loss: 13.925693, acc: 13.925693]\n",
      "595: [D loss: 0.145118, acc: 0.145118]  [G loss: 7.559365, acc: 7.559365]\n",
      "596: [D loss: 0.607869, acc: 0.607869]  [G loss: 11.036783, acc: 11.036783]\n",
      "597: [D loss: 0.041113, acc: 0.041113]  [G loss: 14.055790, acc: 14.055790]\n",
      "598: [D loss: 0.114507, acc: 0.114507]  [G loss: 14.107206, acc: 14.107206]\n",
      "599: [D loss: 0.090044, acc: 0.090044]  [G loss: 11.295616, acc: 11.295616]\n",
      "600: [D loss: 0.032457, acc: 0.032457]  [G loss: 10.047971, acc: 10.047971]\n",
      "601: [D loss: 0.078969, acc: 0.078969]  [G loss: 9.475033, acc: 9.475033]\n",
      "602: [D loss: 0.159200, acc: 0.159200]  [G loss: 11.058285, acc: 11.058285]\n",
      "603: [D loss: 0.034144, acc: 0.034144]  [G loss: 11.367405, acc: 11.367405]\n",
      "604: [D loss: 0.012500, acc: 0.012500]  [G loss: 12.894995, acc: 12.894995]\n",
      "605: [D loss: 0.027541, acc: 0.027541]  [G loss: 13.760529, acc: 13.760529]\n",
      "606: [D loss: 0.183709, acc: 0.183709]  [G loss: 11.734999, acc: 11.734999]\n",
      "607: [D loss: 0.013542, acc: 0.013542]  [G loss: 10.877169, acc: 10.877169]\n",
      "608: [D loss: 0.027497, acc: 0.027497]  [G loss: 9.377409, acc: 9.377409]\n",
      "609: [D loss: 0.041822, acc: 0.041822]  [G loss: 9.343788, acc: 9.343788]\n",
      "610: [D loss: 0.104535, acc: 0.104535]  [G loss: 9.458094, acc: 9.458094]\n",
      "611: [D loss: 0.081165, acc: 0.081165]  [G loss: 9.977122, acc: 9.977122]\n",
      "612: [D loss: 0.029250, acc: 0.029250]  [G loss: 11.331327, acc: 11.331327]\n",
      "613: [D loss: 0.031387, acc: 0.031387]  [G loss: 12.513283, acc: 12.513283]\n",
      "614: [D loss: 0.052578, acc: 0.052578]  [G loss: 12.650633, acc: 12.650633]\n",
      "615: [D loss: 0.058998, acc: 0.058998]  [G loss: 11.954251, acc: 11.954251]\n",
      "616: [D loss: 0.046911, acc: 0.046911]  [G loss: 11.108288, acc: 11.108288]\n",
      "617: [D loss: 0.020980, acc: 0.020980]  [G loss: 10.107872, acc: 10.107872]\n",
      "618: [D loss: 0.036279, acc: 0.036279]  [G loss: 9.428524, acc: 9.428524]\n",
      "619: [D loss: 0.010709, acc: 0.010709]  [G loss: 9.470457, acc: 9.470457]\n",
      "620: [D loss: 0.069712, acc: 0.069712]  [G loss: 9.764852, acc: 9.764852]\n",
      "621: [D loss: 0.064488, acc: 0.064488]  [G loss: 10.354246, acc: 10.354246]\n",
      "622: [D loss: 0.020996, acc: 0.020996]  [G loss: 11.455421, acc: 11.455421]\n",
      "623: [D loss: 0.018955, acc: 0.018955]  [G loss: 11.637506, acc: 11.637506]\n",
      "624: [D loss: 0.036341, acc: 0.036341]  [G loss: 11.974263, acc: 11.974263]\n",
      "625: [D loss: 0.037394, acc: 0.037394]  [G loss: 12.270329, acc: 12.270329]\n",
      "626: [D loss: 0.059710, acc: 0.059710]  [G loss: 11.061396, acc: 11.061396]\n",
      "627: [D loss: 0.016279, acc: 0.016279]  [G loss: 10.971653, acc: 10.971653]\n",
      "628: [D loss: 0.016603, acc: 0.016603]  [G loss: 10.165449, acc: 10.165449]\n",
      "629: [D loss: 0.026677, acc: 0.026677]  [G loss: 9.615811, acc: 9.615811]\n",
      "630: [D loss: 0.013958, acc: 0.013958]  [G loss: 8.921529, acc: 8.921529]\n",
      "631: [D loss: 0.038211, acc: 0.038211]  [G loss: 9.025637, acc: 9.025637]\n",
      "632: [D loss: 0.050185, acc: 0.050185]  [G loss: 9.760412, acc: 9.760412]\n",
      "633: [D loss: 0.011489, acc: 0.011489]  [G loss: 10.457341, acc: 10.457341]\n",
      "634: [D loss: 0.013682, acc: 0.013682]  [G loss: 11.605963, acc: 11.605963]\n",
      "635: [D loss: 0.014517, acc: 0.014517]  [G loss: 11.765013, acc: 11.765013]\n",
      "636: [D loss: 0.056513, acc: 0.056513]  [G loss: 12.499789, acc: 12.499789]\n",
      "637: [D loss: 0.105452, acc: 0.105452]  [G loss: 11.514486, acc: 11.514486]\n",
      "638: [D loss: 0.036812, acc: 0.036812]  [G loss: 10.305496, acc: 10.305496]\n",
      "639: [D loss: 0.023836, acc: 0.023836]  [G loss: 9.049097, acc: 9.049097]\n",
      "640: [D loss: 0.070681, acc: 0.070681]  [G loss: 9.153597, acc: 9.153597]\n",
      "641: [D loss: 0.096839, acc: 0.096839]  [G loss: 9.957100, acc: 9.957100]\n",
      "642: [D loss: 0.048000, acc: 0.048000]  [G loss: 10.958412, acc: 10.958412]\n",
      "643: [D loss: 0.026671, acc: 0.026671]  [G loss: 11.760828, acc: 11.760828]\n",
      "644: [D loss: 0.025518, acc: 0.025518]  [G loss: 11.427048, acc: 11.427048]\n",
      "645: [D loss: 0.090159, acc: 0.090159]  [G loss: 11.536605, acc: 11.536605]\n",
      "646: [D loss: 0.018683, acc: 0.018683]  [G loss: 10.780148, acc: 10.780148]\n",
      "647: [D loss: 0.011098, acc: 0.011098]  [G loss: 9.450183, acc: 9.450183]\n",
      "648: [D loss: 0.056065, acc: 0.056065]  [G loss: 9.243162, acc: 9.243162]\n",
      "649: [D loss: 0.037974, acc: 0.037974]  [G loss: 9.037800, acc: 9.037800]\n",
      "650: [D loss: 0.028776, acc: 0.028776]  [G loss: 9.557702, acc: 9.557702]\n",
      "651: [D loss: 0.020520, acc: 0.020520]  [G loss: 10.072987, acc: 10.072987]\n",
      "652: [D loss: 0.016274, acc: 0.016274]  [G loss: 10.773626, acc: 10.773626]\n",
      "653: [D loss: 0.032532, acc: 0.032532]  [G loss: 11.291892, acc: 11.291892]\n",
      "654: [D loss: 0.009203, acc: 0.009203]  [G loss: 11.425968, acc: 11.425968]\n",
      "655: [D loss: 0.071498, acc: 0.071498]  [G loss: 11.162622, acc: 11.162622]\n",
      "656: [D loss: 0.034512, acc: 0.034512]  [G loss: 9.615451, acc: 9.615451]\n",
      "657: [D loss: 0.026837, acc: 0.026837]  [G loss: 8.845572, acc: 8.845572]\n",
      "658: [D loss: 0.010565, acc: 0.010565]  [G loss: 8.290390, acc: 8.290390]\n",
      "659: [D loss: 0.071059, acc: 0.071059]  [G loss: 8.917009, acc: 8.917009]\n",
      "660: [D loss: 0.045539, acc: 0.045539]  [G loss: 9.300875, acc: 9.300875]\n",
      "661: [D loss: 0.019585, acc: 0.019585]  [G loss: 11.040384, acc: 11.040384]\n",
      "662: [D loss: 0.046253, acc: 0.046253]  [G loss: 11.303810, acc: 11.303810]\n",
      "663: [D loss: 0.051060, acc: 0.051060]  [G loss: 11.428280, acc: 11.428280]\n",
      "664: [D loss: 0.034347, acc: 0.034347]  [G loss: 9.967014, acc: 9.967014]\n",
      "665: [D loss: 0.013806, acc: 0.013806]  [G loss: 9.348332, acc: 9.348332]\n",
      "666: [D loss: 0.074126, acc: 0.074126]  [G loss: 10.972439, acc: 10.972439]\n",
      "667: [D loss: 0.009558, acc: 0.009558]  [G loss: 10.829249, acc: 10.829249]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "668: [D loss: 0.070647, acc: 0.070647]  [G loss: 11.771067, acc: 11.771067]\n",
      "669: [D loss: 0.094185, acc: 0.094185]  [G loss: 10.797190, acc: 10.797190]\n",
      "670: [D loss: 0.043518, acc: 0.043518]  [G loss: 9.891109, acc: 9.891109]\n",
      "671: [D loss: 0.041727, acc: 0.041727]  [G loss: 9.628267, acc: 9.628267]\n",
      "672: [D loss: 0.103568, acc: 0.103568]  [G loss: 11.393190, acc: 11.393190]\n",
      "673: [D loss: 0.060748, acc: 0.060748]  [G loss: 12.685161, acc: 12.685161]\n",
      "674: [D loss: 0.227689, acc: 0.227689]  [G loss: 9.923945, acc: 9.923945]\n",
      "675: [D loss: 0.041712, acc: 0.041712]  [G loss: 8.143908, acc: 8.143908]\n",
      "676: [D loss: 0.222371, acc: 0.222371]  [G loss: 8.840015, acc: 8.840015]\n",
      "677: [D loss: 0.185826, acc: 0.185826]  [G loss: 10.873297, acc: 10.873297]\n",
      "678: [D loss: 0.073540, acc: 0.073540]  [G loss: 12.209654, acc: 12.209654]\n",
      "679: [D loss: 0.151505, acc: 0.151505]  [G loss: 11.181725, acc: 11.181725]\n",
      "680: [D loss: 0.129262, acc: 0.129262]  [G loss: 9.429043, acc: 9.429043]\n",
      "681: [D loss: 0.104313, acc: 0.104313]  [G loss: 8.817634, acc: 8.817634]\n",
      "682: [D loss: 0.058963, acc: 0.058963]  [G loss: 7.869522, acc: 7.869522]\n",
      "683: [D loss: 0.143228, acc: 0.143228]  [G loss: 9.438765, acc: 9.438765]\n",
      "684: [D loss: 0.073283, acc: 0.073283]  [G loss: 9.996691, acc: 9.996691]\n",
      "685: [D loss: 0.080482, acc: 0.080482]  [G loss: 10.839052, acc: 10.839052]\n",
      "686: [D loss: 0.083561, acc: 0.083561]  [G loss: 10.892314, acc: 10.892314]\n",
      "687: [D loss: 0.139890, acc: 0.139890]  [G loss: 9.835855, acc: 9.835855]\n",
      "688: [D loss: 0.074088, acc: 0.074088]  [G loss: 8.929923, acc: 8.929923]\n",
      "689: [D loss: 0.066603, acc: 0.066603]  [G loss: 8.293473, acc: 8.293473]\n",
      "690: [D loss: 0.123707, acc: 0.123707]  [G loss: 8.252255, acc: 8.252255]\n",
      "691: [D loss: 0.064874, acc: 0.064874]  [G loss: 9.428673, acc: 9.428673]\n",
      "692: [D loss: 0.104838, acc: 0.104838]  [G loss: 10.447561, acc: 10.447561]\n",
      "693: [D loss: 0.041275, acc: 0.041275]  [G loss: 10.736071, acc: 10.736071]\n",
      "694: [D loss: 0.103490, acc: 0.103490]  [G loss: 10.764042, acc: 10.764042]\n",
      "695: [D loss: 0.070090, acc: 0.070090]  [G loss: 9.760999, acc: 9.760999]\n",
      "696: [D loss: 0.074189, acc: 0.074189]  [G loss: 8.815656, acc: 8.815656]\n",
      "697: [D loss: 0.044869, acc: 0.044869]  [G loss: 7.705078, acc: 7.705078]\n",
      "698: [D loss: 0.065308, acc: 0.065308]  [G loss: 8.205661, acc: 8.205661]\n",
      "699: [D loss: 0.044567, acc: 0.044567]  [G loss: 9.301080, acc: 9.301080]\n",
      "700: [D loss: 0.017676, acc: 0.017676]  [G loss: 9.953350, acc: 9.953350]\n",
      "701: [D loss: 0.032630, acc: 0.032630]  [G loss: 10.274594, acc: 10.274594]\n",
      "702: [D loss: 0.024100, acc: 0.024100]  [G loss: 10.391961, acc: 10.391961]\n",
      "703: [D loss: 0.044148, acc: 0.044148]  [G loss: 9.729690, acc: 9.729690]\n",
      "704: [D loss: 0.024096, acc: 0.024096]  [G loss: 9.215851, acc: 9.215851]\n",
      "705: [D loss: 0.016905, acc: 0.016905]  [G loss: 8.800423, acc: 8.800423]\n",
      "706: [D loss: 0.029972, acc: 0.029972]  [G loss: 8.757180, acc: 8.757180]\n",
      "707: [D loss: 0.039972, acc: 0.039972]  [G loss: 10.003545, acc: 10.003545]\n",
      "708: [D loss: 0.008236, acc: 0.008236]  [G loss: 10.151377, acc: 10.151377]\n",
      "709: [D loss: 0.028697, acc: 0.028697]  [G loss: 10.287388, acc: 10.287388]\n",
      "710: [D loss: 0.041218, acc: 0.041218]  [G loss: 10.022610, acc: 10.022610]\n",
      "711: [D loss: 0.030135, acc: 0.030135]  [G loss: 8.977334, acc: 8.977334]\n",
      "712: [D loss: 0.016783, acc: 0.016783]  [G loss: 8.517706, acc: 8.517706]\n",
      "713: [D loss: 0.010751, acc: 0.010751]  [G loss: 8.982262, acc: 8.982262]\n",
      "714: [D loss: 0.051508, acc: 0.051508]  [G loss: 9.525570, acc: 9.525570]\n",
      "715: [D loss: 0.017826, acc: 0.017826]  [G loss: 10.021966, acc: 10.021966]\n",
      "716: [D loss: 0.015745, acc: 0.015745]  [G loss: 11.221656, acc: 11.221656]\n",
      "717: [D loss: 0.030089, acc: 0.030089]  [G loss: 11.019493, acc: 11.019493]\n",
      "718: [D loss: 0.019584, acc: 0.019584]  [G loss: 10.201716, acc: 10.201716]\n",
      "719: [D loss: 0.012357, acc: 0.012357]  [G loss: 9.261473, acc: 9.261473]\n",
      "720: [D loss: 0.017609, acc: 0.017609]  [G loss: 8.850750, acc: 8.850750]\n",
      "721: [D loss: 0.017726, acc: 0.017726]  [G loss: 9.345514, acc: 9.345514]\n",
      "722: [D loss: 0.035347, acc: 0.035347]  [G loss: 9.886852, acc: 9.886852]\n",
      "723: [D loss: 0.027367, acc: 0.027367]  [G loss: 10.758449, acc: 10.758449]\n",
      "724: [D loss: 0.021200, acc: 0.021200]  [G loss: 11.507277, acc: 11.507277]\n",
      "725: [D loss: 0.035952, acc: 0.035952]  [G loss: 10.752422, acc: 10.752422]\n",
      "726: [D loss: 0.017840, acc: 0.017840]  [G loss: 9.416475, acc: 9.416475]\n",
      "727: [D loss: 0.059939, acc: 0.059939]  [G loss: 9.309885, acc: 9.309885]\n",
      "728: [D loss: 0.032493, acc: 0.032493]  [G loss: 10.078238, acc: 10.078238]\n",
      "729: [D loss: 0.003693, acc: 0.003693]  [G loss: 10.671482, acc: 10.671482]\n",
      "730: [D loss: 0.033647, acc: 0.033647]  [G loss: 10.404609, acc: 10.404609]\n",
      "731: [D loss: 0.023709, acc: 0.023709]  [G loss: 9.530855, acc: 9.530855]\n",
      "732: [D loss: 0.012140, acc: 0.012140]  [G loss: 9.371564, acc: 9.371564]\n",
      "733: [D loss: 0.016321, acc: 0.016321]  [G loss: 9.869066, acc: 9.869066]\n",
      "734: [D loss: 0.039941, acc: 0.039941]  [G loss: 11.196750, acc: 11.196750]\n",
      "735: [D loss: 0.027736, acc: 0.027736]  [G loss: 11.612951, acc: 11.612951]\n",
      "736: [D loss: 0.058922, acc: 0.058922]  [G loss: 10.068741, acc: 10.068741]\n",
      "737: [D loss: 0.035586, acc: 0.035586]  [G loss: 9.286141, acc: 9.286141]\n",
      "738: [D loss: 0.037915, acc: 0.037915]  [G loss: 9.568859, acc: 9.568859]\n",
      "739: [D loss: 0.041125, acc: 0.041125]  [G loss: 10.169352, acc: 10.169352]\n",
      "740: [D loss: 0.034103, acc: 0.034103]  [G loss: 10.552987, acc: 10.552987]\n",
      "741: [D loss: 0.027135, acc: 0.027135]  [G loss: 10.009258, acc: 10.009258]\n",
      "742: [D loss: 0.021554, acc: 0.021554]  [G loss: 9.739957, acc: 9.739957]\n",
      "743: [D loss: 0.026180, acc: 0.026180]  [G loss: 9.905658, acc: 9.905658]\n",
      "744: [D loss: 0.022615, acc: 0.022615]  [G loss: 10.032731, acc: 10.032731]\n",
      "745: [D loss: 0.072055, acc: 0.072055]  [G loss: 9.752302, acc: 9.752302]\n",
      "746: [D loss: 0.061844, acc: 0.061844]  [G loss: 8.573987, acc: 8.573987]\n",
      "747: [D loss: 0.067795, acc: 0.067795]  [G loss: 10.151825, acc: 10.151825]\n",
      "748: [D loss: 0.028322, acc: 0.028322]  [G loss: 10.534431, acc: 10.534431]\n",
      "749: [D loss: 0.048597, acc: 0.048597]  [G loss: 9.314512, acc: 9.314512]\n",
      "750: [D loss: 0.042159, acc: 0.042159]  [G loss: 8.174835, acc: 8.174835]\n",
      "751: [D loss: 0.038322, acc: 0.038322]  [G loss: 9.013060, acc: 9.013060]\n",
      "752: [D loss: 0.106899, acc: 0.106899]  [G loss: 12.695446, acc: 12.695446]\n",
      "753: [D loss: 0.445617, acc: 0.445617]  [G loss: 6.529558, acc: 6.529558]\n",
      "754: [D loss: 0.561448, acc: 0.561448]  [G loss: 6.577918, acc: 6.577918]\n",
      "755: [D loss: 0.394100, acc: 0.394100]  [G loss: 8.771322, acc: 8.771322]\n",
      "756: [D loss: 0.032924, acc: 0.032924]  [G loss: 11.613599, acc: 11.613599]\n",
      "757: [D loss: 0.116518, acc: 0.116518]  [G loss: 11.038717, acc: 11.038717]\n",
      "758: [D loss: 0.091946, acc: 0.091946]  [G loss: 9.389679, acc: 9.389679]\n",
      "759: [D loss: 0.037535, acc: 0.037535]  [G loss: 8.187034, acc: 8.187034]\n",
      "760: [D loss: 0.067842, acc: 0.067842]  [G loss: 7.698330, acc: 7.698330]\n",
      "761: [D loss: 0.088043, acc: 0.088043]  [G loss: 7.904159, acc: 7.904159]\n",
      "762: [D loss: 0.067122, acc: 0.067122]  [G loss: 8.176174, acc: 8.176174]\n",
      "763: [D loss: 0.063304, acc: 0.063304]  [G loss: 8.896360, acc: 8.896360]\n",
      "764: [D loss: 0.024964, acc: 0.024964]  [G loss: 9.233219, acc: 9.233219]\n",
      "765: [D loss: 0.046300, acc: 0.046300]  [G loss: 9.112276, acc: 9.112276]\n",
      "766: [D loss: 0.057855, acc: 0.057855]  [G loss: 8.899432, acc: 8.899432]\n",
      "767: [D loss: 0.061189, acc: 0.061189]  [G loss: 8.651497, acc: 8.651497]\n",
      "768: [D loss: 0.024968, acc: 0.024968]  [G loss: 8.124732, acc: 8.124732]\n",
      "769: [D loss: 0.015866, acc: 0.015866]  [G loss: 7.362244, acc: 7.362244]\n",
      "770: [D loss: 0.031809, acc: 0.031809]  [G loss: 7.067731, acc: 7.067731]\n",
      "771: [D loss: 0.048093, acc: 0.048093]  [G loss: 7.193150, acc: 7.193150]\n",
      "772: [D loss: 0.053156, acc: 0.053156]  [G loss: 7.757642, acc: 7.757642]\n",
      "773: [D loss: 0.021741, acc: 0.021741]  [G loss: 8.248924, acc: 8.248924]\n",
      "774: [D loss: 0.021136, acc: 0.021136]  [G loss: 8.628355, acc: 8.628355]\n",
      "775: [D loss: 0.028250, acc: 0.028250]  [G loss: 9.072942, acc: 9.072942]\n",
      "776: [D loss: 0.018506, acc: 0.018506]  [G loss: 9.121328, acc: 9.121328]\n",
      "777: [D loss: 0.023957, acc: 0.023957]  [G loss: 8.762568, acc: 8.762568]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "778: [D loss: 0.015836, acc: 0.015836]  [G loss: 8.567268, acc: 8.567268]\n",
      "779: [D loss: 0.023283, acc: 0.023283]  [G loss: 8.405380, acc: 8.405380]\n",
      "780: [D loss: 0.019974, acc: 0.019974]  [G loss: 7.937103, acc: 7.937103]\n",
      "781: [D loss: 0.006084, acc: 0.006084]  [G loss: 7.842532, acc: 7.842532]\n",
      "782: [D loss: 0.026264, acc: 0.026264]  [G loss: 8.037724, acc: 8.037724]\n",
      "783: [D loss: 0.017104, acc: 0.017104]  [G loss: 7.864814, acc: 7.864814]\n",
      "784: [D loss: 0.007881, acc: 0.007881]  [G loss: 7.814484, acc: 7.814484]\n",
      "785: [D loss: 0.010529, acc: 0.010529]  [G loss: 8.591195, acc: 8.591195]\n",
      "786: [D loss: 0.014532, acc: 0.014532]  [G loss: 8.728646, acc: 8.728646]\n",
      "787: [D loss: 0.020338, acc: 0.020338]  [G loss: 8.947939, acc: 8.947939]\n",
      "788: [D loss: 0.006215, acc: 0.006215]  [G loss: 8.916391, acc: 8.916391]\n",
      "789: [D loss: 0.011208, acc: 0.011208]  [G loss: 8.944185, acc: 8.944185]\n",
      "790: [D loss: 0.016791, acc: 0.016791]  [G loss: 9.003572, acc: 9.003572]\n",
      "791: [D loss: 0.022524, acc: 0.022524]  [G loss: 9.446163, acc: 9.446163]\n",
      "792: [D loss: 0.005749, acc: 0.005749]  [G loss: 8.878363, acc: 8.878363]\n",
      "793: [D loss: 0.009487, acc: 0.009487]  [G loss: 9.346722, acc: 9.346722]\n",
      "794: [D loss: 0.004726, acc: 0.004726]  [G loss: 9.055199, acc: 9.055199]\n",
      "795: [D loss: 0.021735, acc: 0.021735]  [G loss: 9.454870, acc: 9.454870]\n",
      "796: [D loss: 0.014064, acc: 0.014064]  [G loss: 9.579537, acc: 9.579537]\n",
      "797: [D loss: 0.016301, acc: 0.016301]  [G loss: 9.825642, acc: 9.825642]\n",
      "798: [D loss: 0.014672, acc: 0.014672]  [G loss: 10.198485, acc: 10.198485]\n",
      "799: [D loss: 0.018614, acc: 0.018614]  [G loss: 9.347486, acc: 9.347486]\n",
      "800: [D loss: 0.029433, acc: 0.029433]  [G loss: 10.255228, acc: 10.255228]\n",
      "801: [D loss: 0.015643, acc: 0.015643]  [G loss: 10.377151, acc: 10.377151]\n",
      "802: [D loss: 0.017889, acc: 0.017889]  [G loss: 10.522557, acc: 10.522557]\n",
      "803: [D loss: 0.010355, acc: 0.010355]  [G loss: 10.746491, acc: 10.746491]\n",
      "804: [D loss: 0.019303, acc: 0.019303]  [G loss: 9.940697, acc: 9.940697]\n",
      "805: [D loss: 0.044051, acc: 0.044051]  [G loss: 9.396273, acc: 9.396273]\n",
      "806: [D loss: 0.046595, acc: 0.046595]  [G loss: 9.192835, acc: 9.192835]\n",
      "807: [D loss: 0.047842, acc: 0.047842]  [G loss: 10.464275, acc: 10.464275]\n",
      "808: [D loss: 0.051463, acc: 0.051463]  [G loss: 11.109430, acc: 11.109430]\n",
      "809: [D loss: 0.097609, acc: 0.097609]  [G loss: 10.802162, acc: 10.802162]\n",
      "810: [D loss: 0.042580, acc: 0.042580]  [G loss: 10.040020, acc: 10.040020]\n",
      "811: [D loss: 0.025505, acc: 0.025505]  [G loss: 8.942553, acc: 8.942553]\n",
      "812: [D loss: 0.036731, acc: 0.036731]  [G loss: 9.084230, acc: 9.084230]\n",
      "813: [D loss: 0.105961, acc: 0.105961]  [G loss: 10.545921, acc: 10.545921]\n",
      "814: [D loss: 0.026441, acc: 0.026441]  [G loss: 11.804409, acc: 11.804409]\n",
      "815: [D loss: 0.048172, acc: 0.048172]  [G loss: 11.098502, acc: 11.098502]\n",
      "816: [D loss: 0.072123, acc: 0.072123]  [G loss: 9.645894, acc: 9.645894]\n",
      "817: [D loss: 0.057357, acc: 0.057357]  [G loss: 9.055529, acc: 9.055529]\n",
      "818: [D loss: 0.036890, acc: 0.036890]  [G loss: 8.689930, acc: 8.689930]\n",
      "819: [D loss: 0.113722, acc: 0.113722]  [G loss: 10.419713, acc: 10.419713]\n",
      "820: [D loss: 0.013352, acc: 0.013352]  [G loss: 10.929457, acc: 10.929457]\n",
      "821: [D loss: 0.079118, acc: 0.079118]  [G loss: 9.774988, acc: 9.774988]\n",
      "822: [D loss: 0.025834, acc: 0.025834]  [G loss: 9.397688, acc: 9.397688]\n",
      "823: [D loss: 0.041054, acc: 0.041054]  [G loss: 8.842855, acc: 8.842855]\n",
      "824: [D loss: 0.033211, acc: 0.033211]  [G loss: 9.231848, acc: 9.231848]\n",
      "825: [D loss: 0.016853, acc: 0.016853]  [G loss: 9.705195, acc: 9.705195]\n",
      "826: [D loss: 0.014329, acc: 0.014329]  [G loss: 9.688263, acc: 9.688263]\n",
      "827: [D loss: 0.028288, acc: 0.028288]  [G loss: 9.775356, acc: 9.775356]\n",
      "828: [D loss: 0.033277, acc: 0.033277]  [G loss: 9.416055, acc: 9.416055]\n",
      "829: [D loss: 0.008119, acc: 0.008119]  [G loss: 9.133560, acc: 9.133560]\n",
      "830: [D loss: 0.022862, acc: 0.022862]  [G loss: 9.242129, acc: 9.242129]\n",
      "831: [D loss: 0.045426, acc: 0.045426]  [G loss: 9.668418, acc: 9.668418]\n",
      "832: [D loss: 0.006786, acc: 0.006786]  [G loss: 10.524620, acc: 10.524620]\n",
      "833: [D loss: 0.056498, acc: 0.056498]  [G loss: 9.949598, acc: 9.949598]\n",
      "834: [D loss: 0.022673, acc: 0.022673]  [G loss: 9.142801, acc: 9.142801]\n",
      "835: [D loss: 0.017681, acc: 0.017681]  [G loss: 8.875851, acc: 8.875851]\n",
      "836: [D loss: 0.041063, acc: 0.041063]  [G loss: 8.984667, acc: 8.984667]\n",
      "837: [D loss: 0.062390, acc: 0.062390]  [G loss: 10.009926, acc: 10.009926]\n",
      "838: [D loss: 0.014403, acc: 0.014403]  [G loss: 10.461802, acc: 10.461802]\n",
      "839: [D loss: 0.070403, acc: 0.070403]  [G loss: 9.906982, acc: 9.906982]\n",
      "840: [D loss: 0.023014, acc: 0.023014]  [G loss: 8.788546, acc: 8.788546]\n",
      "841: [D loss: 0.062002, acc: 0.062002]  [G loss: 9.424765, acc: 9.424765]\n",
      "842: [D loss: 0.019276, acc: 0.019276]  [G loss: 9.618793, acc: 9.618793]\n",
      "843: [D loss: 0.020385, acc: 0.020385]  [G loss: 10.436131, acc: 10.436131]\n",
      "844: [D loss: 0.063738, acc: 0.063738]  [G loss: 9.679551, acc: 9.679551]\n",
      "845: [D loss: 0.033008, acc: 0.033008]  [G loss: 7.736924, acc: 7.736924]\n",
      "846: [D loss: 0.041153, acc: 0.041153]  [G loss: 7.618617, acc: 7.618617]\n",
      "847: [D loss: 0.068889, acc: 0.068889]  [G loss: 9.002591, acc: 9.002591]\n",
      "848: [D loss: 0.032698, acc: 0.032698]  [G loss: 10.648062, acc: 10.648062]\n",
      "849: [D loss: 0.054457, acc: 0.054457]  [G loss: 11.050485, acc: 11.050485]\n",
      "850: [D loss: 0.036685, acc: 0.036685]  [G loss: 9.397049, acc: 9.397049]\n",
      "851: [D loss: 0.013170, acc: 0.013170]  [G loss: 8.630369, acc: 8.630369]\n",
      "852: [D loss: 0.032036, acc: 0.032036]  [G loss: 8.284065, acc: 8.284065]\n",
      "853: [D loss: 0.072225, acc: 0.072225]  [G loss: 10.060291, acc: 10.060291]\n",
      "854: [D loss: 0.049484, acc: 0.049484]  [G loss: 11.571100, acc: 11.571100]\n",
      "855: [D loss: 0.021286, acc: 0.021286]  [G loss: 11.210684, acc: 11.210684]\n",
      "856: [D loss: 0.057342, acc: 0.057342]  [G loss: 10.570703, acc: 10.570703]\n",
      "857: [D loss: 0.020496, acc: 0.020496]  [G loss: 9.394220, acc: 9.394220]\n",
      "858: [D loss: 0.039396, acc: 0.039396]  [G loss: 8.848070, acc: 8.848070]\n",
      "859: [D loss: 0.034741, acc: 0.034741]  [G loss: 9.896832, acc: 9.896832]\n",
      "860: [D loss: 0.025167, acc: 0.025167]  [G loss: 10.609590, acc: 10.609590]\n",
      "861: [D loss: 0.027714, acc: 0.027714]  [G loss: 10.131459, acc: 10.131459]\n",
      "862: [D loss: 0.016184, acc: 0.016184]  [G loss: 9.685850, acc: 9.685850]\n",
      "863: [D loss: 0.033512, acc: 0.033512]  [G loss: 9.440647, acc: 9.440647]\n",
      "864: [D loss: 0.019655, acc: 0.019655]  [G loss: 9.494406, acc: 9.494406]\n",
      "865: [D loss: 0.020285, acc: 0.020285]  [G loss: 10.028242, acc: 10.028242]\n",
      "866: [D loss: 0.014595, acc: 0.014595]  [G loss: 10.597925, acc: 10.597925]\n",
      "867: [D loss: 0.050101, acc: 0.050101]  [G loss: 9.749298, acc: 9.749298]\n",
      "868: [D loss: 0.015925, acc: 0.015925]  [G loss: 9.483087, acc: 9.483087]\n",
      "869: [D loss: 0.053794, acc: 0.053794]  [G loss: 9.361378, acc: 9.361378]\n",
      "870: [D loss: 0.036332, acc: 0.036332]  [G loss: 11.083469, acc: 11.083469]\n",
      "871: [D loss: 0.077789, acc: 0.077789]  [G loss: 10.906418, acc: 10.906418]\n",
      "872: [D loss: 0.016356, acc: 0.016356]  [G loss: 9.746434, acc: 9.746434]\n",
      "873: [D loss: 0.025929, acc: 0.025929]  [G loss: 8.479536, acc: 8.479536]\n",
      "874: [D loss: 0.087959, acc: 0.087959]  [G loss: 10.026539, acc: 10.026539]\n",
      "875: [D loss: 0.022161, acc: 0.022161]  [G loss: 10.766253, acc: 10.766253]\n",
      "876: [D loss: 0.014472, acc: 0.014472]  [G loss: 11.501163, acc: 11.501163]\n",
      "877: [D loss: 0.039908, acc: 0.039908]  [G loss: 10.121376, acc: 10.121376]\n",
      "878: [D loss: 0.008933, acc: 0.008933]  [G loss: 9.048439, acc: 9.048439]\n",
      "879: [D loss: 0.041441, acc: 0.041441]  [G loss: 8.971531, acc: 8.971531]\n",
      "880: [D loss: 0.016648, acc: 0.016648]  [G loss: 9.507711, acc: 9.507711]\n",
      "881: [D loss: 0.015414, acc: 0.015414]  [G loss: 9.812695, acc: 9.812695]\n",
      "882: [D loss: 0.032567, acc: 0.032567]  [G loss: 10.946877, acc: 10.946877]\n",
      "883: [D loss: 0.038784, acc: 0.038784]  [G loss: 10.424877, acc: 10.424877]\n",
      "884: [D loss: 0.026194, acc: 0.026194]  [G loss: 10.131584, acc: 10.131584]\n",
      "885: [D loss: 0.022585, acc: 0.022585]  [G loss: 9.711869, acc: 9.711869]\n",
      "886: [D loss: 0.020336, acc: 0.020336]  [G loss: 10.128776, acc: 10.128776]\n",
      "887: [D loss: 0.039563, acc: 0.039563]  [G loss: 11.113717, acc: 11.113717]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "888: [D loss: 0.022539, acc: 0.022539]  [G loss: 11.924234, acc: 11.924234]\n",
      "889: [D loss: 0.016115, acc: 0.016115]  [G loss: 11.326211, acc: 11.326211]\n",
      "890: [D loss: 0.080184, acc: 0.080184]  [G loss: 9.059551, acc: 9.059551]\n",
      "891: [D loss: 0.015141, acc: 0.015141]  [G loss: 7.369106, acc: 7.369106]\n",
      "892: [D loss: 0.199523, acc: 0.199523]  [G loss: 9.888051, acc: 9.888051]\n",
      "893: [D loss: 0.031114, acc: 0.031114]  [G loss: 13.170509, acc: 13.170509]\n",
      "894: [D loss: 0.297684, acc: 0.297684]  [G loss: 8.373592, acc: 8.373592]\n",
      "895: [D loss: 0.202986, acc: 0.202986]  [G loss: 7.247910, acc: 7.247910]\n",
      "896: [D loss: 0.195105, acc: 0.195105]  [G loss: 8.800092, acc: 8.800092]\n",
      "897: [D loss: 0.079532, acc: 0.079532]  [G loss: 11.857731, acc: 11.857731]\n",
      "898: [D loss: 0.170471, acc: 0.170471]  [G loss: 11.224592, acc: 11.224592]\n",
      "899: [D loss: 0.124409, acc: 0.124409]  [G loss: 8.669161, acc: 8.669161]\n",
      "900: [D loss: 0.037764, acc: 0.037764]  [G loss: 6.986069, acc: 6.986069]\n",
      "901: [D loss: 0.107595, acc: 0.107595]  [G loss: 6.975994, acc: 6.975994]\n",
      "902: [D loss: 0.126901, acc: 0.126901]  [G loss: 8.059772, acc: 8.059772]\n",
      "903: [D loss: 0.028813, acc: 0.028813]  [G loss: 8.965947, acc: 8.965947]\n",
      "904: [D loss: 0.015324, acc: 0.015324]  [G loss: 10.473490, acc: 10.473490]\n",
      "905: [D loss: 0.094089, acc: 0.094089]  [G loss: 10.347385, acc: 10.347385]\n",
      "906: [D loss: 0.069009, acc: 0.069009]  [G loss: 8.730972, acc: 8.730972]\n",
      "907: [D loss: 0.045620, acc: 0.045620]  [G loss: 7.468167, acc: 7.468167]\n",
      "908: [D loss: 0.059128, acc: 0.059128]  [G loss: 6.671618, acc: 6.671618]\n",
      "909: [D loss: 0.228609, acc: 0.228609]  [G loss: 7.817500, acc: 7.817500]\n",
      "910: [D loss: 0.077213, acc: 0.077213]  [G loss: 9.208310, acc: 9.208310]\n",
      "911: [D loss: 0.040813, acc: 0.040813]  [G loss: 10.183129, acc: 10.183129]\n",
      "912: [D loss: 0.097905, acc: 0.097905]  [G loss: 9.890174, acc: 9.890174]\n",
      "913: [D loss: 0.160836, acc: 0.160836]  [G loss: 8.428983, acc: 8.428983]\n",
      "914: [D loss: 0.041475, acc: 0.041475]  [G loss: 7.263206, acc: 7.263206]\n",
      "915: [D loss: 0.064614, acc: 0.064614]  [G loss: 6.671386, acc: 6.671386]\n",
      "916: [D loss: 0.056788, acc: 0.056788]  [G loss: 6.671310, acc: 6.671310]\n",
      "917: [D loss: 0.138759, acc: 0.138759]  [G loss: 7.825107, acc: 7.825107]\n",
      "918: [D loss: 0.071938, acc: 0.071938]  [G loss: 8.987520, acc: 8.987520]\n",
      "919: [D loss: 0.020763, acc: 0.020763]  [G loss: 9.597443, acc: 9.597443]\n",
      "920: [D loss: 0.103985, acc: 0.103985]  [G loss: 9.893423, acc: 9.893423]\n",
      "921: [D loss: 0.174571, acc: 0.174571]  [G loss: 8.638117, acc: 8.638117]\n",
      "922: [D loss: 0.028135, acc: 0.028135]  [G loss: 7.690196, acc: 7.690196]\n",
      "923: [D loss: 0.057885, acc: 0.057885]  [G loss: 6.835505, acc: 6.835505]\n",
      "924: [D loss: 0.101694, acc: 0.101694]  [G loss: 7.015817, acc: 7.015817]\n",
      "925: [D loss: 0.092982, acc: 0.092982]  [G loss: 7.423050, acc: 7.423050]\n",
      "926: [D loss: 0.051294, acc: 0.051294]  [G loss: 8.265808, acc: 8.265808]\n",
      "927: [D loss: 0.057045, acc: 0.057045]  [G loss: 9.063636, acc: 9.063636]\n",
      "928: [D loss: 0.057293, acc: 0.057293]  [G loss: 9.267441, acc: 9.267441]\n",
      "929: [D loss: 0.057839, acc: 0.057839]  [G loss: 8.971476, acc: 8.971476]\n",
      "930: [D loss: 0.031604, acc: 0.031604]  [G loss: 8.813822, acc: 8.813822]\n",
      "931: [D loss: 0.027015, acc: 0.027015]  [G loss: 8.409232, acc: 8.409232]\n",
      "932: [D loss: 0.026041, acc: 0.026041]  [G loss: 8.577478, acc: 8.577478]\n",
      "933: [D loss: 0.029954, acc: 0.029954]  [G loss: 8.463800, acc: 8.463800]\n",
      "934: [D loss: 0.044989, acc: 0.044989]  [G loss: 8.317298, acc: 8.317298]\n",
      "935: [D loss: 0.050046, acc: 0.050046]  [G loss: 8.760104, acc: 8.760104]\n",
      "936: [D loss: 0.031759, acc: 0.031759]  [G loss: 9.186387, acc: 9.186387]\n",
      "937: [D loss: 0.035158, acc: 0.035158]  [G loss: 9.724239, acc: 9.724239]\n",
      "938: [D loss: 0.058387, acc: 0.058387]  [G loss: 9.554287, acc: 9.554287]\n",
      "939: [D loss: 0.054711, acc: 0.054711]  [G loss: 9.625668, acc: 9.625668]\n",
      "940: [D loss: 0.029962, acc: 0.029962]  [G loss: 9.308638, acc: 9.308638]\n",
      "941: [D loss: 0.052648, acc: 0.052648]  [G loss: 9.332994, acc: 9.332994]\n",
      "942: [D loss: 0.036997, acc: 0.036997]  [G loss: 9.769365, acc: 9.769365]\n",
      "943: [D loss: 0.024561, acc: 0.024561]  [G loss: 9.906624, acc: 9.906624]\n",
      "944: [D loss: 0.029566, acc: 0.029566]  [G loss: 9.816382, acc: 9.816382]\n",
      "945: [D loss: 0.016641, acc: 0.016641]  [G loss: 10.264156, acc: 10.264156]\n",
      "946: [D loss: 0.060718, acc: 0.060718]  [G loss: 9.493328, acc: 9.493328]\n",
      "947: [D loss: 0.021316, acc: 0.021316]  [G loss: 9.219246, acc: 9.219246]\n",
      "948: [D loss: 0.018930, acc: 0.018930]  [G loss: 9.131933, acc: 9.131933]\n",
      "949: [D loss: 0.042687, acc: 0.042687]  [G loss: 9.612392, acc: 9.612392]\n",
      "950: [D loss: 0.021427, acc: 0.021427]  [G loss: 10.275001, acc: 10.275001]\n",
      "951: [D loss: 0.009146, acc: 0.009146]  [G loss: 10.919155, acc: 10.919155]\n",
      "952: [D loss: 0.064644, acc: 0.064644]  [G loss: 11.409081, acc: 11.409081]\n",
      "953: [D loss: 0.077370, acc: 0.077370]  [G loss: 9.938179, acc: 9.938179]\n",
      "954: [D loss: 0.046144, acc: 0.046144]  [G loss: 9.077365, acc: 9.077365]\n",
      "955: [D loss: 0.181134, acc: 0.181134]  [G loss: 9.770424, acc: 9.770424]\n",
      "956: [D loss: 0.021306, acc: 0.021306]  [G loss: 10.279987, acc: 10.279987]\n",
      "957: [D loss: 0.058271, acc: 0.058271]  [G loss: 10.275698, acc: 10.275698]\n",
      "958: [D loss: 0.016123, acc: 0.016123]  [G loss: 10.083758, acc: 10.083758]\n",
      "959: [D loss: 0.051024, acc: 0.051024]  [G loss: 9.734632, acc: 9.734632]\n",
      "960: [D loss: 0.008019, acc: 0.008019]  [G loss: 9.333088, acc: 9.333088]\n",
      "961: [D loss: 0.093739, acc: 0.093739]  [G loss: 10.318825, acc: 10.318825]\n",
      "962: [D loss: 0.038638, acc: 0.038638]  [G loss: 10.723164, acc: 10.723164]\n",
      "963: [D loss: 0.067652, acc: 0.067652]  [G loss: 10.210360, acc: 10.210360]\n",
      "964: [D loss: 0.029109, acc: 0.029109]  [G loss: 9.445362, acc: 9.445362]\n",
      "965: [D loss: 0.030207, acc: 0.030207]  [G loss: 9.036064, acc: 9.036064]\n",
      "966: [D loss: 0.091265, acc: 0.091265]  [G loss: 10.233491, acc: 10.233491]\n",
      "967: [D loss: 0.021489, acc: 0.021489]  [G loss: 11.253750, acc: 11.253750]\n",
      "968: [D loss: 0.089438, acc: 0.089438]  [G loss: 10.885750, acc: 10.885750]\n",
      "969: [D loss: 0.042149, acc: 0.042149]  [G loss: 9.328498, acc: 9.328498]\n",
      "970: [D loss: 0.055265, acc: 0.055265]  [G loss: 8.744959, acc: 8.744959]\n",
      "971: [D loss: 0.024420, acc: 0.024420]  [G loss: 8.871149, acc: 8.871149]\n",
      "972: [D loss: 0.037159, acc: 0.037159]  [G loss: 9.358608, acc: 9.358608]\n",
      "973: [D loss: 0.020913, acc: 0.020913]  [G loss: 9.897392, acc: 9.897392]\n",
      "974: [D loss: 0.046175, acc: 0.046175]  [G loss: 10.089328, acc: 10.089328]\n",
      "975: [D loss: 0.014745, acc: 0.014745]  [G loss: 10.197694, acc: 10.197694]\n",
      "976: [D loss: 0.018136, acc: 0.018136]  [G loss: 9.874281, acc: 9.874281]\n",
      "977: [D loss: 0.027736, acc: 0.027736]  [G loss: 9.387000, acc: 9.387000]\n",
      "978: [D loss: 0.039857, acc: 0.039857]  [G loss: 9.088408, acc: 9.088408]\n",
      "979: [D loss: 0.023386, acc: 0.023386]  [G loss: 9.164074, acc: 9.164074]\n",
      "980: [D loss: 0.025695, acc: 0.025695]  [G loss: 10.348827, acc: 10.348827]\n",
      "981: [D loss: 0.022040, acc: 0.022040]  [G loss: 10.590901, acc: 10.590901]\n",
      "982: [D loss: 0.041075, acc: 0.041075]  [G loss: 10.146517, acc: 10.146517]\n",
      "983: [D loss: 0.026656, acc: 0.026656]  [G loss: 8.887141, acc: 8.887141]\n",
      "984: [D loss: 0.007054, acc: 0.007054]  [G loss: 8.202418, acc: 8.202418]\n",
      "985: [D loss: 0.047468, acc: 0.047468]  [G loss: 8.778902, acc: 8.778902]\n",
      "986: [D loss: 0.042891, acc: 0.042891]  [G loss: 10.384974, acc: 10.384974]\n",
      "987: [D loss: 0.036503, acc: 0.036503]  [G loss: 11.459168, acc: 11.459168]\n",
      "988: [D loss: 0.061339, acc: 0.061339]  [G loss: 11.008176, acc: 11.008176]\n",
      "989: [D loss: 0.037240, acc: 0.037240]  [G loss: 9.644569, acc: 9.644569]\n",
      "990: [D loss: 0.059017, acc: 0.059017]  [G loss: 10.006683, acc: 10.006683]\n",
      "991: [D loss: 0.013973, acc: 0.013973]  [G loss: 10.030548, acc: 10.030548]\n",
      "992: [D loss: 0.021007, acc: 0.021007]  [G loss: 9.896027, acc: 9.896027]\n",
      "993: [D loss: 0.022249, acc: 0.022249]  [G loss: 9.877874, acc: 9.877874]\n",
      "994: [D loss: 0.022603, acc: 0.022603]  [G loss: 9.747078, acc: 9.747078]\n",
      "995: [D loss: 0.025597, acc: 0.025597]  [G loss: 9.210861, acc: 9.210861]\n",
      "996: [D loss: 0.021245, acc: 0.021245]  [G loss: 9.622978, acc: 9.622978]\n",
      "997: [D loss: 0.012807, acc: 0.012807]  [G loss: 9.812824, acc: 9.812824]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "998: [D loss: 0.018101, acc: 0.018101]  [G loss: 9.553073, acc: 9.553073]\n",
      "999: [D loss: 0.038178, acc: 0.038178]  [G loss: 9.329873, acc: 9.329873]\n"
     ]
    }
   ],
   "source": [
    "train_steps = 1000\n",
    "batch_size = 64\n",
    "history = {\"d\":[],\"g\":[]}\n",
    "for e in range(train_steps):\n",
    "    # Make generative images\n",
    "    image_batch = X_train[np.random.randint(0,X_train.shape[0],size=batch_size),:,:,:] #sample images from real data\n",
    "    noise_gen = np.random.uniform(-1,1,size=[batch_size,input_dim]) #sample image from generated data\n",
    "    generated_images = generator.predict(noise_gen) #fake images\n",
    "\n",
    "    # Train discriminator on generated images\n",
    "    X = np.concatenate((image_batch, generated_images))\n",
    "    #create labels\n",
    "    y = np.ones([2*batch_size,1])\n",
    "    y[batch_size:,:] = 0\n",
    "    \n",
    "    #train discriminator\n",
    "    #make_trainable(discriminator,True)\n",
    "    d_loss  = discriminator.train_on_batch(X,y)\n",
    "    history[\"d\"].append(d_loss)\n",
    "\n",
    "    # train Generator-Discriminator stack on input noise to non-generated output class\n",
    "    noise_tr = np.random.uniform(-1,1,size=[batch_size,input_dim])\n",
    "    y = np.ones([batch_size, 1])\n",
    "\n",
    "    #make_trainable(discriminator,False)\n",
    "    g_loss = GAN.train_on_batch(noise_tr, y)\n",
    "    history[\"g\"].append(g_loss)\n",
    "\n",
    "    log_mesg = \"%d: [D loss: %f, acc: %f]\" % (e, d_loss, d_loss)\n",
    "    log_mesg = \"%s  [G loss: %f, acc: %f]\" % (log_mesg, g_loss, g_loss)\n",
    "    print(log_mesg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4412a74790>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N = 10\n",
    "noise = np.random.uniform(-1.0, 1.0, size=[N, input_dim]) \n",
    "\n",
    "images = G.predict(noise)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(images.shape[0]):\n",
    "    plt.subplot(4, 4, i+1)\n",
    "    image = images[i, :, :, :]\n",
    "    image = np.reshape(image, [img_rows, img_cols])\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda2]",
   "language": "python",
   "name": "conda-env-anaconda2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
