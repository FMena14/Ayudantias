{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "22276c6e-c66f-42e1-a81e-0f28d1d327d0"
    }
   },
   "source": [
    "<img src=\"http://www.exalumnos.usm.cl/wp-content/uploads/2015/06/Isotipo-Negro.gif\" title=\"Title text\" width=\"20%\" height=\"20%\" />\n",
    "\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "<h1 align='center'> INF-395/477 Redes Neuronales Artificiales I-2018 </h1>\n",
    "\n",
    "<H3 align='center'> Tarea 3 - Aplicaciones de Redes Neuronales </H3>\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "\n",
    "**Temas**  \n",
    "* Modelos Generativos profundos: VAE (*Variational Autoencoder*) y GAN (*Generative Adversarial Network*).\n",
    "* Arquitectura encoder-decoder y mecanismo de antención.\n",
    "* Arquitecturas recientes.\n",
    "* Desafío en donde se aplique todo lo aprendido.\n",
    " \n",
    "\n",
    "** Formalidades **  \n",
    "* Equipos de trabajo de: 2 personas (*cada uno debe estar en condiciones de realizar una presentación y discutir sobre cada punto del trabajo realizado*)\n",
    "* Se debe preparar una presentación de 20 minutos. Presentador será elegido aleatoriamente.\n",
    "* Se debe preparar un (breve) Jupyter/IPython notebook que explique la actividad realizada y las conclusiones del trabajo\n",
    "* Fecha de entrega y discusión: -\n",
    "* Formato de entrega: envı́o de link Github al correo electrónico del ayudante (*<francisco.mena.13@sansano.usm.cl>*) , incluyendo al profesor en copia (*<jnancu@inf.utfsm.cl>*). Por favor especificar el siguiente asunto: [Tarea3-INF395-I-2018]\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "\n",
    "La tarea se divide en secciones:\n",
    "\n",
    "[1.](#primero) Modelos Generativos  \n",
    "[2.](#segundo) *Question-Answering*  \n",
    "[3.](#tercero) Aplicaciones Recientes  \n",
    "[4.](#cuarto) Challenge (*Object Counting*)\n",
    "\n",
    "*Nota: Para esta actividad, al igual que anteriores, si es que no se cuenta con GPU se recomienda utilizar el entorno virtual de __[Colaboratory - Google](https://colab.research.google.com/)__*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "e78a9f09-cadc-4c45-b7c4-b37c95d6227c"
    }
   },
   "source": [
    "<a id=\"primero\"></a>\n",
    "## 1. Modelos Generativos\n",
    "\n",
    "Las redes neuronales hoy en día han sido aplicados a muchos problemas, de los cuales algunos son necesarios tener un modelo generativo el cual pueda artificialmente sintetizar nuevos ejemplos que sean similares a los originales, éste tipo de aprendizaje se llama **Unsupervised Learning**. Existen diferentes *approaches* para ésto, de los cuales solo veremos 2.\n",
    "\n",
    "Vamos a trabajar con los datos anteriormente trabajos de MNIST.\n",
    "\n",
    "```python\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "import keras\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "img_rows, img_cols,channel = X_train.shape[1:]\n",
    "```\n",
    "\n",
    "\n",
    "### *Variational Autoencoder* (VAE) [[1]](#refs)\n",
    "\n",
    "Los VAE son una variación a la arquitectura que ya vimos (autoencoder) en donde la codificación y decodificación están conectadas a través de un enfoque bayesiano en donde la codificación aprende los parámetros de alguna distribución de variables latentes de los datos en donde el decodificador muestra de ésta distribución de variables latentes para poder generar nuevos datos artificiales $\\hat{x}$. Dicho de otra palabras es un autoencoder que aprende el modelo de las variables latentes de los datos.\n",
    "\n",
    "\n",
    "El enfoque optimizador de los parámetros de la red neuronal $\\theta$ es que minimiza la reconstrucción de los datos (al igual que un autoencoder tradicional), en base a alguna medicicón de error (*mse* por ejemplo) y agrega una regularización que se impone para que la distribución aprendida de las variables latentes sea de alguna distribución deseada *a priori*.  \n",
    "\n",
    "$$ Min \\ L(q_{\\theta}(x\\mid z),x) + KL( q_{\\theta}(z\\mid x) \\mid \\mid p_{\\theta}(z))$$\n",
    "\n",
    "Con $L$ la función de pérdida de reconstrucción, $KL$ la *KL Divergence* [[5]](#refs), $q_{\\hat{\\theta}}(x\\mid z)$ la recontrucción aleatoria de los datos a través de las variables latentes $z$ y  $p_{\\theta}(z)$ una distribución *a priori*. \n",
    "\n",
    "<img src=\"https://i.imgur.com/ZN6MyTx.png\" title=\"VAE\" width=\"60%\" />\n",
    "\n",
    "> a) Defina la sección del *encoder* del VAE como el que se muestra en el código, de 3 tandas convolucionales y una tanda *fully conected*, con una distribución Normal multivariada de 2 componentes para las variables latentes. Describa la arquitectura utilizada para el *encoder*.\n",
    "```python\n",
    "# input image dimensions\n",
    "original_img_size = (img_rows, img_cols,channel)\n",
    "# number of convolutional filters to use\n",
    "filters = 32\n",
    "# convolution kernel size\n",
    "num_conv = 3\n",
    "latent_dim = 2\n",
    "intermediate_dim = 128\n",
    "from keras.layers import Input,Conv2D,Flatten,Dense,MaxPool2D\n",
    "from keras.models import Model\n",
    "## Encoder\n",
    "x = Input(shape=original_img_size)\n",
    "conv_1 = Conv2D(filters,kernel_size=num_conv,padding='same', activation='relu')(x)\n",
    "conv_2 = Conv2D(filters,kernel_size=num_conv,padding='same', activation='relu')(conv_1)\n",
    "conv_3 = Conv2D(filters*2, kernel_size=num_conv, padding='same', activation='relu', strides=2)(conv_2)\n",
    "flat = Flatten()(conv_3)\n",
    "hidden = Dense(intermediate_dim, activation='relu')(flat)\n",
    "z_mean = Dense(latent_dim,activation='linear')(hidden)\n",
    "z_log_var = Dense(latent_dim,activation='linear')(hidden)\n",
    "# build a model to project inputs on the latent space\n",
    "encoder = Model(x, z_mean)\n",
    "```\n",
    "\n",
    "> b) Defina la sección del *encoder* del VAE como el que se muestra en el código, una tanda *fully conected* y con 3 tandas de la operación inversa a una convolución (**Convolución transpuesta** [[2]](#refs)), comente cómo ésta trabaja y los parámetros de stride como funcionan. Además se *setea* la distribución de las variables latentes como una distribución Normal multivariada de 2 componentes.\n",
    "\n",
    "```python\n",
    "from keras.layers import Reshape,Conv2DTranspose,Activation\n",
    "## Decoder\n",
    "shape_before_flattening = K.int_shape(conv_3)[1:]\n",
    "# we instantiate these layers separately so as to reuse them later\n",
    "decoder_hid = Dense(intermediate_dim, activation='relu')\n",
    "decoder_upsample = Dense(np.prod(shape_before_flattening), activation='relu')\n",
    "decoder_reshape = Reshape(shape_before_flattening)\n",
    "decoder_deconv_1 = Conv2DTranspose(filters,kernel_size=num_conv, padding='same',strides=2,activation='relu')\n",
    "decoder_deconv_2 = Conv2DTranspose(filters,kernel_size=num_conv,padding='same', activation='relu')\n",
    "decoder_mean_squash = Conv2DTranspose(channel, kernel_size=num_conv,padding='same', activation='sigmoid')\n",
    "```\n",
    "\n",
    "> c) Defina la sección que conecta a estas dos partes a través de un *sampleo*, ésto es lo que lo hace que sea un enfoque probabilistico/bayesiano. Describa el modelo completo.\n",
    "```python\n",
    "def sampling(args):\n",
    "    epsilon_std = 1.0\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim),mean=0., stddev=epsilon_std)\n",
    "    return z_mean + K.exp(z_log_var) * epsilon\n",
    "from keras.layers import Lambda\n",
    "z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "hid_decoded = decoder_hid(z)\n",
    "up_decoded = decoder_upsample(hid_decoded)\n",
    "reshape_decoded =  decoder_reshape(up_decoded)\n",
    "deconv_1_decoded = decoder_deconv_1(reshape_decoded)\n",
    "x_decoded_relu = decoder_deconv_2(deconv_1_decoded)\n",
    "x_decoded_mean_squash = decoder_mean_squash(x_decoded_relu)\n",
    "# instantiate VAE model\n",
    "vae = Model(x, x_decoded_mean_squash)\n",
    "vae.summary()\n",
    "```\n",
    "\n",
    "> d) Como la función objetivo es *customizada* deberemos definirla y poner una distribución a *priori* sobre las variables latentes, en este caso se tendrá como media un vector de ceros y la matriz de covarianza la matriz identidad $p_{\\theta}(z) \\sim N(\\vec{0},I)$. Comente porqué esto más la *KL Divergence* podría funcionar como regularizador.\n",
    "```python\n",
    "# Compute VAE loss\n",
    "reconstruction_loss = img_rows * img_cols * channel* keras.metrics.binary_crossentropy(\n",
    "    K.flatten(x),\n",
    "    K.flatten(x_decoded_mean_squash))\n",
    "kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "vae.add_loss(vae_loss)\n",
    "vae.summary()\n",
    "```\n",
    "\n",
    "> e) Entrene el modelo definido con los datos de MNIST entre 10 a 15 *epochs* con el optimizador de *RMSprop* y tamaño de batch el que estime conveniente.\n",
    "```python\n",
    "batch_size = ...\n",
    "epochs =  [10,15]\n",
    "vae.compile(optimizer='rmsprop')\n",
    "vae.fit(X_train,epochs=epochs, batch_size=batch_size,validation_data=(X_test, None))\n",
    "```\n",
    "\n",
    "> f) Visualice la representación codificada $z$ (variables latentes) de los datos en base a su media. Además genere un histograma de la media y la varianza de las dos componentes. Comente\n",
    "```python\n",
    "# display a 2D plot of the digit classes in the latent space\n",
    "x_test_encoded = encoder.predict(X_test, batch_size=batch_size)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(x_test_encoded[:, 0], x_test_encoded[:, 1], c=y_test)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "encoder_log_var = Model(x,z_log_var)\n",
    "#histogram\n",
    "```\n",
    "\n",
    "> g) Genere nuevos datos artificialmente a través del espacio de las variables latentes, para esto deberá generar puntos linealmente separados por debajo de la distribución Normal. Comente qué significada cada eje en la imagen ¿qué sucede más allá en el espacio del 90% confianza de las variables latentes? ¿Qué objetos se generan?\n",
    "```python\n",
    "#GENERATOR\n",
    "decoder_input = Input(shape=(latent_dim,))\n",
    "_hid_decoded = decoder_hid(decoder_input)\n",
    "_up_decoded = decoder_upsample(_hid_decoded)\n",
    "_reshape_decoded = decoder_reshape(_up_decoded)\n",
    "_deconv_1_decoded = decoder_deconv_1(_reshape_decoded)\n",
    "_x_decoded_relu = decoder_deconv_2(_deconv_1_decoded)\n",
    "_x_decoded_mean_squash = decoder_mean_squash(_x_decoded_relu)\n",
    "generator = Model(decoder_input, _x_decoded_mean_squash)\n",
    "##PLOT\n",
    "n = 30  # figure with 15x15 images \n",
    "image_size = img_cols\n",
    "figure = np.zeros((image_size * n, image_size * n))\n",
    "from scipy.stats import norm\n",
    "grid_x = norm.ppf(np.linspace(0.05, 0.95, n))\n",
    "grid_y = norm.ppf(np.linspace(0.05, 0.95, n))\n",
    "for i, yi in enumerate(grid_x):\n",
    "    for j, xi in enumerate(grid_y):\n",
    "        z_sample = np.array([[xi, yi]])            \n",
    "        x_decoded = generator.predict(z_sample,batch_size=batch_size)\n",
    "        figure[i * image_size: (i + 1) * image_size,\n",
    "               j * image_size: (j + 1) * image_size] = x_decoded[0][:,:,0]\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(figure,cmap='gnuplot2')\n",
    "pos = np.arange(image_size/2,image_size*n,image_size)\n",
    "plt.yticks(pos,np.round(grid_y,1))\n",
    "plt.xticks(pos,np.round(grid_x,1))\n",
    "plt.show()\n",
    "#en los extremos del intervalo de confianza\n",
    "grid = norm.ppf(np.linspace(0.000005, 0.999995, n))\n",
    "```\n",
    "\n",
    "### *Generative Adversarial Networks* (GAN) [[4]](#refs)\n",
    "\n",
    "\n",
    "Las GAN son un enfoque distinto de modelo generativo, a pesar de que tiene dos redes conectadas, las tareas que realiza cada una de ella es distinto, es un modelo adversario en que una red *compite* con la otra. Por un lado se tiene la red discriminadora $D$ que intenta disernir si un dato proviene de los datos reales o fue un dato generado artificialmente, por otro lado se tiene la red generadora $G$ que intenta generar datos artificialmente de manera que la red discriminadora se confunda, es decir, sea lo mas similar a los datos reales. \n",
    "\n",
    "<img src=\"https://oshearesearch.com/wp-content/uploads/2016/07/mnist_gan.png\" title=\"VAE\" width=\"60%\" />\n",
    "\n",
    "\n",
    "El enfoque optimizador de los parámetros de la red neuronal es para $D$ el de maximizar la probabilidad de los datos que provienen de la distribución original de los datos minimizando la probabilidad de los datos que provienen de una distribución corrupta. Mientras que para $G$ es el de maximizzar la probabilidad de que $D$ asigne un dato de $G$ como real, o bien, minimizar la probabilidad de que $D$ asigne un dato de $G$ como corrupto.\n",
    "\n",
    "$$\n",
    "min_G \\ max_G = E_{x\\sim p_{data}(x) }[logD(x)] + E_{z\\sim p_z(z)}[log(1-D(G(z))]\n",
    "$$\n",
    "\n",
    "Ésto tiene un óptimo teórico que es cuando $p_g = p_{data}$, es decir, cuando el *generador* $G$ logra imitar la distribución de probabilidad de los datos.\n",
    "\n",
    "\n",
    "\n",
    "> h) Defina al *discriminador* de la GAN como el que se muestra en el código, de 3 tandas convolucionales y una tanda *fully conected*, con los Dropout para evitar *overfitting*. Describa la arquitectura utilizada y cuál es la función de activación seleccionada.\n",
    "```python\n",
    "from keras.models import Model,Sequential\n",
    "from keras.layers import LeakyReLU,Conv2D,Dropout,Flatten,Dense\n",
    "## Discriminator\n",
    "D = Sequential()\n",
    "depth = 64\n",
    "dropout = 0.4\n",
    "input_shape = (img_rows, img_cols, channel)\n",
    "D.add(Conv2D(depth*1, (5,5), strides=2, input_shape=input_shape,padding='same', activation=LeakyReLU(alpha=0.2)))\n",
    "D.add(Dropout(dropout))\n",
    "D.add(Conv2D(depth*2, (5,5), strides=2, padding='same',activation=LeakyReLU(alpha=0.2)))\n",
    "D.add(Dropout(dropout))\n",
    "D.add(Conv2D(depth*4, (5,5), strides=2, padding='same',activation=LeakyReLU(alpha=0.2)))\n",
    "D.add(Dropout(dropout))\n",
    "D.add(Flatten())\n",
    "D.add(Dense(1024,activation=LeakyReLU(alpha=0.2)))\n",
    "D.add(Dense(1,activation='sigmoid'))\n",
    "D.summary()\n",
    "```\n",
    "\n",
    "> i) Defina al *generador* de la GAN como el que se muestra en el código, con una tanda *fully conected* y 3 tandas convolucionales transpuesta además de agregar *BatchNormalization* entre ellas para tener un aprendizaje mas estable. Describa la arquitectura utilizada (siendo del tipo *fully convolutional* puesto que la salida es un arreglo n-dimensional) y el porqué la función de activación de la salida es *sigmoidal*.\n",
    "```python\n",
    "from keras.layers import BatchNormalization,Reshape,UpSampling2D,Conv2DTranspose,Activation\n",
    "## Generator\n",
    "G = Sequential()\n",
    "dim = 14\n",
    "input_dim= 2 #para que sea similar al vAE\n",
    "G.add(Dense(128, input_dim=input_dim))\n",
    "G.add(BatchNormalization())\n",
    "G.add(Activation('relu'))\n",
    "G.add(Dense(dim*dim*depth))\n",
    "G.add(BatchNormalization())\n",
    "G.add(Activation('relu'))\n",
    "G.add(Reshape((dim, dim, depth)))\n",
    "G.add(Conv2DTranspose(depth/2, (3,3), padding='same',strides=(2,2)))\n",
    "G.add(BatchNormalization())\n",
    "G.add(Activation('relu'))\n",
    "G.add(Conv2DTranspose(depth/2, (3,3), padding='same'))\n",
    "G.add(BatchNormalization())\n",
    "G.add(Activation('relu'))\n",
    "G.add(Conv2DTranspose(channel, (3,3), padding='same')) \n",
    "G.add(Activation('sigmoid')) \n",
    "G.summary()\n",
    "```\n",
    "\n",
    "> j) Conecte los modelos a través del enfoque adversario, será necesario definir dos modelos debido a que el entrenamiento es iterativo, primero se entrena el discriminador el generador fijo, luego se entrena el generador con el discriminador fijo y así. \n",
    "```python\n",
    "from keras.optimizers import RMSprop\n",
    "## Discriminator model (police)\n",
    "optimizer = RMSprop(lr=0.0008, clipvalue=1.0, decay=6e-8)\n",
    "DM = Sequential()\n",
    "DM.add(D)\n",
    "DM.compile(loss='binary_crossentropy', optimizer=optimizer,metrics=['accuracy'])\n",
    "## Adversarial model (Generator->Discriminator)\n",
    "D.trainable=False #set the discriminator freeze  (fixed params)\n",
    "optimizer = RMSprop(lr=0.0004, clipvalue=1.0, decay=3e-8)\n",
    "AM = Sequential()\n",
    "AM.add(G)\n",
    "AM.add(D)\n",
    "AM.compile(loss='binary_crossentropy', optimizer=optimizer,metrics=['accuracy'])\n",
    "```\n",
    "\n",
    "> k) Entrene el modelo definido con el enfoque iterativo como se nombró, para ésto utilice la función que se presenta que lo realiza de manera manual. Grafique la pérdida *loss* de cada red (el generador y el discriminador/adversario) a través de los pasos de actualización de los pesos ¿Cómo se espera que sean estas curvas de aprendizaje?\n",
    "```python\n",
    "def train_on_steps(X_train,DM,AM,G,steps,batch_size):\n",
    "    history = {\"d\":[],\"g\":[]}\n",
    "    for e in range(train_steps):\n",
    "        # Make generative images\n",
    "        image_batch = X_train[np.random.randint(0,X_train.shape[0],size=batch_size),:,:,:] #sample images from real data\n",
    "        noise_gen = np.random.uniform(-1,1,size=[batch_size,input_dim]) #sample image from generated data\n",
    "        generated_images = G.predict(noise_gen) #fake images\n",
    "        # Train discriminator on generated images\n",
    "        X = np.concatenate((image_batch, generated_images))\n",
    "        #create labels\n",
    "        y = np.ones([2*batch_size,1])\n",
    "        y[batch_size:,:] = 0\n",
    "        d_loss  = DM.train_on_batch(X,y)\n",
    "        history[\"d\"].append(d_loss)\n",
    "        # train Generator-Discriminator stack on input noise to non-generated output class\n",
    "        noise_tr = np.random.uniform(-1,1,size=[batch_size,input_dim])\n",
    "        y = np.ones([batch_size, 1])\n",
    "        g_loss = AM.train_on_batch(noise_tr, y)\n",
    "        history[\"g\"].append(g_loss)\n",
    "        log_mesg = \"%d: [D loss: %f, acc: %f]\" % (e, d_loss[0], d_loss[1])\n",
    "        log_mesg = \"%s  [G loss: %f, acc: %f]\" % (log_mesg, g_loss[0], g_loss[1])\n",
    "        print(log_mesg)\n",
    "        return history\n",
    "train_steps = 5000 #or few if  you want\n",
    "hist = train_on_steps(X_train,DM,AM,G,train_steps,64)\n",
    "```\n",
    "\n",
    "> l) Genere nuevos datos artificialmente a través del modelo generador *G* ya entrenado, para esto inicialice aleatoriamente el espacio oculto de dimensiones del generador a través de una distribución Uniforme entre -1 y 1, al igual como fue entrenado. Comente sobre las imágenes generadas y compare con lo realizado con el VAE, en temas de calidad visual y en tiempos de ejecución.\n",
    "```python\n",
    "N = 10\n",
    "noise = np.random.uniform(-1.0, 1.0, size=[N, input_dim]) \n",
    "images = G.predict(noise)\n",
    "import matplotlib.pyplot as plt\n",
    "for i in range(images.shape[0]):\n",
    "    plt.subplot(4, 4, i+1)\n",
    "    image = images[i, :, :, :]\n",
    "    image = np.reshape(image, [img_rows, img_cols])\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "> m) ¿Qué le parece que resulta mas crucial al momento de entrenar las GAN, saber que se tiene un buen generador e intentar mejorar el discriminador o saber que se tiene un buen discriminador e intentar mejorar el generador? en ambos casos para que el generador mejore. Experimente con una de las ideas, modifique el generador o el discriminador e intente generar mejores imágenes artificiales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "bc98692e-2ab1-42be-a32d-19a503d57b53"
    }
   },
   "source": [
    "<a id=\"segundo\"></a>\n",
    "## 2. *Question Answering*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "43bb9ca4-ed5c-45e8-8856-b3a74f64286d"
    }
   },
   "source": [
    "<a id=\"tercero\"></a>\n",
    "## 3. Aplicaciones Recientes\n",
    "\n",
    "en cifar o svhn o stl\n",
    "\n",
    "\n",
    "unpoco de historia de estas redes famosas y xke alexnet es conocida.. \n",
    "los parametros estan casi todos al final...  (densa)\n",
    "inception\n",
    "resnet\n",
    "\n",
    "\n",
    "|Model|Size|Top-1 Accuracy|Top-5 Accuracy|Parameters|Depth|\n",
    "|---|---|---|---|---|---|\n",
    "|VGG16|\t528 MB|\t0.715|\t0.901|\t138,357,544|\t23|\n",
    "|VGG19|\t549 MB|\t0.727|\t0.910|\t143,667,240|\t26|\n",
    "|ResNet50|\t99 MB|\t0.759\t|0.929\t|25,636,712\t|168|\n",
    "|InceptionV3|\t92 MB|\t0.788|\t0.944|\t23,851,784\t|159|\n",
    "|MobileNet|\t17 MB|\t0.665|\t0.871|\t4,253,864|\t88|\n",
    "|DenseNet121|\t33 MB|\t0.745|\t0.918|\t8,062,504|\t121|\n",
    "\n",
    "vgg 16 xke 16 si son 23? o resnet 50 y no son 50...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "c00fcdff-9b11-4f14-b44e-b63922d90c9e"
    }
   },
   "source": [
    "<a id=\"cuarto\"></a>\n",
    "## 4. Challenge: Crowd (Object) Counting\n",
    "\n",
    "<img src=\"http://personal.ie.cuhk.edu.hk/~ccloy/images/shopping_mall_annotated.jpg\" title=\"Crowd Counting\" width=\"30%\"/>\n",
    "\n",
    "Para esta sección final se evaluará todo lo que han aprendido a través de un desafío en donde puedan competir y medir sus resultados *in time* en la plataforma de __[Kaggle](https://www.kaggle.com/)__. El problema y todo su detalle puede ser encontrado en la página de la competencia a través del siguiente link:\n",
    "\n",
    "\n",
    "<center><H2> __[Competencia Object Counting](https://www.kaggle.com/t/59c93ca0e8ae47999f9287a5751d6402)__ </H2></center>\n",
    "\n",
    "\n",
    "\n",
    "Para esto deberán crearse una cuenta en la plataforma *Kaggle* y subir sus respuestas a ésta. Por favor crearse nombres que sean fácil identificar después para saber quién fue quién, sino no podrán tener la nota (o en el correo de entrega ponen cual es su nombre de usuario en la competencia).\n",
    "\n",
    "\n",
    "> Las entregas en *csv* pueden ser generadas de la siguiente manera:\n",
    "```python\n",
    "import pandas as pd\n",
    "d = {'id': test_ids, 'count': prediction_test}\n",
    "entrega = pd.DataFrame(data=d,columns=['id','count'])\n",
    "entrega.to_csv('mysubmission.csv', index=False)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"refs\"></a>\n",
    "## Referencias\n",
    "[1] Kingma, D. P., & Welling, M. (2013). *Auto-encoding variational bayes*. arXiv preprint arXiv:1312.6114.  \n",
    "[2] Dumoulin, V., & Visin, F. (2016). *A guide to convolution arithmetic for deep learning*. arXiv preprint arXiv:1603.07285.  \n",
    "[3] https://github.com/vdumoulin/conv_arithmetic  \n",
    "[4] Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., ... & Bengio, Y. (2014). *Generative adversarial nets*. In Advances in neural information processing systems (pp. 2672-2680).  \n",
    "[5] Joyce, J. M. (2011). *Kullback-leibler divergence*. In International Encyclopedia of Statistical Science (pp. 720-722). Springer Berlin Heidelberg.  \n",
    "[6] https://arxiv.org/pdf/1512.03385.pdf resnet\n",
    "[7] https://arxiv.org/pdf/1512.00567.pdf inception (google)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "3f5f459a-7a59-4f7d-9457-970359da7599"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "nbpresent": {
   "slides": {},
   "themes": {
    "default": "a7c086a1-b19f-4552-b732-d8b33f306280",
    "theme": {}
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
