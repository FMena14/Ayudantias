{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.exalumnos.usm.cl/wp-content/uploads/2015/06/Isotipo-Negro.gif\" title=\"Title text\" width=\"20%\" height=\"20%\" />\n",
    "\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "<h1 align='center'> INF-393 Máquinas de Aprendizaje II-2019 </h1>\n",
    "\n",
    "<H3 align='center'> Tarea 0 - Introducción a Máquinas de Aprendizaje </H3>\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "\n",
    "**Temas**  \n",
    "* Introducción a librerías comunes de *Machine Learning*:\n",
    "    * Pandas\n",
    "    * Numpy\n",
    "    * Sklearn\n",
    "    * Matplotlib\n",
    "    * Otro..\n",
    "* Implementación de Perceptrón y variantes.\n",
    "* Implementación de método aprendizaje online (Gradiente descendente).\n",
    " \n",
    "\n",
    "** Formalidades **  \n",
    "* Equipos de trabajo de: 2 personas\n",
    "* Se debe preparar un (breve) Jupyter/IPython notebook que explique la actividad realizada y las conclusiones del trabajo\n",
    "* Fecha de entrega y discusión: xx.\n",
    "* Formato de entrega: envı́o de link Github al correo electrónico del ayudante (*<francisco.mena.13@sansano.usm.cl>*) , incluyendo al profesor en copia (*<jnancu@inf.utfsm.cl>*). Por favor especificar el siguiente asunto: [Tarea0-INF393-II-2019]\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "\n",
    "La tarea se divide en secciones:\n",
    "\n",
    "[1.](#primero) Perceptrón a mano\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"primero\"></a>\n",
    "## 1. Perceptrón a mano\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1838/1*n6sJ4yZQzwKL9wnF5wnVNg.png\" width=\"40%\"  />\n",
    "\n",
    "En esta sección se le pedirá que implemente el algoritmo online del *perceptrón* [[2]](#refs) para aprender una función de separación lineal en un problema de clasificación binaria (0 o 1) a través de la función de *treshold*. Un algoritmo online, como el caso del *perceptrón*, aprende de una instancia de dato a la vez $(x^{(i)},y^{(i)})$, dentro de un conjunto de datos $\\{(x^{(0)},y^{(0)}), (x^{(1)},y^{(1)}), \\ldots, (x^{(N)},y^{(N)})  \\}$, donde la predicción para cada instancia es través de la función de *treshold*:\n",
    "\n",
    "$$\n",
    "f(x^{(i)}) = \\left\\{ \\begin{array}{lc}\n",
    "       1 &  si \\ \\sum_j w_j \\cdot x^{(i)}_j +b \\geq \\theta \\\\\n",
    "       0 &  si \\ \\sum_j w_j \\cdot x^{(i)}_j +b < \\theta\n",
    "     \\end{array} \\right.\n",
    "$$\n",
    "\n",
    "\n",
    "Donde $\\theta = 0$. Recordar que el *bias* $b$ se puede incluir en $w$ si se agrega una columna extra de 1's a los datos de entrada $x$ (*como se ve en la imagen anterior*). \n",
    "\n",
    "Para lo que sigue de la actividad sólo podrá utilizar *numpy* (para operaciones de algebra lineal).\n",
    "\n",
    "> a) Escriba una función que calcule el valor de salida (*output*) del modelo $f(x)$ para un patrón de entrada $x$ a través de los pesos $w$ del modelo. *Decida si incluir los bias dentro de $w$ o manejarlos de manera separada*\n",
    "\n",
    "> b) Escriba una función que implemente el clásico algoritmo del *perceptrón* para un problema binario que permita entrenarlo en un conjunto de datos de tamaño $N$, leídos de manera *online* (uno a uno). *Recordar la decisión anterior sobre los bias*.\n",
    "\n",
    "> c) Demuestre que lo implementado funciona en un problema real de clasificación. Para esto utilice el dataset **Breast cancer wisconsin**, disponible a través de la librería __[*sklearn*](http://scikit-learn.org)__, el cual corresponde a la detección de cancer mamario a través de características relevantes (numéricas continuas) de un examen realizado, como por ejemplo la textura, simetría y tamaño de una masa mamaria. Estas características deben combinarse linealmente para la detección del cancer.\n",
    "> <div class=\"alert alert-block alert-info\">Es una buena práctica el normalizar los datos antes de trabajar con el modelo</div>\n",
    "```python\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "X_train,y_train = load_breast_cancer(return_X_y=True)\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "```\n",
    "Para evaluar los resultados mida la exactitud (*accuracy*) de la clasificación durante el entrenamiento (por cada iteración/instancia/dato), utilice el conjunto de entrenamiento realizando una sola pasada (el objetivo de esta sección es familiarizarse con el algoritmo). Además reporte el tiempo de entrenamiento mediante el algoritmo implementado.\n",
    "\n",
    "> d) Escriba una función que implemente el *forgetrón* [[3]](#refs) con una memoria de tamaño $K$ y la función de kernel como el producto interno (*inner-product*).\n",
    "\n",
    "> e) Vuelva a realizar el item c) para el *forgetrón* con un $K=10$ y compare los resultados.\n",
    "\n",
    "\n",
    "### ¿Qué sucede al variar la función objetivo del problema? \n",
    "Si utilizáramos la función objetivo *binary cross entropy*\n",
    "$$\n",
    "\\ell (y; f(x)) = - y \\cdot \\log{(f(x))} - (1-y) \\cdot \\log{(1-f(x))}\n",
    "$$\n",
    "\n",
    "\n",
    "> f) Escriba una función que calcule la función de pérdida para un dato $i$.\n",
    "\n",
    "> g) Escriba una función que calcule el gradiente (derivada) de la función de pérdida anterior con respecto a los pesos del modelo $w$. *Escriba explícitamente la derivada (gradiente)*.\n",
    "\n",
    "> h) Realice una modificación al algoritmo implementado en b) (*perceptron*) para que se adatpe a la función objetivo *binary cross entropy* utilizando el algoritmo SGD [[4]](#refs) (*Stochastic Gradient Descend*) con tasa de aprendizaje $\\eta \\in [0,1]$.\n",
    "\n",
    "$$ \\vec{w}^{(t+1)} \\leftarrow \\vec{w}^{(t)} - \\eta \\nabla_{\\vec{w}^{(t)}} loss $$\n",
    "\n",
    "> i) Vuelva a realizar el item c) para esta modificación, compare los resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"refs\"></a>\n",
    "### Referencias\n",
    "[1] Hastie, T.; Tibshirani, R., Friedman, J. (2009), *The Elements of Statistical Learning*, Second Edition.\n",
    "Springer New York Inc.  \n",
    "[2] STEPHEN, I. (1990). *Perceptron-based learning algorithms*. IEEE Transactions on neural networks, 50(2), 179.  \n",
    "[3] Dekel, O., Shalev-Shwartz, S., & Singer, Y. (2006). *The Forgetron: A kernel-based perceptron on a fixed budget*. In Advances in neural information processing systems (pp. 259-266).  \n",
    "[4] Ruder, S. (2016). *An overview of gradient descent optimization algorithms*. arXiv preprint arXiv:1609.04747.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
