{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quora questions pairs\n",
    "link: https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs  \n",
    "link: https://www.kaggle.com/c/quora-question-pairs/data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29716</th>\n",
       "      <td>29716</td>\n",
       "      <td>54951</td>\n",
       "      <td>54952</td>\n",
       "      <td>What is swarm robot?</td>\n",
       "      <td>Will swarm robots exist in the near future?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310538</th>\n",
       "      <td>310538</td>\n",
       "      <td>434667</td>\n",
       "      <td>434668</td>\n",
       "      <td>What is the intuition behind the \"potential\" f...</td>\n",
       "      <td>How the infix to postfix algorithm came to be,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372778</th>\n",
       "      <td>372778</td>\n",
       "      <td>64563</td>\n",
       "      <td>217304</td>\n",
       "      <td>How do I learn ethical hacking in online?</td>\n",
       "      <td>How do I learn ethical hacking, for free of co...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395532</th>\n",
       "      <td>395532</td>\n",
       "      <td>528527</td>\n",
       "      <td>1772</td>\n",
       "      <td>What is the significance of 9 9 5 to a Pythago...</td>\n",
       "      <td>How can you increase your height?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10026</th>\n",
       "      <td>10026</td>\n",
       "      <td>19463</td>\n",
       "      <td>19464</td>\n",
       "      <td>What is a love crime?</td>\n",
       "      <td>If I love someone, is it a crime?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id    qid1    qid2  \\\n",
       "29716    29716   54951   54952   \n",
       "310538  310538  434667  434668   \n",
       "372778  372778   64563  217304   \n",
       "395532  395532  528527    1772   \n",
       "10026    10026   19463   19464   \n",
       "\n",
       "                                                question1  \\\n",
       "29716                                What is swarm robot?   \n",
       "310538  What is the intuition behind the \"potential\" f...   \n",
       "372778          How do I learn ethical hacking in online?   \n",
       "395532  What is the significance of 9 9 5 to a Pythago...   \n",
       "10026                               What is a love crime?   \n",
       "\n",
       "                                                question2  is_duplicate  \n",
       "29716         Will swarm robots exist in the near future?             0  \n",
       "310538  How the infix to postfix algorithm came to be,...             0  \n",
       "372778  How do I learn ethical hacking, for free of co...             0  \n",
       "395532                  How can you increase your height?             0  \n",
       "10026                   If I love someone, is it a crime?             0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "def logloss(act, pred):\n",
    "    epsilon = 1e-15\n",
    "    pred = sp.maximum(epsilon, pred)\n",
    "    pred = sp.minimum(1-epsilon, pred)\n",
    "    ll = np.sum(act*sp.log(pred) + sp.subtract(1,act)*sp.log(sp.subtract(1,pred)))\n",
    "    ll = ll * -1.0/len(act)\n",
    "    return ll\n",
    "\n",
    "train_data = pd.read_csv('./quora/quora_duplicate_questions.tsv',sep='\\t').sample(5000, random_state=40)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 6)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicated questions 1894\n",
      "Not duplicated questions 3106\n",
      "Duplicated questions per cent 37.880000 \n",
      "Not duplicated questions per cent 62.120000 \n"
     ]
    }
   ],
   "source": [
    "n, d = train_data.shape\n",
    "print \"Duplicated questions %d\"%sum(train_data['is_duplicate'])\n",
    "print \"Not duplicated questions %d\"% (n- sum(train_data['is_duplicate']))\n",
    "#porcentajes\n",
    "print \"Duplicated questions per cent %f \"% (100*np.sum(train_data['is_duplicate']) / float(n))\n",
    "print \"Not duplicated questions per cent %f \"% (100*(n- np.sum(train_data['is_duplicate'])) / float(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is swarm robot?\n",
      "Will swarm robots exist in the near future?\n",
      "False\n",
      "[u'swarm', u'robot']\n",
      "[u'swarm', u'robot', u'exist', u'near', u'futur']\n",
      "listo\n"
     ]
    }
   ],
   "source": [
    "import re, time\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import WordNetLemmatizer, word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "def word_extractor(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    commonwords = stopwords.words('english')\n",
    "    try:\n",
    "        text = re.sub(r'([a-z])\\1+', r'\\1\\1',text)#substitute multiple letter by two\n",
    "        text = re.sub(r'([^\\s\\w]|_)+', r'',text) #delete chars that are not letter or number and downcase\n",
    "        text = re.sub(r'[^\\w]', r' ',text)\n",
    "    except: \n",
    "        if text != text:\n",
    "            text = \" \"\n",
    "        print text\n",
    "    words = []\n",
    "    wordtokens = [ stemmer.stem(word.lower())  for word in word_tokenize(text.decode('utf-8', 'ignore')) ]\n",
    "    for word in wordtokens:\n",
    "        if word not in commonwords:\n",
    "            words.append(word)\n",
    "    return words\n",
    "\n",
    "question1 =  train_data.iloc[0][3]\n",
    "question2 = train_data.iloc[0][4]\n",
    "print question1\n",
    "print question2\n",
    "print 1==train_data.iloc[0].is_duplicate\n",
    "\n",
    "print word_extractor(question1)\n",
    "print word_extractor(question2)\n",
    "\n",
    "#tokeniza\n",
    "pregunta1 = [word_extractor(word) for word in train_data.question1 ]\n",
    "pregunta2 = [word_extractor(word) for word in train_data.question2 ]\n",
    "\n",
    "print \"listo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de palabras en el vocabulario: 7931\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer=lambda doc: doc, lowercase=False,ngram_range=(1, 1), binary='False')\n",
    "#vectorizer = TfidfVectorizer(ngram_range=(1, 1), binary='False')\n",
    "preguntas = np.concatenate((np.asarray(pregunta1),np.asarray(pregunta2)))\n",
    "vectorizer.fit(np.asarray(preguntas))\n",
    "vocab = vectorizer.get_feature_names()\n",
    "print \"Cantidad de palabras en el vocabulario: %d\" %len(vocab)\n",
    "\n",
    "pregunta1_vectorizada = vectorizer.transform(pregunta1)\n",
    "pregunta2_vectorizada = vectorizer.transform(pregunta1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============  =========  ============  ===  ============  =========  ============\n",
      "  Pregunta 1  Palabra      Frecuencia  #      Pregunta 2  Palabra      Frecuencia\n",
      "============  =========  ============  ===  ============  =========  ============\n",
      "           1  whi                 507  #               1  whi                 529\n",
      "           2  best                413  #               2  best                457\n",
      "           3  doe                 381  #               3  doe                 324\n",
      "           4  get                 239  #               4  get                 250\n",
      "           5  like                178  #               5  like                180\n",
      "           6  india               161  #               6  peopl               178\n",
      "           7  peopl               159  #               7  way                 174\n",
      "           8  use                 154  #               8  india               172\n",
      "           9  way                 149  #               9  use                 167\n",
      "          10  differ              147  #              10  good                143\n",
      "          11  good                132  #              11  would               142\n",
      "          12  would               121  #              12  make                141\n",
      "          13  make                118  #              13  differ              132\n",
      "          14  one                 113  #              14  one                 124\n",
      "          15  quora               112  #              15  quora               111\n",
      "          16  mean                 96  #              16  ani                 105\n",
      "          17  wa                   95  #              17  thi                 103\n",
      "          18  life                 89  #              18  life                 99\n",
      "          19  learn                89  #              19  wa                   92\n",
      "          20  ha                   88  #              20  know                 91\n",
      "          21  thi                  87  #              21  indian               90\n",
      "          22  ani                  85  #              22  learn                89\n",
      "          23  year                 82  #              23  think                88\n",
      "          24  indian               80  #              24  year                 87\n",
      "          25  time                 79  #              25  mean                 84\n",
      "============  =========  ============  ===  ============  =========  ============\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vector_pregunta1 = vectorizer.transform(pregunta1)  #+pregunta2])\n",
    "vector_pregunta2 = vectorizer.transform(pregunta2)  #+pregunta2])\n",
    "    \n",
    "dist=list(np.array(vector_pregunta1.sum(axis=0)).reshape(-1,))\n",
    "dist2=list(np.array(vector_pregunta2.sum(axis=0)).reshape(-1,))\n",
    "\n",
    "# Se ordenan las palabras por cantidad\n",
    "lista_train = zip(vocab, dist)\n",
    "lista_train.sort(key=lambda x: x[1])\n",
    "lista_train.reverse()\n",
    "# Se ordenan las palabras por cantidad\n",
    "lista_test = zip(vocab, dist2)\n",
    "lista_test.sort(key=lambda x: x[1])\n",
    "lista_test.reverse()\n",
    "\n",
    "N = 25\n",
    "pals_train = []\n",
    "count_train =[]\n",
    "pals_test = []\n",
    "count_test = []\n",
    "for i in range(N):\n",
    "    tag, count = lista_train[i]\n",
    "    pals_train.append(tag)\n",
    "    count_train.append(count)\n",
    "    tag_test, count_t = lista_test[i]\n",
    "    pals_test.append(tag_test)\n",
    "    count_test.append(count_t)    \n",
    "\n",
    "a = [range(1,N+1),pals_train,count_train, [\"#\"]*N, range(1,N+1), pals_test,count_test]\n",
    "table =  zip(*a)\n",
    "from tabulate import tabulate\n",
    "print tabulate(table, headers=[\"Pregunta 1\",\"Palabra\",\"Frecuencia\",\"#\", \"Pregunta 2\",\"Palabra\",\"Frecuencia\"],  tablefmt=\"rst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "X = hstack((pregunta1_vectorizada, pregunta2_vectorizada))\n",
    "\n",
    "y = np.asarray(train_data.is_duplicate.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 15862)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes\n",
      "LogLoss en Train set predict model: 0.421424\n",
      "LogLoss en Val set predict model: 1.207405\n",
      "Multinomial Bayes\n",
      "LogLoss en Train set predict model: 0.304714\n",
      "LogLoss en Val set predict model: 1.086724\n",
      "Random Forest\n",
      "LogLoss en Train set predict model: 0.618490\n",
      "LogLoss en Val set predict model: 0.642403\n",
      "Logistic Regression\n",
      "LogLoss en Train set predict model: 0.492190\n",
      "LogLoss en Val set predict model: 0.616440\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "model = BernoulliNB()\n",
    "model = model.fit(X_train, y_train)\n",
    "print \"Naive Bayes\"\n",
    "print \"LogLoss en Train set predict model: %f\"%logloss(y_train,model.predict_proba(X_train)[:,1])\n",
    "print \"LogLoss en Val set predict model: %f\"%logloss(y_val,model.predict_proba(X_val)[:,1])\n",
    "#print \"Cross validation predict model: %f\"%C_V(model,X.todense(),y)\n",
    "#score_the_model(model,x,y,xt,yt,\"BernoulliNB\")\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model = MultinomialNB()\n",
    "model = model.fit(X_train, y_train)\n",
    "print \"Multinomial Bayes\"\n",
    "print \"LogLoss en Train set predict model: %f\"%logloss(y_train,model.predict_proba(X_train)[:,1])\n",
    "print \"LogLoss en Val set predict model: %f\"%logloss(y_val,model.predict_proba(X_val)[:,1])\n",
    "#print \"Cross validation predict model: %f\"%C_V(model,X.todense(),y)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier as RandomFor\n",
    "model = RandomFor(max_depth=15)\n",
    "model = model.fit(X_train, y_train)\n",
    "print \"Random Forest\"\n",
    "print \"LogLoss en Train set predict model: %f\"%logloss(y_train,model.predict_proba(X_train)[:,1])\n",
    "print \"LogLoss en Val set predict model: %f\"%logloss(y_val,model.predict_proba(X_val)[:,1])\n",
    "#print \"Cross validation predict model: %f\"%C_V(model,X.todense(),y)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(penalty='l2',C=0.1)\n",
    "model = model.fit(X_train, y_train)\n",
    "print \"Logistic Regression\"\n",
    "print \"LogLoss en Train set predict model: %f\"%logloss(y_train,model.predict_proba(X_train)[:,1])\n",
    "print \"LogLoss en Val set predict model: %f\"%logloss(y_val,model.predict_proba(X_val)[:,1])\n",
    "#print \"Cross validation predict model: %f\"%C_V(model,X.todense(),y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
